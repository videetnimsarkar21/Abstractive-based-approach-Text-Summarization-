{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gR-ZiNFACW46"
   },
   "outputs": [],
   "source": [
    "default_path = \" \"\n",
    "\n",
    "#-------------------------------save/load--------------------------------------#\n",
    "pickle_path = default_path + \"pickles/\"\n",
    "log_file = default_path + \"logs.txt\"\n",
    "csv_file = default_path + \"logs.csv\"\n",
    "\n",
    "log_file_handler = open(log_file,\"a\")\n",
    "csv_file_handler = open(csv_file,\"a\")\n",
    "\n",
    "file1 = open(log_file , \"w+\")\n",
    "file2 = open(csv_file , \"w+\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save(obj , filename):\n",
    "  print(\"saving {} ..\".format(filename))\n",
    "  with open(filename, 'wb') as handle:\n",
    "      pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "def load(filename):\n",
    "  print(\"loading {} ..\".format(filename))\n",
    "  with open(filename, 'rb') as handle:\n",
    "    return pickle.load(handle)\n",
    "#-----------------------------------------------------------------------------------#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuHqMQ0xIDMy"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import struct\n",
    "import csv\n",
    "from tensorflow.core.example import example_pb2\n",
    "import operator\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNKNOWN_TOKEN = '[UNK]' \n",
    "START_DECODING = '[START]' \n",
    "STOP_DECODING = '[STOP]' \n",
    "\n",
    "class Vocab(object):\n",
    "  \n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 \n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w.lower()] = self._count\n",
    "      self._id_to_word[self._count] = w.lower()\n",
    "      self._count += 1\n",
    "\n",
    "    with open(vocab_file, 'r',encoding='utf-8') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0].lower()\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          print(\"Duplicate:\",w)\n",
    "          continue\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN.lower()]\n",
    "    return self._word_to_id[word.lower()]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "        return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    \n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f,delimiter=str(u\"\\t\").encode('utf-8') , fieldnames=fieldnames)\n",
    "      for i in range(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "\n",
    "  def LoadWordEmbedding(self, w2v_file, word_dim):\n",
    "    self.wordDict = {}\n",
    "    self.word_dim = word_dim\n",
    "\n",
    "    self.wordDict[UNKNOWN_TOKEN] = np.zeros(self.word_dim,dtype=np.float32)\n",
    "    self.wordDict[PAD_TOKEN] = np.random.uniform(-1,1,self.word_dim)\n",
    "    self.wordDict[START_DECODING] = np.random.uniform(-1,1,self.word_dim)\n",
    "    self.wordDict[STOP_DECODING] = np.random.uniform(-1,1,self.word_dim)\n",
    "    with open(w2v_file) as wf:\n",
    "      for line in wf:\n",
    "        info = line.strip().split()\n",
    "        word = info[0]\n",
    "        coef = np.asarray(info[1:], dtype='float32')\n",
    "        self.wordDict[word] = coef\n",
    "        assert self.word_dim == len(coef)\n",
    "\n",
    "    self.MakeWordEmbedding()\n",
    "\n",
    "  def MakeWordEmbedding(self):\n",
    "    sorted_x = sorted(self._word_to_id.items(), key=operator.itemgetter(1))\n",
    "    self._wordEmbedding = np.zeros((self.size(), self.word_dim),dtype=np.float32) # replace unknown words with UNKNOWN_TOKEN embedding (zero vector)\n",
    "    for word,i in sorted_x:\n",
    "      if word in self.wordDict:\n",
    "        self._wordEmbedding[i,:] = self.wordDict[word.lower()]\n",
    "    print('Word Embedding Reading done.')\n",
    "\n",
    "  def getWordEmbedding(self):\n",
    "    return self._wordEmbedding\n",
    "\n",
    "def example_generator(data_path, single_pass):\n",
    "  \n",
    "  while True:\n",
    "    import glob\n",
    "    filelist = glob.glob(data_path)\n",
    "    \n",
    "    if single_pass:\n",
    "      filelist = sorted(filelist)\n",
    "    else:\n",
    "      random.shuffle(filelist)\n",
    "    for f in filelist:\n",
    "      reader = open(f, 'rb')\n",
    "      while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        yield example_pb2.Example.FromString(example_str)\n",
    "    if single_pass:\n",
    "      print(\"example_generator completed reading all datafiles. No more data.\")\n",
    "      break\n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "  \n",
    "  ids = []\n",
    "  oovs = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in article_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is OOV\n",
    "      if w not in oovs: \n",
    "        oovs.append(w)\n",
    "      oov_num = oovs.index(w) \n",
    "      ids.append(vocab.size() + oov_num) \n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "  \n",
    "  ids = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in abstract_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: \n",
    "      if w in article_oovs: \n",
    "        vocab_idx = vocab.size() + article_oovs.index(w) \n",
    "        ids.append(vocab_idx)\n",
    "      else: \n",
    "        ids.append(unk_id) \n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids\n",
    "\n",
    "\n",
    "def outputids2words(id_list, vocab, article_oovs):\n",
    "  \n",
    "  words = []\n",
    "  for i in id_list:\n",
    "    try:\n",
    "      w = vocab.id2word(i) # might be [UNK]\n",
    "    except ValueError as e: # w is OOV\n",
    "      assert article_oovs is not None, \"Error: model produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"\n",
    "      article_oov_idx = i - vocab.size()\n",
    "      try:\n",
    "        w = article_oovs[article_oov_idx]\n",
    "      except ValueError as e: # i doesn't correspond to an article oov\n",
    "        raise ValueError('Error: model produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs' % (i, article_oov_idx, len(article_oovs)))\n",
    "    words.append(w)\n",
    "  return words\n",
    "\n",
    "\n",
    "def abstract2sents(abstract):\n",
    "  \n",
    "  abstract = abstract.encode(\"utf-8\").decode(encoding=\"utf-8\", errors=\"strict\")\n",
    "  cur = 0\n",
    "  sents = []\n",
    "  while True:\n",
    "    try:\n",
    "      # print(\"SENCENCE TYPE:\",type(SENTENCE_START))\n",
    "      # print(\"CUR TYPE:\", type(cur))\n",
    "      # print(\"ABSTRACT\", type(abstract))\n",
    "      start_p = abstract.index(SENTENCE_START,cur)\n",
    "      end_p = abstract.index(SENTENCE_END, start_p + 1)\n",
    "      cur = end_p + len(SENTENCE_END)\n",
    "      sents.append(abstract[start_p+len(SENTENCE_START):end_p])\n",
    "    except ValueError as e: # no more sentences\n",
    "      return sents\n",
    "\n",
    "\n",
    "def show_art_oovs(article, vocab):\n",
    "  \n",
    "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  words = article.split(' ')\n",
    "  words = [(\"__%s__\" % w) if vocab.word2id(w)==unk_token else w for w in words]\n",
    "  out_str = ' '.join(words)\n",
    "  return out_str\n",
    "\n",
    "\n",
    "def show_abs_oovs(abstract, vocab, article_oovs):\n",
    "  \n",
    "  unk_token = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  words = abstract.split(' ')\n",
    "  new_words = []\n",
    "  for w in words:\n",
    "    if vocab.word2id(w) == unk_token: # w is oov\n",
    "      if article_oovs is None: # baseline mode\n",
    "        new_words.append(\"__%s__\" % w)\n",
    "      else: # pointer-generator mode\n",
    "        if w in article_oovs:\n",
    "          new_words.append(\"__%s__\" % w)\n",
    "        else:\n",
    "          new_words.append(\"!!__%s__!!\" % w)\n",
    "    else: # w is in-vocab word\n",
    "      new_words.append(w)\n",
    "  out_str = ' '.join(new_words)\n",
    "  return out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uH-PxZaBIfQL"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "  import queue\n",
    "except:\n",
    "  import Queue as queue\n",
    "from random import shuffle\n",
    "from random import seed\n",
    "seed(123)\n",
    "from threading import Thread\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import data\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "class Example(object):\n",
    "  \"\"\"Class representing a train/val/test example for text summarization.\"\"\"\n",
    "\n",
    "  def __init__(self, article, abstract_sentences, vocab, hps):\n",
    "    \n",
    "    self.hps = hps\n",
    "\n",
    "    start_decoding = vocab.word2id(START_DECODING)\n",
    "    stop_decoding = vocab.word2id(STOP_DECODING)\n",
    "\n",
    "    article_words = article.split()\n",
    "    if len(article_words) > hps.max_enc_steps:\n",
    "      article_words = article_words[:hps.max_enc_steps]\n",
    "    self.enc_len = len(article_words) \n",
    "    self.enc_input = [vocab.word2id(w) for w in article_words] \n",
    "    \n",
    "    \n",
    "    abstract = ' '.join(abstract_sentences) # string\n",
    "    abstract_words = abstract.split() \n",
    "    abs_ids = [vocab.word2id(w) for w in abstract_words] \n",
    "\n",
    "    self.dec_input, self.target = self.get_dec_inp_targ_seqs(abs_ids, hps.max_dec_steps, start_decoding, stop_decoding)\n",
    "    self.dec_len = len(self.dec_input)\n",
    "\n",
    "    if hps.pointer_gen:\n",
    "      self.enc_input_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
    "\n",
    "      abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)\n",
    "\n",
    "      _, self.target = self.get_dec_inp_targ_seqs(abs_ids_extend_vocab, hps.max_dec_steps, start_decoding, stop_decoding)\n",
    "\n",
    "    self.original_article = article\n",
    "    self.original_abstract = abstract\n",
    "    self.original_abstract_sents = abstract_sentences\n",
    "\n",
    "\n",
    "  def get_dec_inp_targ_seqs(self, sequence, max_len, start_id, stop_id):\n",
    "    \n",
    "    inp = [start_id] + sequence[:]\n",
    "    target = sequence[:]\n",
    "    if len(inp) > max_len: # truncate\n",
    "      inp = inp[:max_len]\n",
    "      target = target[:max_len] # no end_token\n",
    "    else: # no truncation\n",
    "      target.append(stop_id) # end token\n",
    "    assert len(inp) == len(target)\n",
    "    return inp, target\n",
    "\n",
    "\n",
    "  def pad_decoder_inp_targ(self, max_len, pad_id):\n",
    "    while len(self.dec_input) < max_len:\n",
    "      self.dec_input.append(pad_id)\n",
    "    while len(self.target) < max_len:\n",
    "      self.target.append(pad_id)\n",
    "\n",
    "\n",
    "  def pad_encoder_input(self, max_len, pad_id):\n",
    "    while len(self.enc_input) < max_len:\n",
    "      self.enc_input.append(pad_id)\n",
    "    if self.hps.pointer_gen:\n",
    "      while len(self.enc_input_extend_vocab) < max_len:\n",
    "        self.enc_input_extend_vocab.append(pad_id)\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "\n",
    "  def __init__(self, example_list, hps, vocab):\n",
    "    \n",
    "    self.pad_id = vocab.word2id(PAD_TOKEN) # id of the PAD token used to pad sequences\n",
    "    self.init_encoder_seq(example_list, hps) # initialize the input to the encoder\n",
    "    self.init_decoder_seq(example_list, hps) # initialize the input and targets for the decoder\n",
    "    self.store_orig_strings(example_list) # store the original strings\n",
    "\n",
    "  def init_encoder_seq(self, example_list, hps):\n",
    "    \n",
    "    max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
    "\n",
    "    for ex in example_list:\n",
    "      ex.pad_encoder_input(max_enc_seq_len, self.pad_id)\n",
    "\n",
    "    self.enc_batch = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "    self.enc_lens = np.zeros((hps.batch_size), dtype=np.int32)\n",
    "    self.enc_padding_mask = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.float32)\n",
    "\n",
    "    for i, ex in enumerate(example_list):\n",
    "      self.enc_batch[i, :] = ex.enc_input[:]\n",
    "      self.enc_lens[i] = ex.enc_len\n",
    "      for j in range(ex.enc_len):\n",
    "        self.enc_padding_mask[i][j] = 1\n",
    "\n",
    "    if hps.pointer_gen:\n",
    "      # Determine the max number of in-article OOVs in this batch\n",
    "      self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
    "      # Store the in-article OOVs themselves\n",
    "      self.art_oovs = [ex.article_oovs for ex in example_list]\n",
    "      # Store the version of the enc_batch that uses the article OOV ids\n",
    "      self.enc_batch_extend_vocab = np.zeros((hps.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "      for i, ex in enumerate(example_list):\n",
    "        self.enc_batch_extend_vocab[i, :] = ex.enc_input_extend_vocab[:]\n",
    "\n",
    "  def init_decoder_seq(self, example_list, hps):\n",
    "    \n",
    "    for ex in example_list:\n",
    "      ex.pad_decoder_inp_targ(hps.max_dec_steps, self.pad_id)\n",
    "\n",
    "    self.dec_batch = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.int32)\n",
    "    self.target_batch = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.int32)\n",
    "    self.dec_padding_mask = np.zeros((hps.batch_size, hps.max_dec_steps), dtype=np.float32)\n",
    "\n",
    "    for i, ex in enumerate(example_list):\n",
    "      self.dec_batch[i, :] = ex.dec_input[:]\n",
    "      self.target_batch[i, :] = ex.target[:]\n",
    "      for j in range(ex.dec_len):\n",
    "        self.dec_padding_mask[i][j] = 1\n",
    "\n",
    "  def store_orig_strings(self, example_list):\n",
    "    self.original_articles = [ex.original_article for ex in example_list] # list of lists\n",
    "    self.original_abstracts = [ex.original_abstract for ex in example_list] # list of lists\n",
    "    self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list] # list of list of lists\n",
    "\n",
    "\n",
    "class Batcher(object):\n",
    "  \n",
    "  BATCH_QUEUE_MAX = 100 # max number of batches the batch_queue can hold\n",
    "\n",
    "  def __init__(self, data_path, vocab, hps, single_pass, decode_after):\n",
    "    \n",
    "    self._data_path = data_path\n",
    "    self._vocab = vocab\n",
    "    self._hps = hps\n",
    "    self._single_pass = single_pass\n",
    "    self._decode_after = decode_after\n",
    "\n",
    "    self._batch_queue = queue.Queue(self.BATCH_QUEUE_MAX)\n",
    "    self._example_queue = queue.Queue(self.BATCH_QUEUE_MAX * self._hps.batch_size)\n",
    "\n",
    "    if single_pass:\n",
    "      self._num_example_q_threads = 1 # just one thread, so we read through the dataset just once\n",
    "      self._num_batch_q_threads = 1  # just one thread to batch examples\n",
    "      self._bucketing_cache_size = 1 # only load one batch's worth of examples before bucketing; this essentially means no bucketing\n",
    "      self._finished_reading = False # this will tell us when we're finished reading the dataset\n",
    "    else:\n",
    "      self._num_example_q_threads = FLAGS.example_queue_threads # num threads to fill example queue\n",
    "      self._num_batch_q_threads = FLAGS.batch_queue_threads  # num threads to fill batch queue\n",
    "      self._bucketing_cache_size = FLAGS.bucketing_cache_size # how many batches-worth of examples to load into cache before bucketing\n",
    "\n",
    "    self._example_q_threads = []\n",
    "    for _ in range(self._num_example_q_threads):\n",
    "      self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
    "      self._example_q_threads[-1].daemon = True\n",
    "      self._example_q_threads[-1].start()\n",
    "    self._batch_q_threads = []\n",
    "    for _ in range(self._num_batch_q_threads):\n",
    "      self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
    "      self._batch_q_threads[-1].daemon = True\n",
    "      self._batch_q_threads[-1].start()\n",
    "\n",
    "    # Start a thread that watches the other threads and restarts them if they're dead\n",
    "    if not single_pass: # We don't want a watcher in single_pass mode because the threads shouldn't run forever\n",
    "      self._watch_thread = Thread(target=self.watch_threads)\n",
    "      self._watch_thread.daemon = True\n",
    "      self._watch_thread.start()\n",
    "\n",
    "  def next_batch(self):\n",
    "    if self._batch_queue.qsize() == 0:\n",
    "      tf.logging.warning('Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i', self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "      if self._single_pass and self._finished_reading:\n",
    "        tf.logging.info(\"Finished reading dataset in single_pass mode.\")\n",
    "        return None\n",
    "\n",
    "    batch = self._batch_queue.get() # get the next Batch\n",
    "    return batch\n",
    "\n",
    "  def fill_example_queue(self):\n",
    "    \"\"\"Reads data from file and processes into Examples which are then placed into the example queue.\"\"\"\n",
    "\n",
    "    input_gen = self.text_generator(example_generator(self._data_path, self._single_pass))\n",
    "\n",
    "    while True:\n",
    "      try:\n",
    "        (article, abstract) = next(input_gen)\n",
    "        article = article.decode(\"utf-8\")\n",
    "        abstract = abstract.decode(\"utf-8\")\n",
    "        \n",
    "      except StopIteration: \n",
    "        tf.logging.info(\"The example generator for this example queue filling thread has exhausted data.\")\n",
    "        if self._single_pass:\n",
    "          tf.logging.info(\"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n",
    "          self._finished_reading = True\n",
    "          break\n",
    "        else:\n",
    "          raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
    "\n",
    "      abstract_sentences = [sent.strip() for sent in abstract2sents(abstract)] # Use the <s> and </s> tags in abstract to get a list of sentences.\n",
    "      example = Example(article, abstract_sentences, self._vocab, self._hps) # Process into an Example.\n",
    "      self._example_queue.put(example) # place the Example in the example queue.\n",
    "\n",
    "  def fill_batch_queue(self):\n",
    "    \n",
    "    while True:\n",
    "      if self._hps.mode != 'decode':\n",
    "        inputs = []\n",
    "        for _ in range(self._hps.batch_size * self._bucketing_cache_size):\n",
    "          inputs.append(self._example_queue.get())\n",
    "        inputs = sorted(inputs, key=lambda inp: inp.enc_len) # sort by length of encoder sequence\n",
    "\n",
    "        batches = []\n",
    "        for i in range(0, len(inputs), self._hps.batch_size):\n",
    "          batches.append(inputs[i:i + self._hps.batch_size])\n",
    "        if not self._single_pass:\n",
    "          shuffle(batches)\n",
    "        for b in batches:  # each b is a list of Example objects\n",
    "          self._batch_queue.put(Batch(b, self._hps, self._vocab))\n",
    "\n",
    "      else: # beam search decode mode\n",
    "        ex = self._example_queue.get()\n",
    "        b = [ex for _ in range(self._hps.batch_size)]\n",
    "        self._batch_queue.put(Batch(b, self._hps, self._vocab))\n",
    "\n",
    "  def watch_threads(self):\n",
    "    \n",
    "    while True:\n",
    "      time.sleep(60)\n",
    "      for idx,t in enumerate(self._example_q_threads):\n",
    "        if not t.is_alive(): # if the thread is dead\n",
    "          tf.logging.error('Found example queue thread dead. Restarting.')\n",
    "          new_t = Thread(target=self.fill_example_queue)\n",
    "          self._example_q_threads[idx] = new_t\n",
    "          new_t.daemon = True\n",
    "          new_t.start()\n",
    "      for idx,t in enumerate(self._batch_q_threads):\n",
    "        if not t.is_alive(): # if the thread is dead\n",
    "          tf.logging.error('Found batch queue thread dead. Restarting.')\n",
    "          new_t = Thread(target=self.fill_batch_queue)\n",
    "          self._batch_q_threads[idx] = new_t\n",
    "          new_t.daemon = True\n",
    "          new_t.start()\n",
    "\n",
    "  def text_generator(self, example_generator):\n",
    "    \n",
    "    cnt = 0\n",
    "    while True:\n",
    "      e = next(example_generator) # e is a tf.Example\n",
    "      try:\n",
    "        article_text = e.features.feature['article'].bytes_list.value[0] # the article text was saved under the key 'article' in the data files\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0] # the abstract text was saved under the key 'abstract' in the data files\n",
    "      except ValueError:\n",
    "        tf.logging.error('Failed to get article or abstract from example')\n",
    "        continue\n",
    "      if len(article_text)==0:\n",
    "        tf.logging.warning('Found an example with empty article text. Skipping it.')\n",
    "      else:\n",
    "        if self._single_pass and cnt < self._decode_after: #skip already decoded docs\n",
    "          cnt +=1\n",
    "          continue\n",
    "        yield (article_text, abstract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d86tpuvTLjTD"
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "#pylint: disable=C0103\n",
    "\n",
    "\n",
    "def _get_ngrams(n, text):\n",
    "  \n",
    "  ngram_set = set()\n",
    "  text_length = len(text)\n",
    "  max_index_ngram_start = text_length - n\n",
    "  for i in range(max_index_ngram_start + 1):\n",
    "    ngram_set.add(tuple(text[i:i + n]))\n",
    "  return ngram_set\n",
    "\n",
    "def _split_into_words(sentences):\n",
    "  return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
    "\n",
    "def _get_word_ngrams(n, sentences):\n",
    "  \n",
    "  assert len(sentences) > 0\n",
    "  assert n > 0\n",
    "\n",
    "  words = _split_into_words(sentences)\n",
    "  return _get_ngrams(n, words)\n",
    "\n",
    "def _len_lcs(x, y):\n",
    "  \n",
    "  table = _lcs(x, y)\n",
    "  n, m = len(x), len(y)\n",
    "  return table[n, m]\n",
    "\n",
    "def _lcs(x, y):\n",
    "  \n",
    "  n, m = len(x), len(y)\n",
    "  table = dict()\n",
    "  for i in range(n + 1):\n",
    "    for j in range(m + 1):\n",
    "      if i == 0 or j == 0:\n",
    "        table[i, j] = 0\n",
    "      elif x[i - 1] == y[j - 1]:\n",
    "        table[i, j] = table[i - 1, j - 1] + 1\n",
    "      else:\n",
    "        table[i, j] = max(table[i - 1, j], table[i, j - 1])\n",
    "  return table\n",
    "\n",
    "def _recon_lcs(x, y):\n",
    "  \n",
    "  i, j = len(x), len(y)\n",
    "  table = _lcs(x, y)\n",
    "\n",
    "  def _recon(i, j):\n",
    "    \"\"\"private recon calculation\"\"\"\n",
    "    if i == 0 or j == 0:\n",
    "      return []\n",
    "    elif x[i - 1] == y[j - 1]:\n",
    "      return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n",
    "    elif table[i - 1, j] > table[i, j - 1]:\n",
    "      return _recon(i - 1, j)\n",
    "    else:\n",
    "      return _recon(i, j - 1)\n",
    "\n",
    "  recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n",
    "  return recon_tuple\n",
    "\n",
    "def rouge_n(evaluated_sentences, reference_sentences, n=2):\n",
    "  \n",
    "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
    "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
    "  reference_count = len(reference_ngrams)\n",
    "  evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "  overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "  if evaluated_count == 0:\n",
    "    precision = 0.0\n",
    "  else:\n",
    "    precision = overlapping_count / evaluated_count\n",
    "\n",
    "  if reference_count == 0:\n",
    "    recall = 0.0\n",
    "  else:\n",
    "    recall = overlapping_count / reference_count\n",
    "\n",
    "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "\n",
    "  return f1_score, precision, recall\n",
    "\n",
    "def _f_p_r_lcs(llcs, m, n):\n",
    "  \n",
    "  r_lcs = llcs / m\n",
    "  p_lcs = llcs / n\n",
    "  beta = p_lcs / (r_lcs + 1e-12)\n",
    "  num = (1 + (beta**2)) * r_lcs * p_lcs\n",
    "  denom = r_lcs + ((beta**2) * p_lcs)\n",
    "  f_lcs = num / (denom + 1e-12)\n",
    "  return f_lcs, p_lcs, r_lcs\n",
    "\n",
    "\n",
    "def rouge_l_sentence_level(evaluated_sentences, reference_sentences):\n",
    "  \n",
    "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "  reference_words = _split_into_words(reference_sentences)\n",
    "  evaluated_words = _split_into_words(evaluated_sentences)\n",
    "  m = len(reference_words)\n",
    "  n = len(evaluated_words)\n",
    "  lcs = _len_lcs(evaluated_words, reference_words)\n",
    "  return _f_p_r_lcs(lcs, m, n)\n",
    "\n",
    "\n",
    "def _union_lcs(evaluated_sentences, reference_sentence):\n",
    "  \n",
    "  if len(evaluated_sentences) <= 0:\n",
    "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "  lcs_union = set()\n",
    "  reference_words = _split_into_words([reference_sentence])\n",
    "  combined_lcs_length = 0\n",
    "  for eval_s in evaluated_sentences:\n",
    "    evaluated_words = _split_into_words([eval_s])\n",
    "    lcs = set(_recon_lcs(reference_words, evaluated_words))\n",
    "    combined_lcs_length += len(lcs)\n",
    "    lcs_union = lcs_union.union(lcs)\n",
    "\n",
    "  union_lcs_count = len(lcs_union)\n",
    "  union_lcs_value = union_lcs_count / combined_lcs_length\n",
    "  return union_lcs_value\n",
    "\n",
    "\n",
    "def rouge_l_summary_level(evaluated_sentences, reference_sentences):\n",
    "  \n",
    "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "  m = len(_split_into_words(reference_sentences))\n",
    "\n",
    "  n = len(_split_into_words(evaluated_sentences))\n",
    "\n",
    "  union_lcs_sum_across_all_references = 0\n",
    "  for ref_s in reference_sentences:\n",
    "    union_lcs_sum_across_all_references += _union_lcs(evaluated_sentences,\n",
    "                                                      ref_s)\n",
    "  return _f_p_r_lcs(union_lcs_sum_across_all_references, m, n)\n",
    "\n",
    "\n",
    "def rouge(hypotheses, references):\n",
    "  \n",
    "  rouge_1 = [\n",
    "      rouge_n([hyp], [ref], 1) for hyp, ref in zip(hypotheses, references)\n",
    "  ]\n",
    "  rouge_1_f, rouge_1_p, rouge_1_r = map(np.mean, zip(*rouge_1))\n",
    "\n",
    "  rouge_2 = [\n",
    "      rouge_n([hyp], [ref], 2) for hyp, ref in zip(hypotheses, references)\n",
    "  ]\n",
    "  rouge_2_f, rouge_2_p, rouge_2_r = map(np.mean, zip(*rouge_2))\n",
    "\n",
    "  rouge_l = [\n",
    "      rouge_l_sentence_level([hyp], [ref])\n",
    "      for hyp, ref in zip(hypotheses, references)\n",
    "  ]\n",
    "  rouge_l_f, rouge_l_p, rouge_l_r = map(np.mean, zip(*rouge_l))\n",
    "\n",
    "  return {\n",
    "      \"rouge_1/f_score\": rouge_1_f,\n",
    "      \"rouge_1/r_score\": rouge_1_r,\n",
    "      \"rouge_1/p_score\": rouge_1_p,\n",
    "      \"rouge_2/f_score\": rouge_2_f,\n",
    "      \"rouge_2/r_score\": rouge_2_r,\n",
    "      \"rouge_2/p_score\": rouge_2_p,\n",
    "      \"rouge_l/f_score\": rouge_l_f,\n",
    "      \"rouge_l/r_score\": rouge_l_r,\n",
    "      \"rouge_l/p_score\": rouge_l_p,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtrJ_qoPK7lb"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def _len_lcs(x, y):\n",
    "  \n",
    "  table = _lcs(x, y)\n",
    "  n, m = len(x), len(y)\n",
    "  return table[n, m]\n",
    "\n",
    "\n",
    "def _lcs(x, y):\n",
    "  \n",
    "  n, m = len(x), len(y)\n",
    "  table = dict()\n",
    "  for i in range(n + 1):\n",
    "    for j in range(m + 1):\n",
    "      if i == 0 or j == 0:\n",
    "        table[i, j] = 0\n",
    "      elif x[i - 1] == y[j - 1]:\n",
    "        table[i, j] = table[i - 1, j - 1] + 1\n",
    "      else:\n",
    "        table[i, j] = max(table[i - 1, j], table[i, j - 1])\n",
    "  return table\n",
    "\n",
    "\n",
    "def _f_lcs(llcs, m, n):\n",
    "  \n",
    "  r_lcs = llcs / m\n",
    "  p_lcs = llcs / n\n",
    "  beta = p_lcs / (r_lcs + 1e-12)\n",
    "  num = (1 + (beta**2)) * r_lcs * p_lcs\n",
    "  denom = r_lcs + ((beta**2) * p_lcs)\n",
    "  f_lcs = num / (denom + 1e-12)\n",
    "  return f_lcs\n",
    "\n",
    "\n",
    "def rouge_l_sentence_level(eval_sentences, ref_sentences):\n",
    "  \n",
    "\n",
    "  f1_scores = []\n",
    "  for eval_sentence, ref_sentence in zip(eval_sentences, ref_sentences):\n",
    "    m = len(ref_sentence)\n",
    "    n = len(eval_sentence)\n",
    "    lcs = _len_lcs(eval_sentence, ref_sentence)\n",
    "    f1_scores.append(_f_lcs(lcs, m, n))\n",
    "  return np.array(f1_scores).astype(np.float32)\n",
    "\n",
    "\n",
    "def rouge_l_fscore(hypothesis, references, **unused_kwargs):\n",
    "  \n",
    "  rouge_l_f_score = tf.py_func(rouge_l_sentence_level, (hypothesis, references), [tf.float32])\n",
    "  return rouge_l_f_score\n",
    "\n",
    "\n",
    "def _get_ngrams(n, text):\n",
    "  \n",
    "  ngram_set = set()\n",
    "  text_length = len(text)\n",
    "  max_index_ngram_start = text_length - n\n",
    "  for i in range(max_index_ngram_start + 1):\n",
    "    ngram_set.add(tuple(text[i:i + n]))\n",
    "  return ngram_set\n",
    "\n",
    "\n",
    "def rouge_n(eval_sentences, ref_sentences, n=2):\n",
    "  \n",
    "\n",
    "  f1_scores = []\n",
    "  for eval_sentence, ref_sentence in zip(eval_sentences, ref_sentences):\n",
    "    eval_ngrams = _get_ngrams(n, eval_sentence)\n",
    "    ref_ngrams = _get_ngrams(n, ref_sentence)\n",
    "    ref_count = len(ref_ngrams)\n",
    "    eval_count = len(eval_ngrams)\n",
    "\n",
    "    # Gets the overlapping ngrams between evaluated and reference\n",
    "    overlapping_ngrams = eval_ngrams.intersection(ref_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    # Handle edge case. This isn't mathematically correct, but it's good enough\n",
    "    if eval_count == 0:\n",
    "      precision = 0.0\n",
    "    else:\n",
    "      precision = overlapping_count / eval_count\n",
    "\n",
    "    if ref_count == 0:\n",
    "      recall = 0.0\n",
    "    else:\n",
    "      recall = overlapping_count / ref_count\n",
    "\n",
    "    f1_scores.append(2.0 * ((precision * recall) / (precision + recall + 1e-8)))\n",
    "\n",
    "  return np.array(f1_scores).astype(np.float32)\n",
    "\n",
    "\n",
    "def rouge_2_fscore(predictions, labels, **unused_kwargs):\n",
    "  \n",
    "\n",
    "  rouge_2_f_score = tf.py_func(rouge_n, (predictions, labels), [tf.float32])\n",
    "  return rouge_2_f_score, tf.constant(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbSu87w-KxiM"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops.distributions import categorical\n",
    "from tensorflow.python.ops.distributions import bernoulli\n",
    "#from rouge_tensor import rouge_l_fscore\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "def print_shape(str, var):\n",
    "  tf.logging.info('shape of {}: {}'.format(str, [k for k in var.get_shape()]))\n",
    "\n",
    "\n",
    "def _calc_final_dist(_hps, v_size, _max_art_oovs, _enc_batch_extend_vocab, p_gen, vocab_dist, attn_dist):\n",
    "  \n",
    "  with tf.variable_scope('final_distribution'):\n",
    "    vocab_dist = p_gen * vocab_dist\n",
    "    attn_dist = (1-p_gen) * attn_dist\n",
    "\n",
    "    extended_vsize = v_size + _max_art_oovs # the maximum (over the batch) size of the extended vocabulary\n",
    "    extra_zeros = tf.zeros((_hps.batch_size, _max_art_oovs))\n",
    "    vocab_dists_extended = tf.concat(axis=1, values=[vocab_dist, extra_zeros]) # list length max_dec_steps of shape (batch_size, extended_vsize)\n",
    "\n",
    "    batch_nums = tf.range(0, limit=_hps.batch_size) # shape (batch_size)\n",
    "    batch_nums = tf.expand_dims(batch_nums, 1) # shape (batch_size, 1)\n",
    "    attn_len = tf.shape(_enc_batch_extend_vocab)[1] # number of states we attend over\n",
    "    batch_nums = tf.tile(batch_nums, [1, attn_len]) # shape (batch_size, attn_len)\n",
    "    indices = tf.stack( (batch_nums, _enc_batch_extend_vocab), axis=2) # shape (batch_size, enc_t, 2)\n",
    "    shape = [_hps.batch_size, extended_vsize]\n",
    "    attn_dists_projected = tf.scatter_nd(indices, attn_dist, shape) # list length max_dec_steps (batch_size, extended_vsize)\n",
    "\n",
    "    final_dist = vocab_dists_extended + attn_dists_projected\n",
    "    final_dist +=1e-15 # for cases where we have zero in the final dist, especially for oov words\n",
    "    dist_sums = tf.reduce_sum(final_dist, axis=1)\n",
    "    final_dist = final_dist / tf.reshape(dist_sums, [-1, 1]) # re-normalize\n",
    "\n",
    "  return final_dist\n",
    "\n",
    "def attention_decoder(_hps, \n",
    "  v_size, \n",
    "  _max_art_oovs, \n",
    "  _enc_batch_extend_vocab, \n",
    "  emb_dec_inputs,\n",
    "  target_batch,\n",
    "  _dec_in_state, \n",
    "  _enc_states, \n",
    "  enc_padding_mask, \n",
    "  dec_padding_mask, \n",
    "  cell, \n",
    "  embedding, \n",
    "  sampling_probability,\n",
    "  alpha,\n",
    "  unk_id,\n",
    "  initial_state_attention=False,\n",
    "  pointer_gen=True, \n",
    "  use_coverage=False, \n",
    "  prev_coverage=None, \n",
    "  prev_decoder_outputs=[], \n",
    "  prev_encoder_es = []):\n",
    "  \n",
    "  with variable_scope.variable_scope(\"attention_decoder\") as scope:\n",
    "    batch_size = _enc_states.get_shape()[0] # if this line fails, it's because the batch size isn't defined\n",
    "    attn_size = _enc_states.get_shape()[2] # if this line fails, it's because the attention length isn't defined\n",
    "    emb_size = emb_dec_inputs[0].get_shape()[1] # if this line fails, it's because the embedding isn't defined\n",
    "    decoder_attn_size = _dec_in_state.c.get_shape()[1]\n",
    "    tf.logging.info(\"batch_size %i, attn_size: %i, emb_size: %i\", batch_size, attn_size, emb_size)\n",
    "    # Reshape _enc_states (need to insert a dim)\n",
    "    _enc_states = tf.expand_dims(_enc_states, axis=2) # now is shape (batch_size, max_enc_steps, 1, attn_size)\n",
    "\n",
    "    attention_vec_size = attn_size\n",
    "\n",
    "    if _hps.matrix_attention:\n",
    "      w_attn = variable_scope.get_variable(\"w_attn\", [attention_vec_size, attention_vec_size])\n",
    "      if _hps.intradecoder:\n",
    "        w_dec_attn = variable_scope.get_variable(\"w_dec_attn\", [decoder_attn_size, decoder_attn_size])\n",
    "    else:\n",
    "      W_h = variable_scope.get_variable(\"W_h\", [1, 1, attn_size, attention_vec_size])\n",
    "      v = variable_scope.get_variable(\"v\", [attention_vec_size])\n",
    "      encoder_features = nn_ops.conv2d(_enc_states, W_h, [1, 1, 1, 1], \"SAME\") # shape (batch_size,max_enc_steps,1,attention_vec_size)\n",
    "    if _hps.intradecoder:\n",
    "      W_h_d = variable_scope.get_variable(\"W_h_d\", [1, 1, decoder_attn_size, decoder_attn_size])\n",
    "      v_d = variable_scope.get_variable(\"v_d\", [decoder_attn_size])\n",
    "\n",
    "    if use_coverage:\n",
    "      with variable_scope.variable_scope(\"coverage\"):\n",
    "        w_c = variable_scope.get_variable(\"w_c\", [1, 1, 1, attention_vec_size])\n",
    "\n",
    "    if prev_coverage is not None: # for beam search mode with coverage\n",
    "      prev_coverage = tf.expand_dims(tf.expand_dims(prev_coverage,2),3)\n",
    "\n",
    "    def attention(decoder_state, temporal_e, coverage=None):\n",
    "      \n",
    "      with variable_scope.variable_scope(\"Attention\"):\n",
    "        # Pass the decoder state through a linear layer (this is W_s s_t + b_attn in the paper)\n",
    "        decoder_features = linear(decoder_state, attention_vec_size, True) # shape (batch_size, attention_vec_size)\n",
    "        decoder_features = tf.expand_dims(tf.expand_dims(decoder_features, 1), 1) # reshape to (batch_size, 1, 1, attention_vec_size)\n",
    "\n",
    "        # We can't have coverage with matrix attention\n",
    "        if not _hps.matrix_attention and use_coverage and coverage is not None: # non-first step of coverage\n",
    "          # Multiply coverage vector by w_c to get coverage_features.\n",
    "          coverage_features = nn_ops.conv2d(coverage, w_c, [1, 1, 1, 1], \"SAME\") # c has shape (batch_size, max_enc_steps, 1, attention_vec_size)\n",
    "          # Calculate v^T tanh(W_h h_i + W_s s_t + w_c c_i^t + b_attn)\n",
    "          e_not_masked = math_ops.reduce_sum(v * math_ops.tanh(encoder_features + decoder_features + coverage_features), [2, 3])  # shape (batch_size,max_enc_steps)\n",
    "          masked_e = nn_ops.softmax(e_not_masked) * enc_padding_mask # (batch_size, max_enc_steps)\n",
    "          masked_sums = tf.reduce_sum(masked_e, axis=1) # shape (batch_size)\n",
    "          masked_e = masked_e / tf.reshape(masked_sums, [-1, 1])\n",
    "          # Equation 3 in \n",
    "          if _hps.use_temporal_attention:\n",
    "            try:\n",
    "              len_temporal_e = temporal_e.get_shape()[0]\n",
    "            except:\n",
    "              len_temporal_e = 0\n",
    "            if len_temporal_e==0:\n",
    "              attn_dist = masked_e\n",
    "            else:\n",
    "              masked_sums = tf.reduce_sum(temporal_e,axis=0)+1e-10 # if it's zero due to masking we set it to a small value\n",
    "              attn_dist = masked_e / masked_sums # (batch_size, max_enc_steps)\n",
    "          else:\n",
    "            attn_dist = masked_e\n",
    "          masked_attn_sums = tf.reduce_sum(attn_dist, axis=1)\n",
    "          attn_dist = attn_dist / tf.reshape(masked_attn_sums, [-1, 1]) # re-normalize\n",
    "          # Update coverage vector\n",
    "          coverage += array_ops.reshape(attn_dist, [batch_size, -1, 1, 1])\n",
    "        else:\n",
    "          if _hps.matrix_attention:\n",
    "            # Calculate h_d * W_attn * h_i, equation 2 in https://arxiv.org/pdf/1705.04304.pdf\n",
    "            _dec_attn = tf.unstack(tf.matmul(tf.squeeze(decoder_features,axis=[1,2]),w_attn),axis=0) # batch_size * (attention_vec_size)\n",
    "            _enc_states_lst = tf.unstack(tf.squeeze(_enc_states,axis=2),axis=0) # batch_size * (max_enc_steps, attention_vec_size)\n",
    "\n",
    "            e_not_masked = tf.squeeze(tf.stack([tf.matmul(tf.reshape(_dec,[1,-1]), tf.transpose(_enc)) for _dec, _enc in zip(_dec_attn,_enc_states_lst)]),axis=1) # (batch_size, max_enc_steps)\n",
    "            masked_e = tf.exp(e_not_masked * enc_padding_mask) # (batch_size, max_enc_steps)\n",
    "          else:\n",
    "            # Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)\n",
    "            e_not_masked = math_ops.reduce_sum(v * math_ops.tanh(encoder_features + decoder_features), [2, 3]) # calculate e, (batch_size, max_enc_steps)\n",
    "            masked_e = nn_ops.softmax(e_not_masked) * enc_padding_mask # (batch_size, max_enc_steps)\n",
    "            masked_sums = tf.reduce_sum(masked_e, axis=1) # shape (batch_size)\n",
    "            masked_e = masked_e / tf.reshape(masked_sums, [-1, 1])\n",
    "          if _hps.use_temporal_attention:\n",
    "            try:\n",
    "              len_temporal_e = temporal_e.get_shape()[0]\n",
    "            except:\n",
    "              len_temporal_e = 0\n",
    "            if len_temporal_e==0:\n",
    "              attn_dist = masked_e\n",
    "            else:\n",
    "              masked_sums = tf.reduce_sum(temporal_e,axis=0)+1e-10 # if it's zero due to masking we set it to a small value\n",
    "              attn_dist = masked_e / masked_sums # (batch_size, max_enc_steps)\n",
    "          else:\n",
    "            attn_dist = masked_e\n",
    "          masked_attn_sums = tf.reduce_sum(attn_dist, axis=1)\n",
    "          attn_dist = attn_dist / tf.reshape(masked_attn_sums, [-1, 1]) # re-normalize\n",
    "\n",
    "          if use_coverage: # first step of training\n",
    "            coverage = tf.expand_dims(tf.expand_dims(attn_dist,2),2) # initialize coverage\n",
    "\n",
    "        context_vector = math_ops.reduce_sum(array_ops.reshape(attn_dist, [batch_size, -1, 1, 1]) * _enc_states, [1, 2]) # shape (batch_size, attn_size).\n",
    "        context_vector = array_ops.reshape(context_vector, [-1, attn_size])\n",
    "\n",
    "      return context_vector, attn_dist, coverage, masked_e\n",
    "\n",
    "    def intra_decoder_attention(decoder_state, outputs):\n",
    "      \n",
    "      attention_dec_vec_size = attn_dec_size = decoder_state.c.get_shape()[1] # hidden_dim\n",
    "      try:\n",
    "        len_dec_states = outputs.get_shape()[0]\n",
    "      except:\n",
    "        len_dec_states = 0\n",
    "      attention_dec_vec_size = attn_dec_size = decoder_state.c.get_shape()[1] # hidden_dim\n",
    "      _decoder_states = tf.expand_dims(tf.reshape(outputs,[batch_size,-1,attn_dec_size]), axis=2) # now is shape (batch_size,len(decoder_states), 1, attn_size)\n",
    "      _prev_decoder_features = nn_ops.conv2d(_decoder_states, W_h_d, [1, 1, 1, 1], \"SAME\") # shape (batch_size,len(decoder_states),1,attention_vec_size)\n",
    "      with variable_scope.variable_scope(\"DecoderAttention\"):\n",
    "        # Pass the decoder state through a linear layer (this is W_s s_t + b_attn in the paper)\n",
    "        try:\n",
    "          decoder_features = linear(decoder_state, attention_dec_vec_size, True) # shape (batch_size, attention_vec_size)\n",
    "          decoder_features = tf.expand_dims(tf.expand_dims(decoder_features, 1), 1) # reshape to (batch_size, 1, 1, attention_dec_vec_size)\n",
    "          # Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)\n",
    "          if _hps.matrix_attention:\n",
    "            # Calculate h_d * W_attn * h_d, equation 6 in https://arxiv.org/pdf/1705.04304.pdf\n",
    "            _dec_attn = tf.matmul(tf.squeeze(decoder_features),w_dec_attn) # (batch_size, decoder_attn_size)\n",
    "            _dec_states_lst = tf.unstack(tf.reshape(_prev_decoder_features,[batch_size,-1,decoder_attn_size])) # batch_size * (len(decoder_states), decoder_attn_size)\n",
    "            e_not_masked = tf.reshape(tf.stack([tf.matmul(_dec_attn, tf.transpose(k)) for k in _dec_states_lst]),[batch_size,-1]) # (batch_size, len(decoder_states))\n",
    "            masked_e = tf.exp(e_not_masked * dec_padding_mask[:,:len_dec_states]) # (batch_size, len(decoder_states))\n",
    "          else:\n",
    "            # Calculate v^T tanh(W_h h_i + W_s s_t + b_attn)\n",
    "            e_not_masked = math_ops.reduce_sum(v_d * math_ops.tanh(_prev_decoder_features + decoder_features), [2, 3]) # calculate e, (batch_size,len(decoder_states))\n",
    "            masked_e = nn_ops.softmax(e_not_masked) * dec_padding_mask[:,:len_dec_states] # (batch_size,len(decoder_states))\n",
    "          if len_dec_states <= 1:\n",
    "            masked_e = array_ops.ones([batch_size,1]) # first step is filled with equal values\n",
    "          masked_sums = tf.reshape(tf.reduce_sum(masked_e,axis=1),[-1,1]) # (batch_size,1), # if it's zero due to masking we set it to a small value\n",
    "          decoder_attn_dist = masked_e / masked_sums # (batch_size,len(decoder_states))\n",
    "          context_decoder_vector = math_ops.reduce_sum(array_ops.reshape(decoder_attn_dist, [batch_size, -1, 1, 1]) * _decoder_states, [1, 2]) # (batch_size, attn_size)\n",
    "          context_decoder_vector = array_ops.reshape(context_decoder_vector, [-1, attn_dec_size]) # (batch_size, attn_size)\n",
    "        except:\n",
    "          return array_ops.zeros([batch_size, decoder_attn_size]), array_ops.zeros([batch_size, 0])\n",
    "      return context_decoder_vector, decoder_attn_dist\n",
    "\n",
    "    outputs = []\n",
    "    temporal_e = []\n",
    "    attn_dists = []\n",
    "    vocab_scores = []\n",
    "    vocab_dists = []\n",
    "    final_dists = []\n",
    "    p_gens = []\n",
    "    samples = [] # this holds the words chosen by sampling based on the final distribution for each decoding step, list of max_dec_steps of (batch_size, 1)\n",
    "    greedy_search_samples = [] # this holds the words chosen by greedy search (taking the max) on the final distribution for each decoding step, list of max_dec_steps of (batch_size, 1)\n",
    "    sampling_rewards = [] # list of size max_dec_steps (batch_size, k)\n",
    "    greedy_rewards = [] # list of size max_dec_steps (batch_size, k)\n",
    "    state = _dec_in_state\n",
    "    coverage = prev_coverage # initialize coverage to None or whatever was passed in\n",
    "    context_vector = array_ops.zeros([batch_size, attn_size])\n",
    "    context_decoder_vector = array_ops.zeros([batch_size, decoder_attn_size])\n",
    "    context_vector.set_shape([None, attn_size])  # Ensure the second shape of attention vectors is set.\n",
    "    if initial_state_attention: # true in decode mode\n",
    "      # Re-calculate the context vector from the previous step so that we can pass it through a linear layer with this step's input to get a modified version of the input\n",
    "      context_vector, _, coverage, _ = attention(_dec_in_state, tf.stack(prev_encoder_es,axis=0), coverage) # in decode mode, this is what updates the coverage vector\n",
    "      if _hps.intradecoder:\n",
    "        context_decoder_vector, _ = intra_decoder_attention(_dec_in_state, tf.stack(prev_decoder_outputs,axis=0))\n",
    "    for i, inp in enumerate(emb_dec_inputs):\n",
    "      tf.logging.info(\"Adding attention_decoder timestep %i of %i\", i, len(emb_dec_inputs))\n",
    "      \n",
    "      \n",
    "      if i > 0:\n",
    "        variable_scope.get_variable_scope().reuse_variables()\n",
    "\n",
    "      if _hps.mode in ['train','eval'] and _hps.scheduled_sampling and i > 0: # start scheduled sampling after we received the first decoder's output\n",
    "        # modify the input to next decoder using scheduled sampling\n",
    "        if FLAGS.scheduled_sampling_final_dist:\n",
    "          inp = scheduled_sampling(_hps, sampling_probability, final_dist, embedding, inp, alpha)\n",
    "        else:\n",
    "          inp = scheduled_sampling_vocab_dist(_hps, sampling_probability, vocab_dist, embedding, inp, alpha)\n",
    "\n",
    "      emb_dim = inp.get_shape().with_rank(2)[1]\n",
    "      if emb_dim is None:\n",
    "        raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
    "\n",
    "      x = linear([inp] + [context_vector], emb_dim, True)\n",
    "      cell_output, state = cell(x, state)\n",
    "\n",
    "      if i == 0 and initial_state_attention:  # always true in decode mode\n",
    "        with variable_scope.variable_scope(variable_scope.get_variable_scope()):#, reuse=True): # you need this because you've already run the initial attention(...) call\n",
    "          context_vector, attn_dist, _, masked_e = attention(state, tf.stack(prev_encoder_es,axis=0), coverage) # don't allow coverage to update\n",
    "          if _hps.intradecoder:\n",
    "            context_decoder_vector, _ = intra_decoder_attention(state, tf.stack(prev_decoder_outputs,axis=0))\n",
    "      else:\n",
    "        context_vector, attn_dist, coverage, masked_e = attention(state, tf.stack(temporal_e,axis=0), coverage)\n",
    "        if _hps.intradecoder:\n",
    "          context_decoder_vector, _ = intra_decoder_attention(state, tf.stack(outputs,axis=0))\n",
    "      attn_dists.append(attn_dist)\n",
    "      temporal_e.append(masked_e)\n",
    "\n",
    "      with variable_scope.variable_scope(\"combined_context\"):\n",
    "        if _hps.intradecoder:\n",
    "          context_vector = linear([context_vector] + [context_decoder_vector], attn_size, False)\n",
    "      # Calculate p_gen\n",
    "      if pointer_gen:\n",
    "        with tf.variable_scope('calculate_pgen'):\n",
    "          p_gen = linear([context_vector, state.c, state.h, x], 1, True) # Tensor shape (batch_size, 1)\n",
    "          p_gen = tf.sigmoid(p_gen)\n",
    "          p_gens.append(p_gen)\n",
    "\n",
    "      with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "        output = linear([cell_output] + [context_vector], cell.output_size, True)\n",
    "      outputs.append(output)\n",
    "\n",
    "      with tf.variable_scope('output_projection'):\n",
    "        if i > 0:\n",
    "          tf.get_variable_scope().reuse_variables()\n",
    "        trunc_norm_init = tf.truncated_normal_initializer(stddev=_hps.trunc_norm_init_std)\n",
    "        w_out = tf.get_variable('w', [_hps.dec_hidden_dim, v_size], dtype=tf.float32, initializer=trunc_norm_init)\n",
    "        #w_t_out = tf.transpose(w)\n",
    "        v_out = tf.get_variable('v', [v_size], dtype=tf.float32, initializer=trunc_norm_init)\n",
    "        if i > 0:\n",
    "          tf.get_variable_scope().reuse_variables()\n",
    "        if FLAGS.share_decoder_weights: # Eq. 13 in https://arxiv.org/pdf/1705.04304.pdf\n",
    "          w_out = tf.transpose(\n",
    "            math_ops.tanh(linear([embedding] + [tf.transpose(w_out)], _hps.dec_hidden_dim, bias=False)))\n",
    "        score = tf.nn.xw_plus_b(output, w_out, v_out)\n",
    "        if _hps.scheduled_sampling and not _hps.greedy_scheduled_sampling:\n",
    "          # Gumbel reparametrization trick: https://arxiv.org/abs/1704.06970\n",
    "          U = tf.random_uniform(score.get_shape(),10e-12,(1-10e-12)) # add a small number to avoid log(0)\n",
    "          G = -tf.log(-tf.log(U))\n",
    "          score = score + G\n",
    "        vocab_scores.append(score) # apply the linear layer\n",
    "        vocab_dist = tf.nn.softmax(score)\n",
    "        vocab_dists.append(vocab_dist) # The vocabulary distributions. List length max_dec_steps of (batch_size, vsize) arrays. The words are in the order they appear in the vocabulary file.\n",
    "\n",
    "      if _hps.pointer_gen:\n",
    "        final_dist = _calc_final_dist(_hps, v_size, _max_art_oovs, _enc_batch_extend_vocab, p_gen, vocab_dist,\n",
    "                                      attn_dist)\n",
    "      else: # final distribution is just vocabulary distribution\n",
    "        final_dist = vocab_dist\n",
    "      final_dists.append(final_dist)\n",
    "\n",
    "      one_hot_k_samples = tf.distributions.Multinomial(total_count=1., probs=final_dist).sample(\n",
    "        _hps.k)  # sample k times according to https://arxiv.org/pdf/1705.04304.pdf, size (k, batch_size, extended_vsize)\n",
    "      k_argmax = tf.argmax(one_hot_k_samples, axis=2, output_type=tf.int32) # (k, batch_size)\n",
    "      k_sample = tf.transpose(k_argmax) # shape (batch_size, k)\n",
    "      greedy_search_prob, greedy_search_sample = tf.nn.top_k(final_dist, k=_hps.k) # (batch_size, k)\n",
    "      greedy_search_samples.append(greedy_search_sample)\n",
    "      samples.append(k_sample)\n",
    "      if FLAGS.use_discounted_rewards:\n",
    "        _sampling_rewards = []\n",
    "        _greedy_rewards = []\n",
    "        for _ in range(_hps.k):\n",
    "          rl_fscore = tf.reshape(rouge_l_fscore(tf.transpose(tf.stack(samples)[:, :, _]), target_batch),\n",
    "                                 [-1, 1])  # shape (batch_size, 1)\n",
    "          _sampling_rewards.append(tf.reshape(rl_fscore, [-1, 1]))\n",
    "          rl_fscore = tf.reshape(rouge_l_fscore(tf.transpose(tf.stack(greedy_search_samples)[:, :, _]), target_batch),\n",
    "                                 [-1, 1])  # shape (batch_size, 1)\n",
    "          _greedy_rewards.append(tf.reshape(rl_fscore, [-1, 1]))\n",
    "        sampling_rewards.append(tf.squeeze(tf.stack(_sampling_rewards, axis=1), axis = -1)) # (batch_size, k)\n",
    "        greedy_rewards.append(tf.squeeze(tf.stack(_greedy_rewards, axis=1), axis = -1))  # (batch_size, k)\n",
    "\n",
    "    if FLAGS.use_discounted_rewards:\n",
    "      sampling_rewards = tf.stack(sampling_rewards)\n",
    "      greedy_rewards = tf.stack(greedy_rewards)\n",
    "    else:\n",
    "      _sampling_rewards = []\n",
    "      _greedy_rewards = []\n",
    "      for _ in range(_hps.k):\n",
    "        rl_fscore = rouge_l_fscore(tf.transpose(tf.stack(samples)[:, :, _]), target_batch) # shape (batch_size, 1)\n",
    "        _sampling_rewards.append(tf.reshape(rl_fscore, [-1, 1]))\n",
    "        rl_fscore = rouge_l_fscore(tf.transpose(tf.stack(greedy_search_samples)[:, :, _]), target_batch)  # shape (batch_size, 1)\n",
    "        _greedy_rewards.append(tf.reshape(rl_fscore, [-1, 1]))\n",
    "      sampling_rewards = tf.squeeze(tf.stack(_sampling_rewards, axis=1), axis=-1) # (batch_size, k)\n",
    "      greedy_rewards = tf.squeeze(tf.stack(_greedy_rewards, axis=1), axis=-1) # (batch_size, k)\n",
    "    if coverage is not None:\n",
    "      coverage = array_ops.reshape(coverage, [batch_size, -1])\n",
    "\n",
    "  return (\n",
    "  outputs, state, attn_dists, p_gens, coverage, vocab_scores, final_dists, samples, greedy_search_samples, temporal_e,\n",
    "  sampling_rewards, greedy_rewards)\n",
    "\n",
    "def scheduled_sampling(hps, sampling_probability, output, embedding, inp, alpha = 0):\n",
    "  vocab_size = embedding.get_shape()[0]\n",
    "\n",
    "  def soft_argmax(alpha, _output):\n",
    "    new_oov_scores = tf.reshape(_output[:, 0] + tf.reduce_sum(_output[:, vocab_size:], axis=1),\n",
    "                                [-1, 1])  # add score for all OOV to the UNK score\n",
    "    _output = tf.concat([new_oov_scores, _output[:, 1:vocab_size]], axis=1) # select only the vocab_size outputs\n",
    "    _output = _output / tf.reshape(tf.reduce_sum(output, axis=1), [-1, 1]) # re-normalize scores\n",
    "\n",
    "    one_hot_scores = tf.nn.softmax((alpha * _output))\n",
    "    return one_hot_scores\n",
    "\n",
    "  def soft_top_k(alpha, _output, K):\n",
    "    copy = tf.identity(_output)\n",
    "    p = []\n",
    "    arg_top_k = []\n",
    "    for k in range(K):\n",
    "      sargmax = soft_argmax(alpha, copy)\n",
    "      copy = (1-sargmax)* copy\n",
    "      p.append(tf.reduce_sum(sargmax * _output, axis=1))\n",
    "      arg_top_k.append(sargmax)\n",
    "\n",
    "    return tf.stack(p, axis=1), tf.stack(arg_top_k)\n",
    "\n",
    "  with variable_scope.variable_scope(\"ScheduledEmbedding\"):\n",
    "    # Return -1s where we did not sample, and sample_ids elsewhere\n",
    "    select_sampler = bernoulli.Bernoulli(probs=sampling_probability, dtype=tf.bool)\n",
    "    select_sample = select_sampler.sample(sample_shape=hps.batch_size)\n",
    "    sample_id_sampler = categorical.Categorical(probs=output) # equals to argmax{ Multinomial(output, total_count=1) }, our greedy search selection\n",
    "    sample_ids = array_ops.where(\n",
    "            select_sample,\n",
    "            sample_id_sampler.sample(seed=123),\n",
    "            gen_array_ops.fill([hps.batch_size], -1))\n",
    "\n",
    "    where_sampling = math_ops.cast(\n",
    "        array_ops.where(sample_ids > -1), tf.int32)\n",
    "    where_not_sampling = math_ops.cast(\n",
    "        array_ops.where(sample_ids <= -1), tf.int32)\n",
    "\n",
    "    if hps.greedy_scheduled_sampling:\n",
    "      sample_ids = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "\n",
    "    sample_ids_sampling = array_ops.gather_nd(sample_ids, where_sampling)\n",
    "\n",
    "    cond = tf.less(sample_ids_sampling, vocab_size) # replace oov with unk\n",
    "    sample_ids_sampling = tf.cast(cond, tf.int32) * sample_ids_sampling\n",
    "    inputs_not_sampling = array_ops.gather_nd(inp, where_not_sampling)\n",
    "\n",
    "    if hps.E2EBackProp:\n",
    "      if hps.hard_argmax:\n",
    "        greedy_search_prob, greedy_search_sample = tf.nn.top_k(output, k=hps.k) # (batch_size, k)\n",
    "        greedy_search_prob_normalized = greedy_search_prob/tf.reshape(tf.reduce_sum(greedy_search_prob,axis=1),[-1,1])\n",
    "\n",
    "        cond = tf.less(greedy_search_sample, vocab_size) # replace oov with unk\n",
    "        greedy_search_sample = tf.cast(cond, tf.int32) * greedy_search_sample\n",
    "\n",
    "        greedy_embedding = tf.nn.embedding_lookup(embedding, greedy_search_sample)\n",
    "        normalized_embedding = tf.multiply(tf.reshape(greedy_search_prob_normalized,[hps.batch_size,hps.k,1]), greedy_embedding)\n",
    "        e2e_embedding = tf.reduce_mean(normalized_embedding,axis=1)\n",
    "      else:\n",
    "        e = []\n",
    "        greedy_search_prob, greedy_search_sample = soft_top_k(alpha, output,\n",
    "                                                              K=hps.k)  # (batch_size, k), (k, batch_size, vocab_size)\n",
    "        greedy_search_prob_normalized = greedy_search_prob / tf.reshape(tf.reduce_sum(greedy_search_prob, axis=1),\n",
    "                                                                        [-1, 1])\n",
    "\n",
    "        for _ in range(hps.k):\n",
    "          a_k = greedy_search_sample[_]\n",
    "          e_k = tf.matmul(tf.reshape(greedy_search_prob_normalized[:,_],[-1,1]) * a_k, embedding)\n",
    "          e.append(e_k)\n",
    "        e2e_embedding = tf.reduce_sum(e, axis=0) # (batch_size, emb_dim)\n",
    "      sampled_next_inputs = array_ops.gather_nd(e2e_embedding, where_sampling)\n",
    "    else:\n",
    "      if hps.hard_argmax:\n",
    "        sampled_next_inputs = tf.nn.embedding_lookup(embedding, sample_ids_sampling)\n",
    "      else: # using soft armax (greedy) proposed in: https://arxiv.org/abs/1704.06970\n",
    "        #alpha_exp = tf.exp(alpha * (output_not_extended + G)) # (batch_size, vocab_size)\n",
    "        #one_hot_scores = alpha_exp / tf.reduce_sum(alpha_exp, axis=1) #(batch_size, vocab_size)\n",
    "        one_hot_scores = soft_argmax(alpha, output) #(batch_size, vocab_size)\n",
    "        soft_argmax_embedding = tf.matmul(one_hot_scores, embedding) #(batch_size, emb_size)\n",
    "        sampled_next_inputs = array_ops.gather_nd(soft_argmax_embedding, where_sampling)\n",
    "\n",
    "    base_shape = array_ops.shape(inp)\n",
    "    result1 = array_ops.scatter_nd(indices=where_sampling, updates=sampled_next_inputs, shape=base_shape)\n",
    "    result2 = array_ops.scatter_nd(indices=where_not_sampling, updates=inputs_not_sampling, shape=base_shape)\n",
    "    return result1 + result2\n",
    "\n",
    "def scheduled_sampling_vocab_dist(hps, sampling_probability, output, embedding, inp, alpha = 0):\n",
    "  \n",
    "  def soft_argmax(alpha, output):\n",
    "    \n",
    "    one_hot_scores = tf.nn.softmax(alpha * output)\n",
    "    return one_hot_scores\n",
    "\n",
    "  def soft_top_k(alpha, output, K):\n",
    "    copy = tf.identity(output)\n",
    "    p = []\n",
    "    arg_top_k = []\n",
    "    for k in range(K):\n",
    "      sargmax = soft_argmax(alpha, copy)\n",
    "      copy = (1-sargmax)* copy\n",
    "      p.append(tf.reduce_sum(sargmax * output, axis=1))\n",
    "      arg_top_k.append(sargmax)\n",
    "\n",
    "    return tf.stack(p, axis=1), tf.stack(arg_top_k)\n",
    "\n",
    "  with variable_scope.variable_scope(\"ScheduledEmbedding\"):\n",
    "    \n",
    "    select_sampler = bernoulli.Bernoulli(probs=sampling_probability, dtype=tf.bool)\n",
    "    select_sample = select_sampler.sample(sample_shape=hps.batch_size)\n",
    "    sample_id_sampler = categorical.Categorical(probs=output) # equals to argmax{ Multinomial(output, total_count=1) }, our greedy search selection\n",
    "    sample_ids = array_ops.where(\n",
    "            select_sample,\n",
    "            sample_id_sampler.sample(seed=123),\n",
    "            gen_array_ops.fill([hps.batch_size], -1))\n",
    "\n",
    "    where_sampling = math_ops.cast(\n",
    "        array_ops.where(sample_ids > -1), tf.int32)\n",
    "    where_not_sampling = math_ops.cast(\n",
    "        array_ops.where(sample_ids <= -1), tf.int32)\n",
    "\n",
    "    if hps.greedy_scheduled_sampling:\n",
    "      sample_ids = tf.argmax(output, axis=1, output_type=tf.int32)\n",
    "\n",
    "    sample_ids_sampling = array_ops.gather_nd(sample_ids, where_sampling)\n",
    "    inputs_not_sampling = array_ops.gather_nd(inp, where_not_sampling)\n",
    "\n",
    "    if hps.E2EBackProp:\n",
    "      if hps.hard_argmax:\n",
    "        greedy_search_prob, greedy_search_sample = tf.nn.top_k(output, k=hps.k) # (batch_size, k)\n",
    "        greedy_search_prob_normalized = greedy_search_prob/tf.reshape(tf.reduce_sum(greedy_search_prob,axis=1),[-1,1])\n",
    "        greedy_embedding = tf.nn.embedding_lookup(embedding, greedy_search_sample)\n",
    "        normalized_embedding = tf.multiply(tf.reshape(greedy_search_prob_normalized,[hps.batch_size,hps.k,1]), greedy_embedding)\n",
    "        e2e_embedding = tf.reduce_mean(normalized_embedding,axis=1)\n",
    "      else:\n",
    "        e = []\n",
    "        greedy_search_prob, greedy_search_sample = soft_top_k(alpha, output,\n",
    "                                                              K=hps.k)  # (batch_size, k), (k, batch_size, vocab_size)\n",
    "        greedy_search_prob_normalized = greedy_search_prob / tf.reshape(tf.reduce_sum(greedy_search_prob, axis=1),\n",
    "                                                                        [-1, 1])\n",
    "\n",
    "        for _ in range(hps.k):\n",
    "          a_k = greedy_search_sample[_]\n",
    "          e_k = tf.matmul(tf.reshape(greedy_search_prob_normalized[:,_],[-1,1]) * a_k, embedding)\n",
    "          e.append(e_k)\n",
    "        e2e_embedding = tf.reduce_sum(e, axis=0) # (batch_size, emb_dim)\n",
    "      sampled_next_inputs = array_ops.gather_nd(e2e_embedding, where_sampling)\n",
    "    else:\n",
    "      if hps.hard_argmax:\n",
    "        sampled_next_inputs = tf.nn.embedding_lookup(embedding, sample_ids_sampling)\n",
    "      else: \n",
    "        one_hot_scores = soft_argmax(alpha, output) #(batch_size, vocab_size)\n",
    "        soft_argmax_embedding = tf.matmul(one_hot_scores, embedding) #(batch_size, emb_size)\n",
    "        sampled_next_inputs = array_ops.gather_nd(soft_argmax_embedding, where_sampling)\n",
    "\n",
    "    base_shape = array_ops.shape(inp)\n",
    "    result1 = array_ops.scatter_nd(indices=where_sampling, updates=sampled_next_inputs, shape=base_shape)\n",
    "    result2 = array_ops.scatter_nd(indices=where_not_sampling, updates=inputs_not_sampling, shape=base_shape)\n",
    "    return result1 + result2\n",
    "\n",
    "def linear(args, output_size, bias, bias_start=0.0, scope=None):\n",
    "  \n",
    "  if args is None or (isinstance(args, (list, tuple)) and not args):\n",
    "    raise ValueError(\"`args` must be specified\")\n",
    "  if not isinstance(args, (list, tuple)):\n",
    "    args = [args]\n",
    "  \n",
    "  total_arg_size = 0\n",
    "  shapes = [a.get_shape().as_list() for a in args]\n",
    "  for shape in shapes:\n",
    "    if len(shape) != 2:\n",
    "      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n",
    "    if not shape[1]:\n",
    "      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n",
    "    else:\n",
    "      total_arg_size += shape[1]\n",
    "\n",
    "  # Now the computation.\n",
    "  with tf.variable_scope(scope or \"Linear\" , reuse=tf.AUTO_REUSE):\n",
    "    matrix = tf.get_variable(\"Matrix\", [total_arg_size, output_size])\n",
    "    if len(args) == 1:\n",
    "      res = tf.matmul(args[0], matrix)\n",
    "    else:\n",
    "      res = tf.matmul(tf.concat(axis=1, values=args), matrix)\n",
    "    if not bias:\n",
    "      return res\n",
    "    bias_term = tf.get_variable(\n",
    "        \"Bias\", [output_size], initializer=tf.constant_initializer(bias_start))\n",
    "  return res + bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOpxpsHjIwaE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from attention_decoder import attention_decoder\n",
    "##from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "#from rouge import rouge\n",
    "#from rouge_tensor import rouge_l_fscore\n",
    "#import data\n",
    "#from replay_buffer import Transition\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "class SummarizationModel(object):\n",
    "  \n",
    "\n",
    "  def __init__(self, hps, vocab):\n",
    "    self._hps = hps\n",
    "    self._vocab = vocab\n",
    "\n",
    "  def reward_function(self, reference, summary, measure='rouge_l/f_score'):\n",
    "    \n",
    "    if 'rouge' in measure:\n",
    "      return rouge([summary],[reference])[measure]\n",
    "    else:\n",
    "      return sentence_bleu([reference.split()],summary.split(),weights=(0.25,0.25,0.25,0.25))\n",
    "\n",
    "  def variable_summaries(self, var_name, var):\n",
    "    \n",
    "    with tf.name_scope('summaries_{}'.format(var_name)):\n",
    "      mean = tf.reduce_mean(var)\n",
    "      tf.summary.scalar('mean', mean)\n",
    "      with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "      tf.summary.scalar('stddev', stddev)\n",
    "      tf.summary.scalar('max', tf.reduce_max(var))\n",
    "      tf.summary.scalar('min', tf.reduce_min(var))\n",
    "      tf.summary.histogram('histogram', var)\n",
    "\n",
    "  def _add_placeholders(self):\n",
    "    \n",
    "    hps = self._hps\n",
    "\n",
    "    # encoder part\n",
    "    self._enc_batch = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch')\n",
    "    self._enc_lens = tf.placeholder(tf.int32, [hps.batch_size], name='enc_lens')\n",
    "    self._enc_padding_mask = tf.placeholder(tf.float32, [hps.batch_size, None], name='enc_padding_mask')\n",
    "    self._eta = tf.placeholder(tf.float32, None, name='eta')\n",
    "    if FLAGS.embedding:\n",
    "      self.embedding_place = tf.placeholder(tf.float32, [self._vocab.size(), hps.emb_dim])\n",
    "    if FLAGS.pointer_gen:\n",
    "      self._enc_batch_extend_vocab = tf.placeholder(tf.int32, [hps.batch_size, None], name='enc_batch_extend_vocab')\n",
    "      self._max_art_oovs = tf.placeholder(tf.int32, [], name='max_art_oovs')\n",
    "    if FLAGS.ac_training: # added by yaserkl@vt.edu for the purpose of calculating rouge loss\n",
    "      self._q_estimates = tf.placeholder(tf.float32, [self._hps.batch_size,self._hps.k,self._hps.max_dec_steps, None], name='q_estimates')\n",
    "    if FLAGS.scheduled_sampling:\n",
    "      self._sampling_probability = tf.placeholder(tf.float32, None, name='sampling_probability')\n",
    "      self._alpha = tf.placeholder(tf.float32, None, name='alpha')\n",
    "\n",
    "    # decoder part\n",
    "    self._dec_batch = tf.placeholder(tf.int32, [hps.batch_size, hps.max_dec_steps], name='dec_batch')\n",
    "    self._target_batch = tf.placeholder(tf.int32, [hps.batch_size, hps.max_dec_steps], name='target_batch')\n",
    "    self._dec_padding_mask = tf.placeholder(tf.float32, [hps.batch_size, hps.max_dec_steps], name='dec_padding_mask')\n",
    "\n",
    "    if hps.mode == \"decode\":\n",
    "      if hps.coverage:\n",
    "        self.prev_coverage = tf.placeholder(tf.float32, [hps.batch_size, None], name='prev_coverage')\n",
    "      if hps.intradecoder:\n",
    "        self.prev_decoder_outputs = tf.placeholder(tf.float32, [None, hps.batch_size, hps.dec_hidden_dim], name='prev_decoder_outputs')\n",
    "      if hps.use_temporal_attention:\n",
    "        self.prev_encoder_es = tf.placeholder(tf.float32, [None, hps.batch_size, None], name='prev_encoder_es')\n",
    "\n",
    "  def _make_feed_dict(self, batch, just_enc=False):\n",
    "    \n",
    "    feed_dict = {}\n",
    "    feed_dict[self._enc_batch] = batch.enc_batch\n",
    "    feed_dict[self._enc_lens] = batch.enc_lens\n",
    "    feed_dict[self._enc_padding_mask] = batch.enc_padding_mask\n",
    "    if FLAGS.pointer_gen:\n",
    "      feed_dict[self._enc_batch_extend_vocab] = batch.enc_batch_extend_vocab\n",
    "      feed_dict[self._max_art_oovs] = batch.max_art_oovs\n",
    "    if not just_enc:\n",
    "      feed_dict[self._dec_batch] = batch.dec_batch\n",
    "      feed_dict[self._target_batch] = batch.target_batch\n",
    "      feed_dict[self._dec_padding_mask] = batch.dec_padding_mask\n",
    "    return feed_dict\n",
    "\n",
    "  def _add_encoder(self, emb_enc_inputs, seq_len):\n",
    "    \n",
    "    with tf.variable_scope('encoder'):\n",
    "      cell_fw = tf.contrib.rnn.LSTMCell(self._hps.enc_hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)\n",
    "      cell_bw = tf.contrib.rnn.LSTMCell(self._hps.enc_hidden_dim, initializer=self.rand_unif_init, state_is_tuple=True)\n",
    "      (encoder_outputs, (fw_st, bw_st)) = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, emb_enc_inputs, dtype=tf.float32, sequence_length=seq_len, swap_memory=True)\n",
    "      encoder_outputs = tf.concat(axis=2, values=encoder_outputs) # concatenate the forwards and backwards states\n",
    "    return encoder_outputs, fw_st, bw_st\n",
    "\n",
    "  def _reduce_states(self, fw_st, bw_st):\n",
    "    \n",
    "    enc_hidden_dim = self._hps.enc_hidden_dim\n",
    "    dec_hidden_dim = self._hps.dec_hidden_dim\n",
    "\n",
    "    with tf.variable_scope('reduce_final_st'):\n",
    "\n",
    "      # Define weights and biases to reduce the cell and reduce the state\n",
    "      w_reduce_c = tf.get_variable('w_reduce_c', [enc_hidden_dim * 2, dec_hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
    "      w_reduce_h = tf.get_variable('w_reduce_h', [enc_hidden_dim * 2, dec_hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
    "      bias_reduce_c = tf.get_variable('bias_reduce_c', [dec_hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
    "      bias_reduce_h = tf.get_variable('bias_reduce_h', [dec_hidden_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
    "\n",
    "      # Apply linear layer\n",
    "      old_c = tf.concat(axis=1, values=[fw_st.c, bw_st.c]) # Concatenation of fw and bw cell\n",
    "      old_h = tf.concat(axis=1, values=[fw_st.h, bw_st.h]) # Concatenation of fw and bw state\n",
    "      new_c = tf.nn.relu(tf.matmul(old_c, w_reduce_c) + bias_reduce_c) # Get new cell from old cell\n",
    "      new_h = tf.nn.relu(tf.matmul(old_h, w_reduce_h) + bias_reduce_h) # Get new state from old state\n",
    "      return tf.contrib.rnn.LSTMStateTuple(new_c, new_h) # Return new cell and state\n",
    "\n",
    "  def _add_decoder(self, emb_dec_inputs, embedding):\n",
    "    \n",
    "    hps = self._hps\n",
    "    cell = tf.contrib.rnn.LSTMCell(hps.dec_hidden_dim, state_is_tuple=True, initializer=self.rand_unif_init)\n",
    "\n",
    "    prev_coverage = self.prev_coverage if (hps.mode==\"decode\" and hps.coverage) else None # In decode mode, we run attention_decoder one step at a time and so need to pass in the previous step's coverage vector each time\n",
    "    prev_decoder_outputs = self.prev_decoder_outputs if (hps.intradecoder and hps.mode==\"decode\") else tf.stack([],axis=0)\n",
    "    prev_encoder_es = self.prev_encoder_es if (hps.use_temporal_attention and hps.mode==\"decode\") else tf.stack([],axis=0)\n",
    "    return attention_decoder(hps,\n",
    "      self._vocab.size(),\n",
    "      self._max_art_oovs,\n",
    "      self._enc_batch_extend_vocab,\n",
    "      emb_dec_inputs,\n",
    "      self._target_batch,\n",
    "      self._dec_in_state,\n",
    "      self._enc_states,\n",
    "      self._enc_padding_mask,\n",
    "      self._dec_padding_mask,\n",
    "      cell,\n",
    "      embedding,\n",
    "      self._sampling_probability if FLAGS.scheduled_sampling else 0,\n",
    "      self._alpha if FLAGS.E2EBackProp else 0,\n",
    "      self._vocab.word2id(UNKNOWN_TOKEN),\n",
    "      initial_state_attention=(hps.mode==\"decode\"),\n",
    "      pointer_gen=hps.pointer_gen,\n",
    "      use_coverage=hps.coverage,\n",
    "      prev_coverage=prev_coverage,\n",
    "      prev_decoder_outputs=prev_decoder_outputs,\n",
    "      prev_encoder_es = prev_encoder_es)\n",
    "\n",
    "  def _add_emb_vis(self, embedding_var):\n",
    "    \n",
    "    train_dir = os.path.join(FLAGS.log_root, \"train\")\n",
    "    vocab_metadata_path = os.path.join(train_dir, \"vocab_metadata.tsv\")\n",
    "    self._vocab.write_metadata(vocab_metadata_path) # write metadata file\n",
    "    summary_writer = tf.summary.FileWriter(train_dir)\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    embedding.metadata_path = vocab_metadata_path\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "  def discount_rewards(self, r):\n",
    "    \n",
    "    discounted_r = []\n",
    "    running_add = tf.constant(0, tf.float32)\n",
    "    for t in reversed(range(0, len(r))):\n",
    "      running_add = running_add * self._hps.gamma + r[t] # rd_t = r_t + gamma * r_{t+1}\n",
    "      discounted_r.append(running_add)\n",
    "    discounted_r = tf.stack(discounted_r[::-1]) # (max_dec_step, batch_size, k)\n",
    "    normalized_discounted_r = tf.nn.l2_normalize(discounted_r, axis=0)\n",
    "    return tf.unstack(normalized_discounted_r) # list of max_dec_step * (batch_size, k)\n",
    "\n",
    "  def intermediate_rewards(self, r):\n",
    "    \n",
    "    intermediate_r = []\n",
    "    intermediate_r.append(r[0])\n",
    "    for t in range(1, len(r)):\n",
    "      intermediate_r.append(r[t]-r[t-1])\n",
    "    return intermediate_r # list of max_dec_step * (batch_size, k)\n",
    "\n",
    "  def _add_seq2seq(self):\n",
    "    \n",
    "    hps = self._hps\n",
    "    vsize = self._vocab.size() # size of the vocabulary\n",
    "\n",
    "    with tf.variable_scope('seq2seq'):\n",
    "      # Some initializers\n",
    "      self.rand_unif_init = tf.random_uniform_initializer(-hps.rand_unif_init_mag, hps.rand_unif_init_mag, seed=123)\n",
    "      self.trunc_norm_init = tf.truncated_normal_initializer(stddev=hps.trunc_norm_init_std)\n",
    "\n",
    "      # Add embedding matrix (shared by the encoder and decoder inputs)\n",
    "      with tf.variable_scope('embedding'):\n",
    "        if FLAGS.embedding:\n",
    "          embedding = tf.Variable(self.embedding_place)\n",
    "        else:\n",
    "          embedding = tf.get_variable('embedding', [vsize, hps.emb_dim], dtype=tf.float32, initializer=self.trunc_norm_init)\n",
    "        ##if hps.mode==\"train\": self._add_emb_vis(embedding) # add to tensorboard\n",
    "        emb_enc_inputs = tf.nn.embedding_lookup(embedding, self._enc_batch) # tensor with shape (batch_size, max_enc_steps, emb_size)\n",
    "        emb_dec_inputs = [tf.nn.embedding_lookup(embedding, x) for x in tf.unstack(self._dec_batch, axis=1)] # list length max_dec_steps containing shape (batch_size, emb_size)\n",
    "\n",
    "      # Add the encoder.\n",
    "      enc_outputs, fw_st, bw_st = self._add_encoder(emb_enc_inputs, self._enc_lens)\n",
    "      self._enc_states = enc_outputs\n",
    "\n",
    "      # Our encoder is bidirectional and our decoder is unidirectional so we need to reduce the final encoder hidden state to the right size to be the initial decoder hidden state\n",
    "      self._dec_in_state = self._reduce_states(fw_st, bw_st)\n",
    "\n",
    "      # Add the decoder.\n",
    "      with tf.variable_scope('decoder'):\n",
    "        (self.decoder_outputs, self._dec_out_state, self.attn_dists, self.p_gens, self.coverage, self.vocab_scores,\n",
    "         self.final_dists, self.samples, self.greedy_search_samples, self.temporal_es,\n",
    "         self.sampling_rewards, self.greedy_rewards) = self._add_decoder(emb_dec_inputs, embedding)\n",
    "\n",
    "      if FLAGS.use_discounted_rewards and hps.rl_training and hps.mode in ['train', 'eval']:\n",
    "        \n",
    "        self.sampling_discounted_rewards = tf.stack(self.discount_rewards(tf.unstack(self.sampling_rewards))) # list of max_dec_steps * (batch_size, k)\n",
    "        self.greedy_discounted_rewards = tf.stack(self.discount_rewards(tf.unstack(self.greedy_rewards))) # list of max_dec_steps * (batch_size, k)\n",
    "      elif FLAGS.use_intermediate_rewards and hps.rl_training and hps.mode in ['train', 'eval']:\n",
    "        \n",
    "        self.sampling_discounted_rewards = tf.stack(self.intermediate_rewards(tf.unstack(self.sampling_rewards))) # list of max_dec_steps * (batch_size, k)\n",
    "        self.greedy_discounted_rewards = tf.stack(self.intermediate_rewards(tf.unstack(self.greedy_rewards))) # list of max_dec_steps * (batch_size, k)\n",
    "      elif hps.ac_training and hps.mode in ['train', 'eval']:\n",
    "        # Get the sampled and greedy sentence from model output\n",
    "        self.sampled_sentences = tf.transpose(tf.stack(self.samples), perm=[1,2,0]) # (batch_size, k, <=max_dec_steps) word indices\n",
    "        self.greedy_search_sentences = tf.transpose(tf.stack(self.greedy_search_samples), perm=[1,2,0]) # (batch_size, k, <=max_dec_steps) word indices\n",
    "\n",
    "    if hps.mode == \"decode\":\n",
    "      # We run decode beam search mode one decoder step at a time\n",
    "      assert len(self.final_dists)==1 # final_dists is a singleton list containing shape (batch_size, extended_vsize)\n",
    "      self.final_dists = self.final_dists[0]\n",
    "      topk_probs, self._topk_ids = tf.nn.top_k(self.final_dists, hps.batch_size*2) # take the k largest probs. note batch_size=beam_size in decode mode\n",
    "      self._topk_log_probs = tf.log(topk_probs)\n",
    "\n",
    "  def _add_shared_loss_op(self):\n",
    "    \n",
    "    with tf.variable_scope('shared_loss'):\n",
    "      \n",
    "      loss_per_step = [] # will be list length max_dec_steps containing shape (batch_size)\n",
    "      batch_nums = tf.range(0, limit=self._hps.batch_size) # shape (batch_size)\n",
    "      for dec_step, dist in enumerate(self.final_dists):\n",
    "        targets = self._target_batch[:,dec_step] # The indices of the target words. shape (batch_size)\n",
    "        indices = tf.stack( (batch_nums, targets), axis=1) # shape (batch_size, 2)\n",
    "        gold_probs = tf.gather_nd(dist, indices) # shape (batch_size). prob of correct words on this step\n",
    "        losses = -tf.log(gold_probs)\n",
    "        loss_per_step.append(losses)\n",
    "      self._pgen_loss = _mask_and_avg(loss_per_step, self._dec_padding_mask)\n",
    "      self.variable_summaries('pgen_loss', self._pgen_loss)\n",
    "      # Adding Q-Estimation to CE loss in Actor-Critic Model\n",
    "      if self._hps.ac_training:\n",
    "        \n",
    "        loss_per_step = [] # will be list length k each containing a list of shape <=max_dec_steps which each has the shape (batch_size)\n",
    "        q_loss_per_step = [] # will be list length k each containing a list of shape <=max_dec_steps which each has the shape (batch_size)\n",
    "        batch_nums = tf.range(0, limit=self._hps.batch_size) # shape (batch_size)\n",
    "        unstacked_q = tf.unstack(self._q_estimates, axis =1) # list of k with size (batch_size, <=max_dec_steps, vsize_extended)\n",
    "        for sample_id in range(self._hps.k):\n",
    "          loss_per_sample = [] # length <=max_dec_steps of batch_sizes\n",
    "          q_loss_per_sample = [] # length <=max_dec_steps of batch_sizes\n",
    "          q_val_per_sample = tf.unstack(unstacked_q[sample_id], axis =1) # list of <=max_dec_step (batch_size, vsize_extended)\n",
    "          for dec_step, (dist, q_value) in enumerate(zip(self.final_dists, q_val_per_sample)):\n",
    "            targets = tf.squeeze(self.samples[dec_step][:,sample_id]) # The indices of the sampled words. shape (batch_size)\n",
    "            indices = tf.stack((batch_nums, targets), axis=1) # shape (batch_size, 2)\n",
    "            gold_probs = tf.gather_nd(dist, indices) # shape (batch_size). prob of correct words on this step\n",
    "            losses = -tf.log(gold_probs)\n",
    "            dist_q_val = -tf.log(dist) * q_value\n",
    "            q_losses = tf.gather_nd(dist_q_val, indices) # shape (batch_size). prob of correct words on this step\n",
    "            loss_per_sample.append(losses)\n",
    "            q_loss_per_sample.append(q_losses)\n",
    "          loss_per_step.append(loss_per_sample)\n",
    "          q_loss_per_step.append(q_loss_per_sample)\n",
    "        with tf.variable_scope('reinforce_loss'):\n",
    "          #### this is the actual loss\n",
    "          self._rl_avg_logprobs = tf.reduce_mean([_mask_and_avg(loss_per_sample, self._dec_padding_mask) for loss_per_sample in loss_per_step])\n",
    "          self._rl_loss = tf.reduce_mean([_mask_and_avg(q_loss_per_sample, self._dec_padding_mask) for q_loss_per_sample in q_loss_per_step])\n",
    "          # Eq. 34 in https://arxiv.org/pdf/1805.09461.pdf\n",
    "          self._reinforce_shared_loss = self._eta * self._rl_loss + (tf.constant(1.,dtype=tf.float32) - self._eta) * self._pgen_loss # equation 16 in https://arxiv.org/pdf/1705.04304.pdf\n",
    "          #### the following is only for monitoring purposes\n",
    "          self.variable_summaries('reinforce_avg_logprobs', self._rl_avg_logprobs)\n",
    "          self.variable_summaries('reinforce_loss', self._rl_loss)\n",
    "          self.variable_summaries('reinforce_shared_loss', self._reinforce_shared_loss)\n",
    "\n",
    "      if self._hps.rl_training:\n",
    "        loss_per_step = [] # will be list length max_dec_steps*k containing shape (batch_size)\n",
    "        rl_loss_per_step = [] # will be list length max_dec_steps*k containing shape (batch_size)\n",
    "        batch_nums = tf.range(0, limit=self._hps.batch_size) # shape (batch_size)\n",
    "        self._sampled_rouges = []\n",
    "        self._greedy_rouges = []\n",
    "        self._reward_diff = []\n",
    "        for _ in range(self._hps.k):\n",
    "          if FLAGS.use_discounted_rewards or FLAGS.use_intermediate_rewards:\n",
    "            self._sampled_rouges.append(self.sampling_discounted_rewards[:, :, _]) # shape (max_enc_steps, batch_size)\n",
    "            self._greedy_rouges.append(self.greedy_discounted_rewards[:, :, _]) # shape (max_enc_steps, batch_size)\n",
    "          else:\n",
    "            # use the reward of last step, since we use the reward of the whole sentence in this case\n",
    "            self._sampled_rouges.append(self.sampling_rewards[:, _]) # shape (batch_size)\n",
    "            self._greedy_rouges.append(self.greedy_rewards[:, _]) # shape (batch_size)\n",
    "          if FLAGS.self_critic:\n",
    "            self._reward_diff.append(self._greedy_rouges[_]-self._sampled_rouges[_])\n",
    "          else:\n",
    "            self._reward_diff.append(self._sampled_rouges[_])\n",
    "        for dec_step, dist in enumerate(self.final_dists):\n",
    "          _targets = self.samples[dec_step] # The indices of the sampled words. shape (batch_size, k)\n",
    "          for _k, targets in enumerate(tf.unstack(_targets,axis=1)): # list of k samples of size (batch_size)\n",
    "            indices = tf.stack( (batch_nums, targets), axis=1) # shape (batch_size, 2)\n",
    "            gold_probs = tf.gather_nd(dist, indices) # shape (batch_size). prob of correct words on this step\n",
    "            losses = -tf.log(gold_probs)\n",
    "            loss_per_step.append(losses)\n",
    "            if FLAGS.use_discounted_rewards or FLAGS.use_intermediate_rewards:\n",
    "              rl_losses = -tf.log(gold_probs) * self._reward_diff[_k][dec_step, :]  # positive values\n",
    "            else:\n",
    "              rl_losses = -tf.log(gold_probs) * self._reward_diff[_k] # positive values\n",
    "            rl_loss_per_step.append(rl_losses)\n",
    "\n",
    "        # new size: (k, max_dec_steps, batch_size)\n",
    "        rl_loss_per_step = tf.unstack(\n",
    "          tf.transpose(tf.reshape(rl_loss_per_step, [-1, self._hps.k, self._hps.batch_size]),perm=[1,0,2]))\n",
    "        loss_per_step = tf.unstack(\n",
    "          tf.transpose(tf.reshape(loss_per_step, [-1, self._hps.k, self._hps.batch_size]), perm=[1, 0, 2]))\n",
    "\n",
    "        if FLAGS.use_intermediate_rewards:\n",
    "          self._sampled_rouges = tf.reduce_sum(self._sampled_rouges, axis=1) # shape (k, batch_size)\n",
    "          self._greedy_rouges = tf.reduce_sum(self._greedy_rouges, axis=1) # shape (k, batch_size)\n",
    "          self._reward_diff = tf.reduce_sum(self._reward_diff, axis=1) # shape (k, batch_size)\n",
    "\n",
    "        with tf.variable_scope('reinforce_loss'):\n",
    "          self._rl_avg_logprobs = []\n",
    "          self._rl_loss = []\n",
    "\n",
    "          for _k in range(self._hps.k):\n",
    "            self._rl_avg_logprobs.append(_mask_and_avg(tf.unstack(loss_per_step[_k]), self._dec_padding_mask))\n",
    "            self._rl_loss.append(_mask_and_avg(tf.unstack(tf.reshape(rl_loss_per_step[_k], [self._hps.max_dec_steps, self._hps.batch_size])), self._dec_padding_mask))\n",
    "\n",
    "          self._rl_avg_logprobs = tf.reduce_mean(self._rl_avg_logprobs)\n",
    "          self._rl_loss = tf.reduce_mean(self._rl_loss)\n",
    "          self._reinforce_shared_loss = self._eta * self._rl_loss + (tf.constant(1.,dtype=tf.float32) - self._eta) * self._pgen_loss\n",
    "          self.variable_summaries('reinforce_avg_logprobs', self._rl_avg_logprobs)\n",
    "          self.variable_summaries('reinforce_loss', self._rl_loss)\n",
    "          self.variable_summaries('reinforce_sampled_r_value', tf.reduce_mean(self._sampled_rouges))\n",
    "          self.variable_summaries('reinforce_greedy_r_value', tf.reduce_mean(self._greedy_rouges))\n",
    "          self.variable_summaries('reinforce_r_diff', tf.reduce_mean(self._reward_diff))\n",
    "          self.variable_summaries('reinforce_shared_loss', self._reinforce_shared_loss)\n",
    "\n",
    "      if self._hps.coverage:\n",
    "        with tf.variable_scope('coverage_loss'):\n",
    "          self._coverage_loss = _coverage_loss(self.attn_dists, self._dec_padding_mask)\n",
    "          self.variable_summaries('coverage_loss', self._coverage_loss)\n",
    "        if self._hps.rl_training or self._hps.ac_training:\n",
    "          with tf.variable_scope('reinforce_loss'):\n",
    "            self._reinforce_cov_total_loss = self._reinforce_shared_loss + self._hps.cov_loss_wt * self._coverage_loss\n",
    "            self.variable_summaries('reinforce_coverage_loss', self._reinforce_cov_total_loss)\n",
    "        if self._hps.pointer_gen:\n",
    "          self._pointer_cov_total_loss = self._pgen_loss + self._hps.cov_loss_wt * self._coverage_loss\n",
    "          self.variable_summaries('pointer_coverage_loss', self._pointer_cov_total_loss)\n",
    "\n",
    "  def _add_shared_train_op(self):\n",
    "    \n",
    "    if self._hps.rl_training or self._hps.ac_training:\n",
    "      loss_to_minimize = self._reinforce_shared_loss\n",
    "      if self._hps.coverage:\n",
    "        loss_to_minimize = self._reinforce_cov_total_loss\n",
    "    else:\n",
    "      loss_to_minimize = self._pgen_loss\n",
    "      if self._hps.coverage:\n",
    "        loss_to_minimize = self._pointer_cov_total_loss\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    gradients = tf.gradients(loss_to_minimize, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "\n",
    "    with tf.device(\"/gpu:{}\".format(self._hps.gpu_num)):\n",
    "      grads, global_norm = tf.clip_by_global_norm(gradients, self._hps.max_grad_norm)\n",
    "\n",
    "    tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(self._hps.lr, initial_accumulator_value=self._hps.adagrad_init_acc)\n",
    "    with tf.device(\"/gpu:{}\".format(self._hps.gpu_num)):\n",
    "      self._shared_train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step, name='train_step')\n",
    "\n",
    "  def build_graph(self):\n",
    "    tf.logging.info('Building graph...')\n",
    "    t0 = time.time()\n",
    "    self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    self._add_placeholders()\n",
    "    with tf.device(\"/gpu:{}\".format(self._hps.gpu_num)):\n",
    "      self._add_seq2seq()\n",
    "      if self._hps.mode in ['train', 'eval']:\n",
    "        self._add_shared_loss_op()\n",
    "      if self._hps.mode == 'train':\n",
    "        self._add_shared_train_op()\n",
    "      self._summaries = tf.summary.merge_all()\n",
    "    t1 = time.time()\n",
    "    tf.logging.info('Time to build graph: %i seconds', t1 - t0)\n",
    "\n",
    "  def collect_dqn_transitions(self, sess, batch, step, max_art_oovs):\n",
    "    \n",
    "\n",
    "    feed_dict = self._make_feed_dict(batch)\n",
    "    if self._hps.fixed_eta:\n",
    "      feed_dict[self._eta] = self._hps.eta\n",
    "    else:\n",
    "      feed_dict[self._eta] = min(step * self._hps.eta,1.)\n",
    "    if self._hps.scheduled_sampling:\n",
    "      if self._hps.fixed_sampling_probability:\n",
    "        feed_dict[self._sampling_probability] = self._hps.sampling_probability\n",
    "      else:\n",
    "        feed_dict[self._sampling_probability] = min(step * self._hps.sampling_probability,1.) # linear decay function\n",
    "      ranges = [np.exp(float(step) * self._hps.alpha),np.finfo(np.float64).max] # to avoid overflow\n",
    "      feed_dict[self._alpha] = np.log(ranges[np.argmin(ranges)]) # linear decay function\n",
    "\n",
    "    vsize_extended = self._vocab.size() + max_art_oovs\n",
    "    if self._hps.calculate_true_q:\n",
    "      self.advantages = np.zeros((self._hps.batch_size, self._hps.k, self._hps.max_dec_steps, vsize_extended),dtype=np.float32) # (batch_size, k, <=max_dec_steps,vocab_size)\n",
    "      self.q_values = np.zeros((self._hps.batch_size, self._hps.k, self._hps.max_dec_steps, vsize_extended),dtype=np.float32) # (batch_size, k, <=max_dec_steps,vocab_size)\n",
    "      self.r_values = np.zeros((self._hps.batch_size, self._hps.k, self._hps.max_dec_steps, vsize_extended),dtype=np.float32) # (batch_size, k, <=max_dec_steps,vocab_size)\n",
    "      self.v_values = np.zeros((self._hps.batch_size, self._hps.k, self._hps.max_dec_steps),dtype=np.float32) # (batch_size, k, <=max_dec_steps)\n",
    "    else:\n",
    "      self.r_values = np.zeros((self._hps.batch_size, self._hps.k, self._hps.max_dec_steps),dtype=np.float32) # (batch_size, k, <=max_dec_steps)\n",
    "    to_return = {\n",
    "      'sampled_sentences': self.sampled_sentences,\n",
    "      'greedy_search_sentences': self.greedy_search_sentences,\n",
    "      'decoder_outputs': self.decoder_outputs,\n",
    "    }\n",
    "    ret_dict = sess.run(to_return, feed_dict)\n",
    "\n",
    "\n",
    "    _t = time.time()\n",
    "    for _n, (sampled_sentence, greedy_search_sentence, target_sentence) in enumerate(zip(ret_dict['sampled_sentences'],ret_dict['greedy_search_sentences'], batch.target_batch)): # run batch_size time\n",
    "      _gts = target_sentence\n",
    "      for i in range(self._hps.k):\n",
    "        _ss = greedy_search_sentence[i] # reward is calculated over the best action through greedy search\n",
    "        if self._hps.calculate_true_q:\n",
    "          # Collect Reward, Q, V, and A only when we want to train DDQN using true Q-estimation\n",
    "          A, Q, V, R = self.caluclate_advantage_function(_ss, _gts, vsize_extended)\n",
    "          self.r_values[_n,i,:,:] = R\n",
    "          self.advantages[_n,i,:,:] = A\n",
    "          self.q_values[_n,i,:,:] = Q\n",
    "          self.v_values[_n,i,:] = V\n",
    "        else:\n",
    "          # if using DDQN estimates, we only need to calculate the reward and later on estimate Q values.\n",
    "          self.r_values[_n, i, :] = self.caluclate_single_reward(_ss, _gts) # len max_dec_steps\n",
    "    tf.logging.info('seconds for dqn collection: {}'.format(time.time()-_t))\n",
    "    trasitions = self.prepare_dqn_transitions(self._hps, ret_dict['decoder_outputs'], ret_dict['greedy_search_sentences'], vsize_extended)\n",
    "    return trasitions\n",
    "\n",
    "  def caluclate_advantage_function(self, _ss, _gts, vsize_extended):\n",
    "    \n",
    "\n",
    "    R = np.zeros((self._hps.max_dec_steps, vsize_extended)) # shape (max_dec_steps, vocab_size)\n",
    "    Q = np.zeros((self._hps.max_dec_steps, vsize_extended)) # shape (max_dec_steps, vocab_size)\n",
    "    for t in range(self._hps.max_dec_steps,0,-1):\n",
    "      R[t-1][:] = self.reward(t, _ss, _gts, vsize_extended)\n",
    "      try:\n",
    "        Q[t-1][:] = R[t-1][:] + self._hps.gamma * Q[t,:].max()\n",
    "      except:\n",
    "        Q[t-1][:] = R[t-1][:]\n",
    "\n",
    "    V = np.reshape(np.mean(Q,axis=1),[-1,1])\n",
    "\n",
    "    A = Q - V\n",
    "    return A, Q, np.squeeze(V), R\n",
    "\n",
    "  def caluclate_single_reward(self, _ss, _gts):\n",
    "    \n",
    "\n",
    "    return [self.calc_reward(t, _ss,_gts) for t in range(1,self._hps.max_dec_steps+1)]\n",
    "\n",
    "  def prepare_dqn_transitions(self, hps, decoder_states, greedy_samples, vsize_extended):\n",
    "    \n",
    "    decoder_states = np.transpose(np.stack(decoder_states),[1,0,2]) # now of shape (batch_size, <=max_dec_steps, hidden_dim)\n",
    "    greedy_samples = np.stack(greedy_samples) # now of shape (batch_size, <=max_dec_steps)\n",
    "\n",
    "    dec_length = decoder_states.shape[1]\n",
    "    hidden_dim = decoder_states.shape[-1]\n",
    "\n",
    "    # modifying decoder state tensor to shape (batch_size, k, <=max_dec_steps, hidden_dim)\n",
    "    _decoder_states = np.expand_dims(decoder_states, 1)\n",
    "    _decoder_states = np.concatenate([_decoder_states] * hps.k, axis=1) # shape (batch_size, k, <=max_dec_steps, hidden_dim)\n",
    "    features = _decoder_states # shape (batch_size, k, <=max_dec_steps, hidden_dim)\n",
    "\n",
    "    q_func = lambda i,k,t: self.q_values[i,k,t] # (vsize_extended)\n",
    "    zero_func = lambda i, k, t: np.zeros((vsize_extended))\n",
    "    raction_func = lambda i,k,t,action: self.r_values[i,k,t,action]\n",
    "    r_func = lambda i,k,t,action: self.r_values[i,k,t]\n",
    "\n",
    "    if self._hps.calculate_true_q:\n",
    "      pass_q_func = q_func\n",
    "      pass_r_func = raction_func\n",
    "    else:\n",
    "      pass_q_func = zero_func\n",
    "      pass_r_func = r_func\n",
    "\n",
    "    transitions = [] # (h_t, w_t, h_{t+1}, r_t, q_t, done)\n",
    "    for i in range(self._hps.batch_size):\n",
    "      for k in range(self._hps.k):\n",
    "        for t in range(self._hps.max_dec_steps):\n",
    "          action = greedy_samples[i,k,t]\n",
    "          done = (t==(self._hps.max_dec_steps-1) or action==3) # 3 is the id for [STOP] in our vocabularity to stop decoding\n",
    "          state = features[i, k, t]\n",
    "          if done:\n",
    "            state_prime = np.zeros((features.shape[-1]))\n",
    "            action_prime = 3 # 3 is the id for [STOP] in our vocabularity to stop decoding\n",
    "          else:\n",
    "            state_prime = features[i,k,t+1]\n",
    "            action_prime = greedy_samples[i,k,t+1]\n",
    "          transitions.append(Transition(state, action, state_prime, action_prime, pass_r_func(i,k,t,action), pass_q_func(i,k,t), done))\n",
    "\n",
    "    return transitions\n",
    "\n",
    "  def calc_reward(self, _ss, _gts): # optimizing based on ROUGE-L\n",
    "    \n",
    "\n",
    "    summary = ' '.join([str(k) for k in _ss])\n",
    "    reference = ' '.join([str(k) for k in _gts])\n",
    "    reward = self.reward_function(reference, summary, self._hps.reward_function)\n",
    "    return reward\n",
    "\n",
    "  def reward(self, t, _ss, _gts, vsize_extended): # shape (vocab_size)\n",
    "    \"\"\" A wrapper for calculating the reward. \"\"\"\n",
    "    first_case = np.append(_ss[0:t],[0]) # our special character is '[UNK]' which has the id of 0 in our vocabulary\n",
    "    special_reward = self.calc_reward(first_case, _gts)\n",
    "    reward = [special_reward for _ in range(vsize_extended)]\n",
    "    # change the ground-truth reward\n",
    "    second_case = np.append(_ss[0:t],[_gts[t-1]])\n",
    "    reward[_gts[t-1]] = self.calc_reward(second_case, _gts)\n",
    "\n",
    "    return reward\n",
    "\n",
    "  def run_train_steps(self, sess, batch, step, q_estimates=None):\n",
    "    \n",
    "    feed_dict = self._make_feed_dict(batch)\n",
    "    if self._hps.ac_training or self._hps.rl_training:\n",
    "      if self._hps.fixed_eta:\n",
    "        feed_dict[self._eta] = self._hps.eta\n",
    "      else:\n",
    "        feed_dict[self._eta] = min(step * self._hps.eta, 1.)\n",
    "    if self._hps.scheduled_sampling:\n",
    "      if self._hps.fixed_sampling_probability:\n",
    "        feed_dict[self._sampling_probability] = self._hps.sampling_probability\n",
    "      else:\n",
    "        feed_dict[self._sampling_probability] = min(step * self._hps.sampling_probability, 1.) # linear decay function\n",
    "      ranges = [np.exp(float(step) * self._hps.alpha), np.finfo(np.float64).max] # to avoid overflow\n",
    "      feed_dict[self._alpha] = np.log(ranges[np.argmin(ranges)]) # linear decay function\n",
    "    if self._hps.ac_training:\n",
    "      self.q_estimates = q_estimates\n",
    "      feed_dict[self._q_estimates]= self.q_estimates\n",
    "    to_return = {\n",
    "        'train_op': self._shared_train_op,\n",
    "        'summaries': self._summaries,\n",
    "        'pgen_loss': self._pgen_loss,\n",
    "        'global_step': self.global_step,\n",
    "        'decoder_outputs': self.decoder_outputs\n",
    "    }\n",
    "\n",
    "    if self._hps.rl_training:\n",
    "      to_return['sampled_sentence_r_values'] = self._sampled_rouges\n",
    "      to_return['greedy_sentence_r_values'] = self._greedy_rouges\n",
    "\n",
    "    if self._hps.coverage:\n",
    "      to_return['coverage_loss'] = self._coverage_loss\n",
    "      if self._hps.rl_training or self._hps.ac_training:\n",
    "        to_return['reinforce_cov_total_loss']= self._reinforce_cov_total_loss\n",
    "      if self._hps.pointer_gen:\n",
    "        to_return['pointer_cov_total_loss'] = self._pointer_cov_total_loss\n",
    "    if self._hps.rl_training or self._hps.ac_training:\n",
    "      to_return['shared_loss']= self._reinforce_shared_loss\n",
    "      to_return['rl_loss']= self._rl_loss\n",
    "      to_return['rl_avg_logprobs']= self._rl_avg_logprobs\n",
    "\n",
    "    # We feed the collected reward and feed it back to model to update the loss\n",
    "    return sess.run(to_return, feed_dict)\n",
    "\n",
    "  def run_eval_step(self, sess, batch, step, q_estimates=None):\n",
    "    \n",
    "    feed_dict = self._make_feed_dict(batch)\n",
    "    if self._hps.ac_training or self._hps.rl_training:\n",
    "      if self._hps.fixed_eta:\n",
    "        feed_dict[self._eta] = self._hps.eta\n",
    "      else:\n",
    "        feed_dict[self._eta] = min(step * self._hps.eta,1.)\n",
    "    if self._hps.scheduled_sampling:\n",
    "      if self._hps.fixed_sampling_probability:\n",
    "        feed_dict[self._sampling_probability] = self._hps.sampling_probability\n",
    "      else:\n",
    "        feed_dict[self._sampling_probability] = min(step * self._hps.sampling_probability,1.) # linear decay function\n",
    "      ranges = [np.exp(float(step) * self._hps.alpha),np.finfo(np.float64).max] # to avoid overflow\n",
    "      feed_dict[self._alpha] = np.log(ranges[np.argmin(ranges)]) # linear decay function\n",
    "    if self._hps.ac_training:\n",
    "      self.q_estimates = q_estimates\n",
    "      feed_dict[self._q_estimates]= self.q_estimates\n",
    "    to_return = {\n",
    "        'summaries': self._summaries,\n",
    "        'pgen_loss': self._pgen_loss,\n",
    "        'global_step': self.global_step,\n",
    "        'decoder_outputs': self.decoder_outputs\n",
    "    }\n",
    "\n",
    "    if self._hps.rl_training:\n",
    "      to_return['sampled_sentence_r_values'] = self._sampled_rouges\n",
    "      to_return['greedy_sentence_r_values'] = self._greedy_rouges\n",
    "\n",
    "    if self._hps.coverage:\n",
    "      to_return['coverage_loss'] = self._coverage_loss\n",
    "      if self._hps.rl_training or self._hps.ac_training:\n",
    "        to_return['reinforce_cov_total_loss']= self._reinforce_cov_total_loss\n",
    "      if self._hps.pointer_gen:\n",
    "        to_return['pointer_cov_total_loss'] = self._pointer_cov_total_loss\n",
    "    if self._hps.rl_training or self._hps.ac_training:\n",
    "      to_return['shared_loss']= self._reinforce_shared_loss\n",
    "      to_return['rl_loss']= self._rl_loss\n",
    "      to_return['rl_avg_logprobs']= self._rl_avg_logprobs\n",
    "\n",
    "    # We feed the collected reward and feed it back to model to update the loss\n",
    "    return sess.run(to_return, feed_dict)\n",
    "\n",
    "  def run_encoder(self, sess, batch):\n",
    "    \n",
    "    feed_dict = self._make_feed_dict(batch, just_enc=True) # feed the batch into the placeholders\n",
    "    (enc_states, dec_in_state, global_step) = sess.run([self._enc_states, self._dec_in_state, self.global_step], feed_dict) # run the encoder\n",
    "\n",
    "    # dec_in_state is LSTMStateTuple shape ([batch_size,hidden_dim],[batch_size,hidden_dim])\n",
    "    # Given that the batch is a single example repeated, dec_in_state is identical across the batch so we just take the top row.\n",
    "    dec_in_state = tf.contrib.rnn.LSTMStateTuple(dec_in_state.c[0], dec_in_state.h[0])\n",
    "    return enc_states, dec_in_state\n",
    "\n",
    "  def decode_onestep(self, sess, batch, latest_tokens, enc_states, dec_init_states, prev_coverage, prev_decoder_outputs, prev_encoder_es):\n",
    "    \n",
    "\n",
    "    beam_size = len(dec_init_states)\n",
    "\n",
    "    # Turn dec_init_states (a list of LSTMStateTuples) into a single LSTMStateTuple for the batch\n",
    "    cells = [np.expand_dims(state.c, axis=0) for state in dec_init_states]\n",
    "    hiddens = [np.expand_dims(state.h, axis=0) for state in dec_init_states]\n",
    "    new_c = np.concatenate(cells, axis=0)  # shape [batch_size,hidden_dim]\n",
    "    new_h = np.concatenate(hiddens, axis=0)  # shape [batch_size,hidden_dim]\n",
    "    new_dec_in_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "    feed = {\n",
    "        self._enc_states: enc_states,\n",
    "        self._enc_padding_mask: batch.enc_padding_mask,\n",
    "        self._dec_in_state: new_dec_in_state,\n",
    "        self._dec_batch: np.transpose(np.array([latest_tokens])),\n",
    "        self._dec_padding_mask: np.ones((beam_size,1),dtype=np.float32)\n",
    "    }\n",
    "\n",
    "    to_return = {\n",
    "      \"ids\": self._topk_ids,\n",
    "      \"probs\": self._topk_log_probs,\n",
    "      \"states\": self._dec_out_state,\n",
    "      \"attn_dists\": self.attn_dists,\n",
    "      \"final_dists\": self.final_dists\n",
    "    }\n",
    "\n",
    "    if FLAGS.pointer_gen:\n",
    "      feed[self._enc_batch_extend_vocab] = batch.enc_batch_extend_vocab\n",
    "      feed[self._max_art_oovs] = batch.max_art_oovs\n",
    "      to_return['p_gens'] = self.p_gens\n",
    "\n",
    "    if self._hps.coverage:\n",
    "      feed[self.prev_coverage] = np.stack(prev_coverage, axis=0)\n",
    "      to_return['coverage'] = self.coverage\n",
    "\n",
    "    if FLAGS.ac_training or FLAGS.intradecoder:\n",
    "      to_return['output']=self.decoder_outputs\n",
    "    if FLAGS.intradecoder:\n",
    "      feed[self.prev_decoder_outputs]= prev_decoder_outputs\n",
    "    if FLAGS.use_temporal_attention:\n",
    "      to_return['temporal_e'] = self.temporal_es\n",
    "      feed[self.prev_encoder_es] = prev_encoder_es\n",
    "\n",
    "    results = sess.run(to_return, feed_dict=feed) # run the decoder step\n",
    "\n",
    "    new_states = [tf.contrib.rnn.LSTMStateTuple(results['states'].c[i, :], results['states'].h[i, :]) for i in range(beam_size)]\n",
    "\n",
    "    assert len(results['attn_dists'])==1\n",
    "    attn_dists = results['attn_dists'][0].tolist()\n",
    "    final_dists = results['final_dists'][0].tolist()\n",
    "\n",
    "    if FLAGS.pointer_gen:\n",
    "      assert len(results['p_gens'])==1\n",
    "      p_gens = results['p_gens'][0].tolist()\n",
    "    else:\n",
    "      p_gens = [None for _ in range(beam_size)]\n",
    "\n",
    "    if FLAGS.ac_training or FLAGS.intradecoder:\n",
    "      output = results['output'][0] # used for calculating the intradecoder at later steps and for calcualting q-estimate in Actor-Critic training.\n",
    "    else:\n",
    "      output = None\n",
    "    if FLAGS.use_temporal_attention:\n",
    "      temporal_e = results['temporal_e'][0] # used for calculating the attention at later steps\n",
    "    else:\n",
    "      temporal_e = None\n",
    "\n",
    "    if FLAGS.coverage:\n",
    "      new_coverage = results['coverage'].tolist()\n",
    "      assert len(new_coverage) == beam_size\n",
    "    else:\n",
    "      new_coverage = [None for _ in range(beam_size)]\n",
    "\n",
    "    return results['ids'], results['probs'], new_states, attn_dists, final_dists, p_gens, new_coverage, output, temporal_e\n",
    "\n",
    "def _mask_and_avg(values, padding_mask):\n",
    "  \n",
    "\n",
    "  dec_lens = tf.reduce_sum(padding_mask, axis=1) # shape batch_size. float32\n",
    "  values_per_step = [v * padding_mask[:,dec_step] for dec_step,v in enumerate(values)] # list of k\n",
    "  values_per_ex = sum(values_per_step)/dec_lens # shape (batch_size); normalized value for each batch member\n",
    "  return tf.reduce_mean(values_per_ex) # overall average\n",
    "\n",
    "def _coverage_loss(attn_dists, padding_mask):\n",
    "  \n",
    "  coverage = tf.zeros_like(attn_dists[0]) # shape (batch_size, attn_length). Initial coverage is zero.\n",
    "  covlosses = [] # Coverage loss per decoder timestep. Will be list length max_dec_steps containing shape (batch_size).\n",
    "  for a in attn_dists:\n",
    "    covloss = tf.reduce_sum(tf.minimum(a, coverage), [1]) # calculate the coverage loss for this step\n",
    "    covlosses.append(covloss)\n",
    "    coverage += a # update the coverage vector\n",
    "  coverage_loss = _mask_and_avg(covlosses, padding_mask)\n",
    "  return coverage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfIt9CWWMN1n"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import queue\n",
    "except:\n",
    "  import Queue as queue\n",
    "from random import shuffle\n",
    "from random import seed\n",
    "seed(123)\n",
    "from threading import Thread\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "try:\n",
    "  import Queue as Q  # ver. < 3.0\n",
    "except ImportError:\n",
    "  import queue as Q\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "PriorityQueue = Q.PriorityQueue\n",
    "\n",
    "\n",
    "class CustomQueue(PriorityQueue):\n",
    "  \n",
    "  def __init__(self, size):\n",
    "    PriorityQueue.__init__(self, size)\n",
    "\n",
    "  def clear(self):\n",
    "    \n",
    "    with self.mutex:\n",
    "      unfinished = self.unfinished_tasks - len(self.queue)\n",
    "      if unfinished <= 0:\n",
    "        if unfinished < 0:\n",
    "          raise ValueError('task_done() called too many times')\n",
    "        self.all_tasks_done.notify_all()\n",
    "      self.queue = self.queue[0:len(self.queue)/2]\n",
    "      self.unfinished_tasks = unfinished + len(self.queue)\n",
    "      self.not_full.notify_all()\n",
    "\n",
    "  def isempty(self):\n",
    "    with self.mutex:\n",
    "      return len(self.queue) == 0\n",
    "\n",
    "  def isfull(self):\n",
    "    with self.mutex:\n",
    "      return len(self.queue) == self.maxsize\n",
    "\n",
    "class Transition(object):\n",
    "  \n",
    "  def __init__(self, state, action, state_prime, action_prime, reward, q_value, done):\n",
    "    \n",
    "    self.state = state # size: dqn_input_feature_len\n",
    "    self.action = action # size: 1\n",
    "    self.state_prime = state_prime # size: dqn_input_feature_len\n",
    "    self.action_prime = action_prime\n",
    "    self.reward = reward # size: vocab_size\n",
    "    self.q_value = q_value # size: vocab_size\n",
    "    self.done = done # true/false\n",
    "\n",
    "  def __cmp__(self, item):\n",
    "    \n",
    "    return cmp(item.reward, self.reward) # bigger numbers have more priority\n",
    "\n",
    "class ReplayBatch(object):\n",
    "  \n",
    "\n",
    "  def __init__(self, hps, example_list, dqn_batch_size, use_state_prime = False, max_art_oovs = 0):\n",
    "    \n",
    "    self._x = np.zeros((dqn_batch_size, hps.dqn_input_feature_len))\n",
    "    self._y = np.zeros((dqn_batch_size, hps.vocab_size))\n",
    "    self._y_extended = np.zeros((dqn_batch_size, hps.vocab_size + max_art_oovs))\n",
    "    for i,e in enumerate(example_list):\n",
    "      if use_state_prime:\n",
    "        self._x[i,:]=e.state_prime\n",
    "      else:\n",
    "        self._x[i,:]=e.state\n",
    "        self._y[i,:]=normalize([e.q_value[0:hps.vocab_size]], axis=1, norm='l1')\n",
    "      if max_art_oovs == 0:\n",
    "        self._y_extended[i,:] = normalize([e.q_value[0:hps.vocab_size]], axis=1, norm='l1')\n",
    "      else:\n",
    "        self._y_extended[i,:] = e.q_value\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "  \n",
    "\n",
    "  BATCH_QUEUE_MAX = 100 # max number of batches the batch_queue can hold\n",
    "\n",
    "  def __init__(self, hps):\n",
    "    self._hps = hps\n",
    "    self._buffer = CustomQueue(self._hps.dqn_replay_buffer_size)\n",
    "\n",
    "    self._batch_queue = queue.Queue(self.BATCH_QUEUE_MAX)\n",
    "    self._example_queue = queue.Queue(self.BATCH_QUEUE_MAX * self._hps.dqn_batch_size)\n",
    "    self._num_example_q_threads = 1 # num threads to fill example queue\n",
    "    self._num_batch_q_threads = 1  # num threads to fill batch queue\n",
    "    self._bucketing_cache_size = 100 # how many batches-worth of examples to load into cache before bucketing\n",
    "\n",
    "    \n",
    "    self._example_q_threads = []\n",
    "    for _ in range(self._num_example_q_threads):\n",
    "      self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
    "      self._example_q_threads[-1].daemon = True\n",
    "      self._example_q_threads[-1].start()\n",
    "    self._batch_q_threads = []\n",
    "    for _ in range(self._num_batch_q_threads):\n",
    "      self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
    "      self._batch_q_threads[-1].daemon = True\n",
    "      self._batch_q_threads[-1].start()\n",
    "\n",
    "    \n",
    "    self._watch_thread = Thread(target=self.watch_threads)\n",
    "    self._watch_thread.daemon = True\n",
    "    self._watch_thread.start()\n",
    "\n",
    "  def next_batch(self):\n",
    "    \n",
    "    if self._batch_queue.qsize() == 0:\n",
    "      tf.logging.warning('Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i', self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "      return None\n",
    "\n",
    "    batch = self._batch_queue.get() # get the next Batch\n",
    "    return batch\n",
    "\n",
    "  def create_batch(_hps, batch, batch_size, use_state_prime=False, max_art_oovs=0):\n",
    "    \n",
    "\n",
    "    return ReplayBatch(_hps, batch, batch_size, use_state_prime, max_art_oovs)\n",
    "\n",
    "  def fill_example_queue(self):\n",
    "    \n",
    "    while True:\n",
    "      try:\n",
    "        input_gen = self._example_generator().next()\n",
    "      except StopIteration: # if there are no more examples:\n",
    "        tf.logging.info(\"The example generator for this example queue filling thread has exhausted data.\")\n",
    "        raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
    "      self._example_queue.put(input_gen) # place the pair in the example queue.\n",
    "\n",
    "  def fill_batch_queue(self):\n",
    "    while True:\n",
    "      # Get bucketing_cache_size-many batches of Examples into a list, then sort\n",
    "      inputs = []\n",
    "      for _ in range(self._hps.dqn_batch_size * self._bucketing_cache_size):\n",
    "        inputs.append(self._example_queue.get())\n",
    "\n",
    "      self.add(inputs)\n",
    "\n",
    "      batches = []\n",
    "      for i in range(0, len(inputs), self._hps.dqn_batch_size):\n",
    "        batches.append(inputs[i:i + self._hps.dqn_batch_size])\n",
    "        shuffle(batches)\n",
    "      for b in batches:  # each b is a list of Example objects\n",
    "        self._batch_queue.put(ReplayBatch(self._hps, b, self._hps.dqn_batch_size))\n",
    "\n",
    "  def watch_threads(self):\n",
    "    while True:\n",
    "      time.sleep(60)\n",
    "      for idx,t in enumerate(self._example_q_threads):\n",
    "        if not t.is_alive(): # if the thread is dead\n",
    "          tf.logging.error('Found example queue thread dead. Restarting.')\n",
    "          new_t = Thread(target=self.fill_example_queue)\n",
    "          self._example_q_threads[idx] = new_t\n",
    "          new_t.daemon = True\n",
    "          new_t.start()\n",
    "      for idx,t in enumerate(self._batch_q_threads):\n",
    "        if not t.is_alive(): # if the thread is dead\n",
    "          tf.logging.error('Found batch queue thread dead. Restarting.')\n",
    "          new_t = Thread(target=self.fill_batch_queue)\n",
    "          self._batch_q_threads[idx] = new_t\n",
    "          new_t.daemon = True\n",
    "          new_t.start()\n",
    "\n",
    "  def add(self, items):\n",
    "    \n",
    "    for item in items:\n",
    "      if not self._buffer.isfull():\n",
    "        self._buffer.put_nowait(item)\n",
    "      else:\n",
    "        print('Replay Buffer is full, getting rid of unimportant transitions...')\n",
    "        self._buffer.clear()\n",
    "        self._buffer.put_nowait(item)\n",
    "    print('ReplayBatch size: {}'.format(self._buffer.qsize()))\n",
    "    print('ReplayBatch example queue size: {}'.format(self._example_queue.qsize()))\n",
    "    print('ReplayBatch batch queue size: {}'.format(self._batch_queue.qsize()))\n",
    "\n",
    "  def _buffer_len(self):\n",
    "    return self._buffer.qsize()\n",
    "\n",
    "  def _example_generator(self):\n",
    "    while True:\n",
    "      if not self._buffer.isempty():\n",
    "        item = self._buffer.get_nowait()\n",
    "        self._buffer.task_done()\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMIXJiEIL3-l"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import data\n",
    "#from replay_buffer import Transition, ReplayBuffer\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "class Hypothesis(object):\n",
    "  \n",
    "\n",
    "  def __init__(self, tokens, log_probs, state, decoder_output, encoder_mask, attn_dists, p_gens, coverage):\n",
    "    \n",
    "    self.tokens = tokens\n",
    "    self.log_probs = log_probs\n",
    "    self.state = state\n",
    "    self.decoder_output = decoder_output\n",
    "    self.encoder_mask = encoder_mask\n",
    "    self.attn_dists = attn_dists\n",
    "    self.p_gens = p_gens\n",
    "    self.coverage = coverage\n",
    "\n",
    "  def extend(self, token, log_prob, state, decoder_output, encoder_mask, attn_dist, p_gen, coverage):\n",
    "    \n",
    "    if FLAGS.avoid_trigrams and self._has_trigram(self.tokens + [token]):\n",
    "        log_prob = -np.infty\n",
    "    return Hypothesis(tokens = self.tokens + [token],\n",
    "                      log_probs = self.log_probs + [log_prob],\n",
    "                      state = state,\n",
    "                      decoder_output= self.decoder_output + [decoder_output] if decoder_output is not None else [],\n",
    "                      encoder_mask = self.encoder_mask + [encoder_mask] if encoder_mask is not None else [],\n",
    "                      attn_dists = self.attn_dists + [attn_dist],\n",
    "                      p_gens = self.p_gens + [p_gen],\n",
    "                      coverage = coverage)\n",
    "\n",
    "  def _find_ngrams(self, input_list, n):\n",
    "      return zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "  def _has_trigram(self, tokens):\n",
    "      tri_grams = self._find_ngrams(tokens, 3)\n",
    "      cnt = Counter(tri_grams)\n",
    "      return not all((cnt[g] == 1 for g in cnt))\n",
    "\n",
    "  def latest_token(self):\n",
    "    return self.tokens[-1]\n",
    "\n",
    "  def log_prob(self):\n",
    "    # the log probability of the hypothesis so far is the sum of the log probabilities of the tokens so far\n",
    "    return sum(self.log_probs)\n",
    "\n",
    "  def avg_log_prob(self):\n",
    "    return self.log_prob / len(self.tokens)\n",
    "\n",
    "\n",
    "def run_beam_search(sess, model, vocab, batch, dqn = None, dqn_sess = None, dqn_graph = None):\n",
    "  \n",
    "  enc_states, dec_in_state = model.run_encoder(sess, batch)\n",
    "  \n",
    "  hyps = [Hypothesis(tokens=[vocab.word2id(START_DECODING)],\n",
    "                     log_probs=[0.0],\n",
    "                     state=dec_in_state,\n",
    "                     decoder_output = [np.zeros([FLAGS.dec_hidden_dim])],\n",
    "                     encoder_mask = [np.zeros([batch.enc_batch.shape[1]])],\n",
    "                     attn_dists=[],\n",
    "                     p_gens=[],\n",
    "                     coverage=np.zeros([batch.enc_batch.shape[1]]) # zero vector of length attention_length\n",
    "                     ) for _ in range(FLAGS.beam_size)]\n",
    "  results = [] # this will contain finished hypotheses (those that have emitted the [STOP] token)\n",
    "\n",
    "  steps = 0\n",
    "  while steps < FLAGS.max_dec_steps and len(results) < FLAGS.beam_size:\n",
    "    latest_tokens = [h.latest_token for h in hyps] # latest token produced by each hypothesis\n",
    "    latest_tokens = [t if t in range(vocab.size()) else vocab.word2id(UNKNOWN_TOKEN) for t in latest_tokens] # change any in-article temporary OOV ids to [UNK] id, so that we can lookup word embeddings\n",
    "    states = [h.state for h in hyps] # list of current decoder states of the hypotheses\n",
    "    prev_coverage = [h.coverage for h in hyps] # list of coverage vectors (or None)\n",
    "    decoder_outputs = np.array([h.decoder_output for h in hyps]).swapaxes(0, 1) # shape (?, batch_size, dec_hidden_dim)\n",
    "    encoder_es = np.array([h.encoder_mask for h in hyps]).swapaxes(0, 1)  # shape (?, batch_size, enc_hidden_dim)\n",
    "    # Run one step of the decoder to get the new info\n",
    "    (topk_ids, topk_log_probs, new_states, attn_dists, final_dists, p_gens, new_coverage, decoder_output, encoder_e) = model.decode_onestep(sess=sess,\n",
    "                        batch=batch,\n",
    "                        latest_tokens=latest_tokens,\n",
    "                        enc_states=enc_states,\n",
    "                        dec_init_states=states,\n",
    "                        prev_coverage=prev_coverage,\n",
    "                        prev_decoder_outputs= decoder_outputs if FLAGS.intradecoder else tf.stack([], axis=0),\n",
    "                        prev_encoder_es = encoder_es if FLAGS.use_temporal_attention else tf.stack([], axis=0))\n",
    "\n",
    "    if FLAGS.ac_training:\n",
    "      with dqn_graph.as_default():\n",
    "        dqn_results = dqn.run_test_steps(dqn_sess, x=decoder_output)\n",
    "        q_estimates = dqn_results['estimates'] # shape (len(transitions), vocab_size)\n",
    "        # we use the q_estimate of UNK token for all the OOV tokens\n",
    "        q_estimates = np.concatenate([q_estimates,np.reshape(q_estimates[:,0],[-1,1])*np.ones((FLAGS.beam_size,batch.max_art_oovs))],axis=-1)\n",
    "        # normalized q_estimate\n",
    "        q_estimates = normalize(q_estimates, axis=1, norm='l1')\n",
    "        combined_estimates = final_dists * q_estimates\n",
    "        combined_estimates = normalize(combined_estimates, axis=1, norm='l1')\n",
    "        # overwriting topk ids and probs\n",
    "        topk_ids = np.argsort(combined_estimates,axis=-1)[:,-FLAGS.beam_size*2:][:,::-1]\n",
    "        topk_probs = [combined_estimates[i,_] for i,_ in enumerate(topk_ids)]\n",
    "        topk_log_probs = np.log(topk_probs)\n",
    "\n",
    "    # Extend each hypothesis and collect them all in all_hyps\n",
    "    all_hyps = []\n",
    "    num_orig_hyps = 1 if steps == 0 else len(hyps) # On the first step, we only had one original hypothesis (the initial hypothesis). On subsequent steps, all original hypotheses are distinct.\n",
    "    for i in range(num_orig_hyps):\n",
    "      h, new_state, attn_dist, p_gen, new_coverage_i = hyps[i], new_states[i], attn_dists[i], p_gens[i], new_coverage[i]  # take the ith hypothesis and new decoder state info\n",
    "      decoder_output_i = None\n",
    "      encoder_mask_i = None\n",
    "      if FLAGS.intradecoder:\n",
    "        decoder_output_i = decoder_output[i]\n",
    "      if FLAGS.use_temporal_attention:\n",
    "        encoder_mask_i = encoder_e[i]\n",
    "      for j in range(FLAGS.beam_size * 2):  # for each of the top 2*beam_size hyps:\n",
    "        new_hyp = h.extend(token=topk_ids[i, j],\n",
    "                           log_prob=topk_log_probs[i, j],\n",
    "                           state=new_state,\n",
    "                           decoder_output = decoder_output_i,\n",
    "                           encoder_mask = encoder_mask_i,\n",
    "                           attn_dist=attn_dist,\n",
    "                           p_gen=p_gen,\n",
    "                           coverage=new_coverage_i)\n",
    "        all_hyps.append(new_hyp)\n",
    "\n",
    "    # Filter and collect any hypotheses that have produced the end token.\n",
    "    hyps = [] # will contain hypotheses for the next step\n",
    "    for h in sort_hyps(all_hyps): # in order of most likely h\n",
    "      if h.latest_token == vocab.word2id(STOP_DECODING): # if stop token is reached...\n",
    "        # If this hypothesis is sufficiently long, put in results. Otherwise discard.\n",
    "        if steps >= FLAGS.min_dec_steps:\n",
    "          results.append(h)\n",
    "      else: # hasn't reached stop token, so continue to extend this hypothesis\n",
    "        hyps.append(h)\n",
    "      if len(hyps) == FLAGS.beam_size or len(results) == FLAGS.beam_size:\n",
    "        break\n",
    "\n",
    "    steps += 1\n",
    "\n",
    "  \n",
    "\n",
    "  if len(results)==0: # if we don't have any complete results, add all current hypotheses (incomplete summaries) to results\n",
    "    results = hyps\n",
    "\n",
    "  \n",
    "  hyps_sorted = sort_hyps(results)\n",
    "\n",
    "  \n",
    "  return hyps_sorted[0]\n",
    "\n",
    "def sort_hyps(hyps):\n",
    "  \n",
    "  return sorted(hyps, key=lambda h: h.avg_log_prob, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDHyrNwaMfVK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "def get_config():\n",
    "  \n",
    "  config = tf.ConfigProto(allow_soft_placement=True)\n",
    "  #config = tf.ConfigProto(log_device_placement=True)\n",
    "  config.gpu_options.allow_growth=True\n",
    "  return config\n",
    "\n",
    "def load_ckpt(saver, sess, ckpt_dir=\"train\"):\n",
    "  \n",
    "  while True:\n",
    "    try:\n",
    "      latest_filename = \"checkpoint_best\" if ckpt_dir==\"eval\" else None\n",
    "      ckpt_dir = os.path.join(FLAGS.log_root, ckpt_dir)\n",
    "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir, latest_filename=latest_filename)\n",
    "      tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\n",
    "      saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "      return ckpt_state.model_checkpoint_path\n",
    "    except:\n",
    "      tf.logging.info(\"Failed to load checkpoint from %s. Sleeping for %i secs...\", ckpt_dir, 10)\n",
    "      time.sleep(10)\n",
    "\n",
    "def load_dqn_ckpt(saver, sess):\n",
    "  \n",
    "  while True:\n",
    "    try:\n",
    "      ckpt_dir = os.path.join(FLAGS.log_root, \"dqn\", \"train\")\n",
    "      ckpt_state = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "      tf.logging.info('Loading checkpoint %s', ckpt_state.model_checkpoint_path)\n",
    "      saver.restore(sess, ckpt_state.model_checkpoint_path)\n",
    "      return ckpt_state.model_checkpoint_path\n",
    "    except:\n",
    "      tf.logging.info(\"Failed to load checkpoint from %s. Sleeping for %i secs...\", ckpt_dir, 10)\n",
    "      time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-vKdSWVJC2b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "#import beam_search\n",
    "#import data\n",
    "import json\n",
    "import pyrouge\n",
    "#import util\n",
    "import logging\n",
    "#from unidecode import unidecode\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "SECS_UNTIL_NEW_CKPT = 60  # max number of seconds before loading new checkpoint\n",
    "\n",
    "article = []\n",
    "reference = []\n",
    "summary = []\n",
    "\n",
    "class BeamSearchDecoder(object):\n",
    "  \n",
    "\n",
    "  def __init__(self, model, batcher, vocab, dqn = None):\n",
    "    \n",
    "    self._model = model\n",
    "    self._model.build_graph()\n",
    "    self._batcher = batcher\n",
    "    self._vocab = vocab\n",
    "    self._saver = tf.train.Saver() # we use this to load checkpoints for decoding\n",
    "    self._sess = tf.Session(config=get_config())\n",
    "\n",
    "    if FLAGS.ac_training:\n",
    "      self._dqn = dqn\n",
    "      self._dqn_graph = tf.Graph()\n",
    "      with self._dqn_graph.as_default():\n",
    "        self._dqn.build_graph()\n",
    "        self._dqn_saver = tf.train.Saver() # we use this to load checkpoints for decoding\n",
    "        self._dqn_sess = tf.Session(config=get_config())\n",
    "        _ = load_dqn_ckpt(self._dqn_saver, self._dqn_sess)\n",
    "\n",
    "    \n",
    "    ckpt_path = load_ckpt(self._saver, self._sess, FLAGS.decode_from)\n",
    "\n",
    "    if FLAGS.single_pass:\n",
    "      \n",
    "      ckpt_name = \"{}-ckpt-\".format(FLAGS.decode_from) + ckpt_path.split('-')[\n",
    "        -1]  # this is something of the form \"ckpt-123456\"\n",
    "      self._decode_dir = os.path.join(FLAGS.log_root, get_decode_dir_name(ckpt_name))\n",
    "    else: \n",
    "      self._decode_dir = os.path.join(FLAGS.log_root, \"decode\")\n",
    "\n",
    "    \n",
    "    if not os.path.exists(self._decode_dir): os.mkdir(self._decode_dir)\n",
    "\n",
    "    if FLAGS.single_pass:\n",
    "      \n",
    "      self._rouge_ref_dir = os.path.join(self._decode_dir, \"reference\")\n",
    "      if not os.path.exists(self._rouge_ref_dir): os.mkdir(self._rouge_ref_dir)\n",
    "      self._rouge_dec_dir = os.path.join(self._decode_dir, \"decoded\")\n",
    "      if not os.path.exists(self._rouge_dec_dir): os.mkdir(self._rouge_dec_dir)\n",
    "\n",
    "  def decode(self):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    counter = FLAGS.decode_after\n",
    "    while True:\n",
    "      tf.reset_default_graph()\n",
    "      batch = self._batcher.next_batch()  # 1 example repeated across batch\n",
    "      if batch is None: # finished decoding dataset in single_pass mode\n",
    "        assert FLAGS.single_pass, \"Dataset exhausted, but we are not in single_pass mode\"\n",
    "        tf.logging.info(\"Decoder has finished reading dataset for single_pass.\")\n",
    "        tf.logging.info(\"Output has been saved in %s and %s. Now starting ROUGE eval...\", self._rouge_ref_dir, self._rouge_dec_dir)\n",
    "        results_dict = rouge_eval(self._rouge_ref_dir, self._rouge_dec_dir)\n",
    "        rouge_log(results_dict, self._decode_dir)\n",
    "        return\n",
    "\n",
    "      original_article = batch.original_articles[0]  # string\n",
    "      original_abstract = batch.original_abstracts[0]  # string\n",
    "      original_abstract_sents = batch.original_abstracts_sents[0]  # list of strings\n",
    "\n",
    "      article_withunks = show_art_oovs(original_article, self._vocab) # string\n",
    "      abstract_withunks = show_abs_oovs(original_abstract, self._vocab, (batch.art_oovs[0] if FLAGS.pointer_gen else None)) # string\n",
    "\n",
    "      \n",
    "      if FLAGS.ac_training:\n",
    "        best_hyp = run_beam_search(self._sess, self._model, self._vocab, batch, self._dqn, self._dqn_sess, self._dqn_graph)\n",
    "      else:\n",
    "        best_hyp = run_beam_search(self._sess, self._model, self._vocab, batch)\n",
    "      # Extract the output ids from the hypothesis and convert back to words\n",
    "      output_ids = [int(t) for t in best_hyp.tokens[1:]]\n",
    "      decoded_words = outputids2words(output_ids, self._vocab, (batch.art_oovs[0] if FLAGS.pointer_gen else None))\n",
    "\n",
    "      try:\n",
    "        fst_stop_idx = decoded_words.index(STOP_DECODING) # index of the (first) [STOP] symbol\n",
    "        decoded_words = decoded_words[:fst_stop_idx]\n",
    "      except ValueError:\n",
    "        decoded_words = decoded_words\n",
    "      decoded_output = ' '.join(decoded_words) # single string\n",
    "\n",
    "      if FLAGS.single_pass:#me when \n",
    "        self.write_for_rouge(original_abstract_sents, decoded_words, counter) # write ref summary and decoded summary to file, to eval with pyrouge later\n",
    "        \n",
    "        article.append(article_withunks)\n",
    "        reference.append(abstract_withunks)\n",
    "        summary.append(decoded_output)\n",
    "        \n",
    "        counter += 1 # this is how many examples we've decoded\n",
    "        if counter == 10 :\n",
    "            tf.logging.info(\"Counter 10 stopped.\")\n",
    "            return\n",
    "      else:\n",
    "        print_results(article_withunks, abstract_withunks, decoded_output) # log output to screen\n",
    "        self.write_for_attnvis(article_withunks, abstract_withunks, decoded_words, best_hyp.attn_dists, best_hyp.p_gens) # write info to .json file for visualization tool\n",
    "\n",
    "        t1 = time.time()\n",
    "        if t1-t0 > SECS_UNTIL_NEW_CKPT:\n",
    "          tf.logging.info('We\\'ve been decoding with same checkpoint for %i seconds. Time to load new checkpoint', t1-t0)\n",
    "          _ = load_ckpt(self._saver, self._sess, FLAGS.decode_from)\n",
    "          t0 = time.time()\n",
    "\n",
    "  def remove_non_ascii(self, text):\n",
    "    \n",
    "    return text #str(unidecode(text))\n",
    "\n",
    "  def write_for_rouge(self, reference_sents, decoded_words, ex_index):\n",
    "    \n",
    "    decoded_sents = []\n",
    "    while len(decoded_words) > 0:\n",
    "      try:\n",
    "        fst_period_idx = decoded_words.index(\".\")\n",
    "      except ValueError: # there is text remaining that doesn't end in \".\"\n",
    "        fst_period_idx = len(decoded_words)\n",
    "      sent = decoded_words[:fst_period_idx+1] # sentence up to and including the period\n",
    "      decoded_words = decoded_words[fst_period_idx+1:] # everything else\n",
    "      decoded_sents.append(' '.join(sent))\n",
    "\n",
    "    decoded_sents = [self.remove_non_ascii(make_html_safe(w)) for w in decoded_sents]\n",
    "    reference_sents = [self.remove_non_ascii(make_html_safe(w)) for w in reference_sents]\n",
    "\n",
    "    # Write to file\n",
    "    ref_file = os.path.join(self._rouge_ref_dir, \"%06d_reference.txt\" % ex_index)\n",
    "    decoded_file = os.path.join(self._rouge_dec_dir, \"%06d_decoded.txt\" % ex_index)\n",
    "\n",
    "    with open(ref_file, \"w\",encoding='utf-8') as f:\n",
    "      for idx,sent in enumerate(reference_sents):\n",
    "        f.write(sent) if idx==len(reference_sents)-1 else f.write(sent+\"\\n\")\n",
    "    with open(decoded_file, \"w\",encoding='utf-8') as f:\n",
    "      for idx,sent in enumerate(decoded_sents):\n",
    "        f.write(sent) if idx==len(decoded_sents)-1 else f.write(sent+\"\\n\")\n",
    "\n",
    "    tf.logging.info(\"Wrote example %i to file\" % ex_index)\n",
    "\n",
    "  def write_for_attnvis(self, article, abstract, decoded_words, attn_dists, p_gens):\n",
    "    \n",
    "    article_lst = article.split() # list of words\n",
    "    decoded_lst = decoded_words # list of decoded words\n",
    "    to_write = {\n",
    "        'article_lst': [make_html_safe(t) for t in article_lst],\n",
    "        'decoded_lst': [make_html_safe(t) for t in decoded_lst],\n",
    "        'abstract_str': make_html_safe(abstract),\n",
    "        'attn_dists': attn_dists\n",
    "    }\n",
    "    if FLAGS.pointer_gen:\n",
    "      to_write['p_gens'] = p_gens\n",
    "    output_fname = os.path.join(self._decode_dir, 'attn_vis_data.json')\n",
    "    with open(output_fname, 'w') as output_file:\n",
    "      json.dump(to_write, output_file)\n",
    "    tf.logging.info('Wrote visualization data to %s', output_fname)\n",
    "\n",
    "\n",
    "def print_results(article, abstract, decoded_output):\n",
    "  \n",
    "  print(\"\")\n",
    "  tf.logging.info('ARTICLE:  %s', article)\n",
    "  tf.logging.info('REFERENCE SUMMARY: %s', abstract)\n",
    "  tf.logging.info('GENERATED SUMMARY: %s', decoded_output)\n",
    "  print(\"\")\n",
    "\n",
    "\n",
    "def make_html_safe(s):\n",
    "  try:\n",
    "    \n",
    "    s.replace(\"<\", \"&lt;\")\n",
    "    s.replace(\">\", \"&gt;\")\n",
    "  except:\n",
    "    pass\n",
    "  return s\n",
    "\n",
    "\n",
    "def rouge_eval(ref_dir, dec_dir):\n",
    "  \n",
    "  r = pyrouge.Rouge155()\n",
    "  r.model_filename_pattern = '#ID#_reference.txt'\n",
    "  r.system_filename_pattern = '(\\d+)_decoded.txt'\n",
    "  r.model_dir = ref_dir\n",
    "  r.system_dir = dec_dir\n",
    "  logging.getLogger('global').setLevel(logging.WARNING) # silence pyrouge logging\n",
    "  rouge_results = r.convert_and_evaluate()\n",
    "  return r.output_to_dict(rouge_results)\n",
    "\n",
    "\n",
    "def rouge_log(results_dict, dir_to_write):\n",
    "  \n",
    "  log_str = \"\"\n",
    "  for x in [\"1\",\"2\",\"l\"]:\n",
    "    log_str += \"\\nROUGE-%s:\\n\" % x\n",
    "    for y in [\"f_score\", \"recall\", \"precision\"]:\n",
    "      key = \"rouge_%s_%s\" % (x,y)\n",
    "      key_cb = key + \"_cb\"\n",
    "      key_ce = key + \"_ce\"\n",
    "      val = results_dict[key]\n",
    "      val_cb = results_dict[key_cb]\n",
    "      val_ce = results_dict[key_ce]\n",
    "      log_str += \"%s: %.4f with confidence interval (%.4f, %.4f)\\n\" % (key, val, val_cb, val_ce)\n",
    "  tf.logging.info(log_str) # log to screen\n",
    "  results_file = os.path.join(dir_to_write, \"ROUGE_results.txt\")\n",
    "  tf.logging.info(\"Writing final ROUGE results to %s...\", results_file)\n",
    "  with open(results_file, \"w\") as f:\n",
    "    f.write(log_str)\n",
    "\n",
    "def get_decode_dir_name(ckpt_name):\n",
    "  \n",
    "  if \"train\" in FLAGS.data_path: dataset = \"train\"\n",
    "  elif \"val\" in FLAGS.data_path: dataset = \"val\"\n",
    "  elif \"test\" in FLAGS.data_path: dataset = \"test\"\n",
    "  else: raise ValueError(\"FLAGS.data_path %s should contain one of train, val or test\" % (FLAGS.data_path))\n",
    "  dirname = \"decode_%s_%s_%imaxenc_%ibeam_%imindec_%imaxdec\" % (dataset, FLAGS.decode_from, FLAGS.max_enc_steps, FLAGS.beam_size, FLAGS.min_dec_steps, FLAGS.max_dec_steps)\n",
    "  if ckpt_name is not None:\n",
    "    dirname += \"_%s\" % ckpt_name\n",
    "  return dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmnGbytcJPb7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorlayer as tl\n",
    "import numpy as np\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self, hps, name_variable):\n",
    "        self._hps = hps\n",
    "        self._name_variable = name_variable\n",
    "\n",
    "    def variable_summaries(self, var_name, var):\n",
    "        \n",
    "        with tf.name_scope('summaries_{}'.format(var_name)):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "\n",
    "    def _add_placeholders(self):\n",
    "        \n",
    "        self._x = tf.placeholder(tf.float32, [None, self._hps.dqn_input_feature_len], name='x') # size (dataset_len, input_feature_len)\n",
    "        self._y = tf.placeholder(tf.float32, [None, self._hps.vocab_size], name='y') # size (dataset_len, 1)\n",
    "        self._train_step = tf.placeholder(tf.int32, None,name='train_step')\n",
    "\n",
    "    def _make_feed_dict(self, batch):\n",
    "        feed_dict = {}\n",
    "        feed_dict[self._x] = batch._x\n",
    "        feed_dict[self._y] = batch._y\n",
    "        return feed_dict\n",
    "\n",
    "    def _add_tf_layers(self):\n",
    "        \n",
    "\n",
    "        h = tf.layers.dense(self._x, units = self._hps.dqn_input_feature_len, activation=tf.nn.relu, name='{}_input_layer'.format(self._name_variable))\n",
    "        for i, layer in enumerate(self._hps.dqn_layers.split(',')):\n",
    "            h = tf.layers.dense(h, units = int(layer), activation = tf.nn.relu, name='{}_h_{}'.format(self._name_variable, i))\n",
    "\n",
    "        self.advantage_layer = tf.layers.dense(h, units = self._hps.vocab_size, activation = tf.nn.softmax, name='{}_advantage'.format(self._name_variable))\n",
    "        if self._hps.dueling_net:\n",
    "            # in dueling net, we have two extra output layers; one for value function estimation\n",
    "            # and the other for advantage estimation, we then use the difference between these two layers\n",
    "            # to calculate the q-estimation\n",
    "            self_layer = tf.layers.dense(h, units = 1, activation = tf.identity, name='{}_value'.format(self._name_variable))\n",
    "            normalized_al = self.advantage_layer-tf.reshape(tf.reduce_mean(self.advantage_layer,axis=1),[-1,1]) # equation 9 in https://arxiv.org/pdf/1511.06581.pdf\n",
    "            value_extended = tf.concat([self_layer] * self._hps.vocab_size, axis=1)\n",
    "            self.output = value_extended + normalized_al\n",
    "        else:\n",
    "            self.output = self.advantage_layer\n",
    "\n",
    "    def _add_train_op(self):\n",
    "        # In regression, the objective loss is Mean Squared Error (MSE).\n",
    "        self.loss = tf.losses.mean_squared_error(labels = self._y, predictions = self.output)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.loss, tvars, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
    "\n",
    "        # Clip the gradients\n",
    "        with tf.device(\"/gpu:{}\".format(self._hps.dqn_gpu_num)):\n",
    "            grads, global_norm = tf.clip_by_global_norm(gradients, self._hps.max_grad_norm)\n",
    "\n",
    "        # Add a summary\n",
    "        tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "        # Apply adagrad optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self._hps.lr)\n",
    "        with tf.device(\"/gpu:{}\".format(self._hps.dqn_gpu_num)):\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=self.global_step, name='train_step')\n",
    "\n",
    "        self.variable_summaries('dqn_loss',self.loss)\n",
    "\n",
    "    def _add_update_weights_op(self):\n",
    "        \n",
    "        self.model_trainables = tf.trainable_variables(scope='{}_relay_network'.format(self._name_variable)) # target variables\n",
    "        self._new_trainables = [tf.placeholder(tf.float32, None,name='trainables_{}'.format(i)) for i in range(len(self.model_trainables))]\n",
    "        self.assign_ops = []\n",
    "        if self._hps.dqn_polyak_averaging: # target parameters are slowly updating using: \\phi_target = \\tau * \\phi_target + (1-\\tau) * \\phi_target\n",
    "            tau = (tf.cast(self._train_step,tf.float32) % self._hps.dqn_target_update)/float(self._hps.dqn_target_update)\n",
    "            for i, mt in enumerate(self.model_trainables):\n",
    "                nt = self._new_trainables[i]\n",
    "                self.assign_ops.append(mt.assign(tau * mt + (1-tau) * nt))\n",
    "        else:\n",
    "          if self._train_step % self._hps.dqn_target_update == 0:\n",
    "            for i, mt in enumerate(self.model_trainables):\n",
    "                nt = self._new_trainables[i]\n",
    "                self.assign_ops.append(mt.assign(nt))\n",
    "\n",
    "    def build_graph(self):\n",
    "        with tf.variable_scope('{}_relay_network'.format(self._name_variable)), tf.device(\"/gpu:{}\".format(self._hps.dqn_gpu_num)):\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self._add_placeholders()\n",
    "            self._add_tf_layers()\n",
    "            self._add_train_op()\n",
    "            self._add_update_weights_op()\n",
    "            self._summaries = tf.summary.merge_all()\n",
    "\n",
    "    def run_train_steps(self, sess, batch):\n",
    "        feed_dict = self._make_feed_dict(batch)\n",
    "        to_return = {'train_op': self.train_op,\n",
    "        'summaries': self._summaries,\n",
    "        'loss': self.loss,\n",
    "        'global_step': self.global_step}\n",
    "        return sess.run(to_return, feed_dict)\n",
    "\n",
    "    def run_test_steps(self, sess, x, y=None, return_loss=False, return_best_action=False):\n",
    "        # when return_loss is True, the model will return the loss of the prediction\n",
    "        # return_loss should be False, during estimation (decoding)\n",
    "        feed_dict = {self._x:x}\n",
    "        to_return = {'estimates': self.output}\n",
    "        if return_loss:\n",
    "            feed_dict.update({self._y:y})\n",
    "            to_return.update({'loss': self.loss})\n",
    "        output = sess.run(to_return, feed_dict)\n",
    "        if return_best_action:\n",
    "            output['best_action']=np.argmax(output['estimates'],axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def run_update_weights(self, sess, train_step, weights):\n",
    "        feed_dict = {self._train_step:train_step}\n",
    "        for i, w in enumerate(weights):\n",
    "            feed_dict.update({self._new_trainables[i]:w})\n",
    "        _ = sess.run(self.assign_ops, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydqQhOWz213h"
   },
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "from functools import reduce\n",
    "\n",
    "def prettify(elem):\n",
    "    \n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "  \n",
    "from xml.etree.ElementTree import Element, SubElement, Comment\n",
    "\n",
    "\n",
    "def zaksum(article , reference , summary_array , directory_path):\n",
    "  top = Element('ZakSum')\n",
    "\n",
    "  comment = Comment('Generated without scores')\n",
    "  top.append(comment)\n",
    "\n",
    "  i=0\n",
    "  for summ in summary_array:\n",
    "    example = SubElement(top, 'example')\n",
    "    article_element   = SubElement(example, 'article')\n",
    "    article_element.text = article[i]\n",
    "\n",
    "    reference_element = SubElement(example, 'reference')\n",
    "    reference_element.text = reference[i]\n",
    "\n",
    "    summary_element   = SubElement(example, 'summary')\n",
    "    summary_element.text = summ\n",
    "    i+=1\n",
    "    \n",
    "  with open(directory_path + \"result_rlfeaasf_9_1_2019_1_46am.xml\", \"w\") as f:\n",
    "    f.write(prettify(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvV0aTajHL1U"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tensorflow.python import debug as tf_debug\n",
    "#from replay_buffer import ReplayBuffer\n",
    "#from dqn import DQN\n",
    "from threading import Thread\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import gen_array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops.distributions import bernoulli\n",
    "import logging\n",
    "\n",
    "\n",
    "#FLAGS = tf.app.flags.FLAGS\n",
    "#FLAGS.remove_flag_values(FLAGS.flag_values_dict()) \n",
    "\n",
    "\n",
    "class Seq2Seq(object):\n",
    "\n",
    "  def calc_running_avg_loss(self, loss, running_avg_loss, step, decay=0.99):\n",
    "    \n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "      running_avg_loss = loss\n",
    "    else:\n",
    "      running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    loss_sum = tf.Summary()\n",
    "    \n",
    "    tag_name = 'running_avg_loss/decay=%f' % (decay)\n",
    "    loss_sum.value.add(tag=tag_name, simple_value=running_avg_loss)\n",
    "    self.summary_writer.add_summary(loss_sum, step)\n",
    "    tf.logging.info('running_avg_loss: %f', running_avg_loss)\n",
    "    return running_avg_loss\n",
    "\n",
    "  def restore_best_model(self):\n",
    "    \n",
    "    tf.logging.info(\"Restoring bestmodel for training...\")\n",
    "\n",
    "    \n",
    "    #sess = tf.InteractiveSession(config=get_config())#me\n",
    "    sess = tf.Session(config=get_config())\n",
    "    print(\"Initializing all variables...\")\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    #tf.reset_default_graph() #me\n",
    "    saver = tf.train.Saver([v for v in tf.all_variables() if \"Adagrad\" not in v.name])\n",
    "    print(\"Restoring all non-adagrad variables from best model in eval dir...\")\n",
    "    curr_ckpt = load_ckpt(saver, sess, \"eval\")\n",
    "    print(\"Restored %s.\" % curr_ckpt)\n",
    "\n",
    "    new_model_name = curr_ckpt.split(\"/\")[-1].replace(\"bestmodel\", \"model\")\n",
    "    new_fname = os.path.join(FLAGS.log_root, \"train\", new_model_name)\n",
    "    print(\"Saving model to %s...\" % (new_fname))\n",
    "    log_file_handler.write(\"Saving model to %s...\" % (new_fname)+\"\\n\")\n",
    "\n",
    "    new_saver = tf.train.Saver()\n",
    "    new_saver.save(sess, new_fname)\n",
    "    print(\"Saved.\")\n",
    "    exit()\n",
    "\n",
    "  def restore_best_eval_model(self):\n",
    "    \n",
    "    best_loss = None\n",
    "    best_step = None\n",
    "    \n",
    "    event_files = sorted(glob('{}/eval/events*'.format(FLAGS.log_root)))\n",
    "    for ef in event_files:\n",
    "      try:\n",
    "        for e in tf.train.summary_iterator(ef):\n",
    "          for v in e.summary.value:\n",
    "            step = e.step\n",
    "            if 'running_avg_loss/decay' in v.tag:\n",
    "              running_avg_loss = v.simple_value\n",
    "              if best_loss is None or running_avg_loss < best_loss:\n",
    "                best_loss = running_avg_loss\n",
    "                best_step = step\n",
    "      except:\n",
    "        continue\n",
    "    tf.logging.info('resotring best loss from the current logs: {}\\tstep: {}'.format(best_loss, best_step))\n",
    "    return best_loss\n",
    "\n",
    "  def convert_to_coverage_model(self):\n",
    "    \n",
    "    tf.logging.info(\"converting non-coverage model to coverage model..\")\n",
    "\n",
    "    \n",
    "    sess = tf.Session(config=get_config())\n",
    "    print(\"initializing everything...\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    saver = tf.train.Saver([v for v in tf.global_variables() if \"coverage\" not in v.name and \"Adagrad\" not in v.name])\n",
    "    print(\"restoring non-coverage variables...\")\n",
    "    curr_ckpt = load_ckpt(saver, sess)\n",
    "    print(\"restored.\")\n",
    "\n",
    "    new_fname = curr_ckpt + '_cov_init'\n",
    "    print(\"saving model to %s...\" % (new_fname))\n",
    "    new_saver = tf.train.Saver() # this one will save all variables that now exist\n",
    "    new_saver.save(sess, new_fname)\n",
    "    print(\"saved.\")\n",
    "    exit()\n",
    "\n",
    "  def convert_to_reinforce_model(self):\n",
    "    tf.logging.info(\"converting non-reinforce model to reinforce model..\")\n",
    "\n",
    "    sess = tf.Session(config=get_config())\n",
    "    print(\"initializing everything...\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    saver = tf.train.Saver([v for v in tf.global_variables() if \"reinforce\" not in v.name and \"Adagrad\" not in v.name])\n",
    "    print(\"restoring non-reinforce variables...\")\n",
    "    curr_ckpt = load_ckpt(saver, sess)\n",
    "    print(\"restored.\")\n",
    "\n",
    "    # save this model and quit\n",
    "    new_fname = curr_ckpt + '_rl_init'\n",
    "    print(\"saving model to %s...\" % (new_fname))\n",
    "    new_saver = tf.train.Saver() # this one will save all variables that now exist\n",
    "    new_saver.save(sess, new_fname)\n",
    "    print(\"saved.\")\n",
    "    exit()\n",
    "\n",
    "  def setup_training(self):\n",
    "    log = logging.getLogger('tensorflow')\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    fh = logging.FileHandler(default_path + 'tensorflow.log')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(formatter)\n",
    "    log.addHandler(fh)\n",
    "    \n",
    "    train_dir = os.path.join(FLAGS.log_root, \"train\")\n",
    "    if not os.path.exists(train_dir): os.makedirs(train_dir)\n",
    "    if FLAGS.ac_training:\n",
    "      dqn_train_dir = os.path.join(FLAGS.log_root, \"dqn\", \"train\")\n",
    "      if not os.path.exists(dqn_train_dir): os.makedirs(dqn_train_dir)\n",
    "    #replaybuffer_pcl_path = os.path.join(FLAGS.log_root, \"replaybuffer.pcl\")\n",
    "    #if not os.path.exists(dqn_target_train_dir): os.makedirs(dqn_target_train_dir)\n",
    "\n",
    "    self.model.build_graph() # build the graph\n",
    "\n",
    "    if FLAGS.convert_to_reinforce_model:\n",
    "      assert (FLAGS.rl_training or FLAGS.ac_training), \"To convert your pointer model to a reinforce model, run with convert_to_reinforce_model=True and either rl_training=True or ac_training=True\"\n",
    "      self.convert_to_reinforce_model()\n",
    "    if FLAGS.convert_to_coverage_model:\n",
    "      assert FLAGS.coverage, \"To convert your non-coverage model to a coverage model, run with convert_to_coverage_model=True and coverage=True\"\n",
    "      self.convert_to_coverage_model()\n",
    "    if FLAGS.restore_best_model:\n",
    "      self.restore_best_model()\n",
    "    saver = tf.train.Saver(max_to_keep=3) # keep 3 checkpoints at a time\n",
    "\n",
    "    # Loads pre-trained word-embedding. By default the model learns the embedding.\n",
    "    if FLAGS.embedding:\n",
    "      self.vocab.LoadWordEmbedding(FLAGS.embedding, FLAGS.emb_dim)\n",
    "      word_vector = self.vocab.getWordEmbedding()\n",
    "\n",
    "    self.sv = tf.train.Supervisor(logdir=train_dir,\n",
    "                       is_chief=True,\n",
    "                       saver=saver,\n",
    "                       summary_op=None,\n",
    "                       save_summaries_secs=60, # save summaries for tensorboard every 60 secs\n",
    "                       save_model_secs=60, # checkpoint every 60 secs\n",
    "                       global_step=self.model.global_step,\n",
    "                       init_feed_dict= {self.model.embedding_place:word_vector} if FLAGS.embedding else None\n",
    "                       )\n",
    "    self.summary_writer = self.sv.summary_writer\n",
    "    self.sess = self.sv.prepare_or_wait_for_session(config=get_config())\n",
    "    if FLAGS.ac_training:\n",
    "      tf.logging.info('DDQN building graph')\n",
    "      t1 = time.time()\n",
    "      self.dqn_graph = tf.Graph()\n",
    "      with self.dqn_graph.as_default():\n",
    "        self.dqn.build_graph() \n",
    "        tf.logging.info('building current network took {} seconds'.format(time.time()-t1))\n",
    "\n",
    "        self.dqn_target.build_graph() \n",
    "        tf.logging.info('building target network took {} seconds'.format(time.time()-t1))\n",
    "\n",
    "        dqn_saver = tf.train.Saver(max_to_keep=3) # keep 3 checkpoints at a time\n",
    "        self.dqn_sv = tf.train.Supervisor(logdir=dqn_train_dir,\n",
    "                           is_chief=True,\n",
    "                           saver=dqn_saver,\n",
    "                           summary_op=None,\n",
    "                           save_summaries_secs=60, # save summaries for tensorboard every 60 secs\n",
    "                           save_model_secs=60, # checkpoint every 60 secs\n",
    "                           global_step=self.dqn.global_step,\n",
    "                           )\n",
    "        self.dqn_summary_writer = self.dqn_sv.summary_writer\n",
    "        self.dqn_sess = self.dqn_sv.prepare_or_wait_for_session(config=get_config())\n",
    "      \n",
    "      self.replay_buffer = ReplayBuffer(self.dqn_hps)\n",
    "    tf.logging.info(\"Preparing or waiting for session...\")\n",
    "    tf.logging.info(\"Created session.\")\n",
    "    try:\n",
    "      self.run_training() # this is an infinite loop until interrupted\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "      tf.logging.info(\"Caught keyboard interrupt on worker. Stopping supervisor...\")\n",
    "      self.sv.stop()\n",
    "      if FLAGS.ac_training:\n",
    "        self.dqn_sv.stop()\n",
    "\n",
    "  def run_training(self):\n",
    "    tf.logging.info(\"Starting run_training\")\n",
    "    log_file_handler.write(\"starting run_training..\\n\")\n",
    "    \n",
    "    if FLAGS.debug: \n",
    "      self.sess = tf_debug.LocalCLIDebugWrapperSession(self.sess)\n",
    "      self.sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "\n",
    "    self.train_step = 0\n",
    "    if FLAGS.ac_training:\n",
    "      \n",
    "      tf.logging.info('Starting DQN training thread...')\n",
    "      self.dqn_train_step = 0\n",
    "      self.thrd_dqn_training = Thread(target=self.dqn_training)\n",
    "      self.thrd_dqn_training.daemon = True\n",
    "      self.thrd_dqn_training.start()\n",
    "\n",
    "      watcher = Thread(target=self.watch_threads)\n",
    "      watcher.daemon = True\n",
    "      watcher.start()\n",
    "    \n",
    "    tf.logging.info('Starting Seq2Seq training...')\n",
    "    log_file_handler.write(\"Starting Seq2Seq training...\\n\")\n",
    "    while True: \n",
    "      batch = self.batcher.next_batch()\n",
    "      t0=time.time()\n",
    "      if FLAGS.ac_training:\n",
    "        \n",
    "        transitions = self.model.collect_dqn_transitions(self.sess, batch, self.train_step, batch.max_art_oovs) # len(batch_size * k * max_dec_steps)\n",
    "        tf.logging.info('Q-values collection time: {}'.format(time.time()-t0))\n",
    "        \n",
    "        with self.dqn_graph.as_default():\n",
    "          batch_len = len(transitions)\n",
    "          \n",
    "          b = ReplayBuffer.create_batch(self.dqn_hps, transitions,len(transitions), use_state_prime = False, max_art_oovs = batch.max_art_oovs)\n",
    "          \n",
    "          b_prime = ReplayBuffer.create_batch(self.dqn_hps, transitions,len(transitions), use_state_prime = True, max_art_oovs = batch.max_art_oovs)\n",
    "          \n",
    "          dqn_results = self.dqn.run_test_steps(sess=self.dqn_sess, x= b._x, return_best_action=True)\n",
    "          q_estimates = dqn_results['estimates'] # shape (len(transitions), vocab_size)\n",
    "          dqn_best_action = dqn_results['best_action']\n",
    "          #dqn_q_estimate_loss = dqn_results['loss']\n",
    "\n",
    "          dqn_target_results = self.dqn_target.run_test_steps(self.dqn_sess, x= b_prime._x)\n",
    "          q_vals_new_t = dqn_target_results['estimates'] # shape (len(transitions), vocab_size)\n",
    "\n",
    "          q_estimates = np.concatenate([q_estimates,\n",
    "            np.reshape(q_estimates[:,0],[-1,1])*np.ones((len(transitions),batch.max_art_oovs))],axis=-1)\n",
    "          for i, tr in enumerate(transitions):\n",
    "            if tr.done:\n",
    "              q_estimates[i][tr.action] = tr.reward\n",
    "            else:\n",
    "              q_estimates[i][tr.action] = tr.reward + FLAGS.gamma * q_vals_new_t[i][dqn_best_action[i]]\n",
    "          if FLAGS.dqn_scheduled_sampling:\n",
    "            q_estimates = self.scheduled_sampling(batch_len, FLAGS.sampling_probability, b._y_extended, q_estimates)\n",
    "          if not FLAGS.calculate_true_q:\n",
    "            # when we are not training DDQN based on true Q-values,\n",
    "            # we need to update Q-values in our transitions based on the q_estimates we collected from DQN current network.\n",
    "            for trans, q_val in zip(transitions,q_estimates):\n",
    "              trans.q_values = q_val # each have the size vocab_extended\n",
    "          q_estimates = np.reshape(q_estimates, [FLAGS.batch_size, FLAGS.k, FLAGS.max_dec_steps, -1]) # shape (batch_size, k, max_dec_steps, vocab_size_extended)\n",
    "        \n",
    "        self.replay_buffer.add(transitions)\n",
    "        \n",
    "        if FLAGS.dqn_pretrain:\n",
    "          tf.logging.info('RUNNNING DQN PRETRAIN: Adding data to relplay buffer only...')\n",
    "          continue\n",
    "        \n",
    "        results = self.model.run_train_steps(self.sess, batch, self.train_step, q_estimates)\n",
    "      else:\n",
    "          results = self.model.run_train_steps(self.sess, batch, self.train_step)\n",
    "      t1=time.time()\n",
    "      \n",
    "      summaries = results['summaries'] \n",
    "      self.train_step = results['global_step'] \n",
    "      tf.logging.info('seconds for training step {}: {}'.format(self.train_step, t1-t0))\n",
    "\n",
    "      printer_helper = {}\n",
    "      printer_helper['pgen_loss']= results['pgen_loss']\n",
    "      if FLAGS.coverage:\n",
    "        printer_helper['coverage_loss'] = results['coverage_loss']\n",
    "        if FLAGS.rl_training or FLAGS.ac_training:\n",
    "          printer_helper['rl_cov_total_loss']= results['reinforce_cov_total_loss']\n",
    "        else:\n",
    "          printer_helper['pointer_cov_total_loss'] = results['pointer_cov_total_loss']\n",
    "      if FLAGS.rl_training or FLAGS.ac_training:\n",
    "        printer_helper['shared_loss'] = results['shared_loss']\n",
    "        printer_helper['rl_loss'] = results['rl_loss']\n",
    "        printer_helper['rl_avg_logprobs'] = results['rl_avg_logprobs']\n",
    "      if FLAGS.rl_training:\n",
    "        printer_helper['sampled_r'] = np.mean(results['sampled_sentence_r_values'])\n",
    "        printer_helper['greedy_r'] = np.mean(results['greedy_sentence_r_values'])\n",
    "        printer_helper['r_diff'] = printer_helper['greedy_r'] - printer_helper['sampled_r']\n",
    "      if FLAGS.ac_training:\n",
    "        printer_helper['dqn_loss'] = np.mean(self.avg_dqn_loss) if len(self.avg_dqn_loss)>0 else 0\n",
    "\n",
    "      for (k,v) in printer_helper.items():\n",
    "        if not np.isfinite(v):\n",
    "          raise Exception(\"{} is not finite. Stopping.\".format(k))\n",
    "        tf.logging.info('{}: {}\\t'.format(k,v))\n",
    "      tf.logging.info('-------------------------------------------')\n",
    "\n",
    "      self.summary_writer.add_summary(summaries, self.train_step) \n",
    "      if self.train_step % 100 == 0: \n",
    "        self.summary_writer.flush()\n",
    "      if FLAGS.ac_training:\n",
    "        self.dqn_summary_writer.flush()\n",
    "      if self.train_step > FLAGS.max_iter: break\n",
    "\n",
    "  def dqn_training(self):\n",
    "    \n",
    "    try:\n",
    "      while True:\n",
    "        if self.dqn_train_step == FLAGS.dqn_pretrain_steps: raise SystemExit()\n",
    "        _t = time.time()\n",
    "        self.avg_dqn_loss = []\n",
    "        avg_dqn_target_loss = []\n",
    "        \n",
    "        dqn_batch = self.replay_buffer.next_batch()\n",
    "        if dqn_batch is None:\n",
    "          tf.logging.info('replay buffer not loaded enough yet...')\n",
    "          time.sleep(60)\n",
    "          continue\n",
    "        \n",
    "        dqn_results = self.dqn.run_train_steps(self.dqn_sess, dqn_batch)\n",
    "        \n",
    "        dqn_target_results = self.dqn_target.run_test_steps(self.dqn_sess, x=dqn_batch._x, y=dqn_batch._y, return_loss=True)\n",
    "        self.dqn_train_step = dqn_results['global_step']\n",
    "        self.dqn_summary_writer.add_summary(dqn_results['summaries'], self.dqn_train_step) # write the summaries\n",
    "        self.avg_dqn_loss.append(dqn_results['loss'])\n",
    "        avg_dqn_target_loss.append(dqn_target_results['loss'])\n",
    "        self.dqn_train_step = self.dqn_train_step + 1\n",
    "        tf.logging.info('seconds for training dqn model: {}'.format(time.time()-_t))\n",
    "        \n",
    "        with self.dqn_graph.as_default():\n",
    "          current_model_weights = self.dqn_sess.run([self.dqn.model_trainables])[0] # get weights of current model\n",
    "          self.dqn_target.run_update_weights(self.dqn_sess, self.dqn_train_step, current_model_weights) # update target model weights with current model weights\n",
    "        tf.logging.info('DQN loss at step {}: {}'.format(self.dqn_train_step, np.mean(self.avg_dqn_loss)))\n",
    "        tf.logging.info('DQN Target loss at step {}: {}'.format(self.dqn_train_step, np.mean(avg_dqn_target_loss)))\n",
    "        \n",
    "        time.sleep(FLAGS.dqn_sleep_time)\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "      tf.logging.info(\"Caught keyboard interrupt on worker. Stopping supervisor...\")\n",
    "      self.sv.stop()\n",
    "      self.dqn_sv.stop()\n",
    "\n",
    "  def watch_threads(self):\n",
    "    \n",
    "    while True:\n",
    "      time.sleep(60)\n",
    "      if not self.thrd_dqn_training.is_alive(): # if the thread is dead\n",
    "        tf.logging.error('Found DQN Learning thread dead. Restarting.')\n",
    "        self.thrd_dqn_training = Thread(target=self.dqn_training)\n",
    "        self.thrd_dqn_training.daemon = True\n",
    "        self.thrd_dqn_training.start()\n",
    "\n",
    "  def run_eval(self):\n",
    "    \n",
    "    self.model.build_graph() \n",
    "    saver = tf.train.Saver(max_to_keep=3) \n",
    "    sess = tf.Session(config=get_config())\n",
    "\n",
    "    if FLAGS.embedding:\n",
    "      sess.run(tf.global_variables_initializer(),feed_dict={self.model.embedding_place:self.word_vector})\n",
    "    eval_dir = os.path.join(FLAGS.log_root, \"eval\") \n",
    "    bestmodel_save_path = os.path.join(eval_dir, 'bestmodel') \n",
    "    self.summary_writer = tf.summary.FileWriter(eval_dir)\n",
    "\n",
    "    if FLAGS.ac_training:\n",
    "      tf.logging.info('DDQN building graph')\n",
    "      t1 = time.time()\n",
    "      dqn_graph = tf.Graph()\n",
    "      with dqn_graph.as_default():\n",
    "        self.dqn.build_graph() \n",
    "        tf.logging.info('building current network took {} seconds'.format(time.time()-t1))\n",
    "        self.dqn_target.build_graph() \n",
    "        tf.logging.info('building target network took {} seconds'.format(time.time()-t1))\n",
    "        dqn_saver = tf.train.Saver(max_to_keep=3)\n",
    "        dqn_sess = tf.Session(config=get_config())\n",
    "      dqn_train_step = 0\n",
    "      replay_buffer = ReplayBuffer(self.dqn_hps)\n",
    "\n",
    "    running_avg_loss = 0 \n",
    "    best_loss = self.restore_best_eval_model() \n",
    "    train_step = 0\n",
    "\n",
    "    while True:\n",
    "      _ = load_ckpt(saver, sess) # load a new checkpoint\n",
    "      if FLAGS.ac_training:\n",
    "        _ = load_dqn_ckpt(dqn_saver, dqn_sess) # load a new checkpoint\n",
    "      processed_batch = 0\n",
    "      avg_losses = []\n",
    "      while processed_batch < 100*FLAGS.batch_size:\n",
    "        processed_batch += FLAGS.batch_size\n",
    "        batch = self.batcher.next_batch() # get the next batch\n",
    "        if FLAGS.ac_training:\n",
    "          t0 = time.time()\n",
    "          transitions = self.model.collect_dqn_transitions(sess, batch, train_step, batch.max_art_oovs) # len(batch_size * k * max_dec_steps)\n",
    "          tf.logging.info('Q values collection time: {}'.format(time.time()-t0))\n",
    "          with dqn_graph.as_default():\n",
    "            \n",
    "            batch_len = len(transitions)\n",
    "            b = ReplayBuffer.create_batch(self.dqn_hps, transitions,len(transitions), use_state_prime = True, max_art_oovs = batch.max_art_oovs)\n",
    "            b_prime = ReplayBuffer.create_batch(self.dqn_hps, transitions,len(transitions), use_state_prime = True, max_art_oovs = batch.max_art_oovs)\n",
    "            dqn_results = self.dqn.run_test_steps(sess=dqn_sess, x= b._x, return_best_action=True)\n",
    "            q_estimates = dqn_results['estimates'] # shape (len(transitions), vocab_size)\n",
    "            dqn_best_action = dqn_results['best_action']\n",
    "\n",
    "            tf.logging.info('running test step on dqn_target')\n",
    "            dqn_target_results = self.dqn_target.run_test_steps(dqn_sess, x= b_prime._x)\n",
    "            q_vals_new_t = dqn_target_results['estimates'] # shape (len(transitions), vocab_size)\n",
    "\n",
    "            q_estimates = np.concatenate([q_estimates,np.zeros((len(transitions),batch.max_art_oovs))],axis=-1)\n",
    "\n",
    "            tf.logging.info('fixing the action q-estimates')\n",
    "            for i, tr in enumerate(transitions):\n",
    "              if tr.done:\n",
    "                q_estimates[i][tr.action] = tr.reward\n",
    "              else:\n",
    "                q_estimates[i][tr.action] = tr.reward + FLAGS.gamma * q_vals_new_t[i][dqn_best_action[i]]\n",
    "            if FLAGS.dqn_scheduled_sampling:\n",
    "              tf.logging.info('scheduled sampling on q-estimates')\n",
    "              q_estimates = self.scheduled_sampling(batch_len, FLAGS.sampling_probability, b._y_extended, q_estimates)\n",
    "            if not FLAGS.calculate_true_q:\n",
    "              for trans, q_val in zip(transitions,q_estimates):\n",
    "                trans.q_values = q_val # each have the size vocab_extended\n",
    "            q_estimates = np.reshape(q_estimates, [FLAGS.batch_size, FLAGS.k, FLAGS.max_dec_steps, -1]) # shape (batch_size, k, max_dec_steps, vocab_size_extended)\n",
    "          tf.logging.info('run eval step on seq2seq model.')\n",
    "          t0=time.time()\n",
    "          results = self.model.run_eval_step(sess, batch, train_step, q_estimates)\n",
    "          t1=time.time()\n",
    "        else:\n",
    "          tf.logging.info('run eval step on seq2seq model.')\n",
    "          t0=time.time()\n",
    "          results = self.model.run_eval_step(sess, batch, train_step)\n",
    "          t1=time.time()\n",
    "\n",
    "        tf.logging.info('experiment: {}'.format(FLAGS.exp_name))\n",
    "        tf.logging.info('processed_batch: {}, seconds for batch: {}'.format(processed_batch, t1-t0))\n",
    "\n",
    "        printer_helper = {}\n",
    "        loss = printer_helper['pgen_loss']= results['pgen_loss']\n",
    "        if FLAGS.coverage:\n",
    "          printer_helper['coverage_loss'] = results['coverage_loss']\n",
    "          if FLAGS.rl_training or FLAGS.ac_training:\n",
    "            printer_helper['rl_cov_total_loss']= results['reinforce_cov_total_loss']\n",
    "          loss = printer_helper['pointer_cov_total_loss'] = results['pointer_cov_total_loss']\n",
    "        if FLAGS.rl_training or FLAGS.ac_training:\n",
    "          printer_helper['shared_loss'] = results['shared_loss']\n",
    "          printer_helper['rl_loss'] = results['rl_loss']\n",
    "          printer_helper['rl_avg_logprobs'] = results['rl_avg_logprobs']\n",
    "        if FLAGS.rl_training:\n",
    "          printer_helper['sampled_r'] = np.mean(results['sampled_sentence_r_values'])\n",
    "          printer_helper['greedy_r'] = np.mean(results['greedy_sentence_r_values'])\n",
    "          printer_helper['r_diff'] = printer_helper['greedy_r'] - printer_helper['sampled_r']\n",
    "        if FLAGS.ac_training:\n",
    "          printer_helper['dqn_loss'] = np.mean(self.avg_dqn_loss) if len(self.avg_dqn_loss) > 0 else 0\n",
    "\n",
    "        for (k,v) in printer_helper.items():\n",
    "          if not np.isfinite(v):\n",
    "            raise Exception(\"{} is not finite. Stopping.\".format(k))\n",
    "          tf.logging.info('{}: {}\\t'.format(k,v))\n",
    "\n",
    "        # add summaries\n",
    "        summaries = results['summaries']\n",
    "        train_step = results['global_step']\n",
    "        self.summary_writer.add_summary(summaries, train_step)\n",
    "\n",
    "        # calculate running avg loss\n",
    "        avg_losses.append(self.calc_running_avg_loss(np.asscalar(loss), running_avg_loss, train_step))\n",
    "        tf.logging.info('-------------------------------------------')\n",
    "\n",
    "      running_avg_loss = np.mean(avg_losses)\n",
    "      tf.logging.info('==========================================')\n",
    "      tf.logging.info('best_loss: {}\\trunning_avg_loss: {}\\t'.format(best_loss, running_avg_loss))\n",
    "      tf.logging.info('==========================================')\n",
    "\n",
    "      if best_loss is None or running_avg_loss < best_loss:\n",
    "        tf.logging.info('Found new best model with %.3f running_avg_loss. Saving to %s', running_avg_loss, bestmodel_save_path)\n",
    "        saver.save(sess, bestmodel_save_path, global_step=train_step, latest_filename='checkpoint_best')\n",
    "        best_loss = running_avg_loss\n",
    "\n",
    "      # flush the summary writer every so often\n",
    "      if train_step % 100 == 0:\n",
    "        self.summary_writer.flush()\n",
    "      #time.sleep(600) # run eval every 10 minute\n",
    "\n",
    "  def main(self):\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    #FLAGS.log_root = os.path.join(FLAGS.log_root, FLAGS.exp_name)\n",
    "    tf.logging.set_verbosity(tf.logging.INFO) # choose what level of logging you want\n",
    "    tf.logging.info('Starting seq2seq_attention in %s mode...', (FLAGS.mode))\n",
    "\n",
    "    #flags = getattr(FLAGS,\"__flags\")\n",
    "\n",
    "    if not os.path.exists(FLAGS.log_root):\n",
    "      if FLAGS.mode==\"train\":\n",
    "        os.makedirs(FLAGS.log_root)\n",
    "      else:\n",
    "        raise Exception(\"Logdir %s doesn't exist. Run in train mode to create it.\" % (FLAGS.log_root))\n",
    "\n",
    "    fw = open('{}/config.txt'.format(FLAGS.log_root), 'w')\n",
    "    #for k, v in flags.items():\n",
    "    #  fw.write('{}\\t{}\\n'.format(k, v))\n",
    "    #fw.close()\n",
    "\n",
    "    self.vocab = Vocab(FLAGS.vocab_path, FLAGS.vocab_size) # create a vocabulary\n",
    "\n",
    "    if FLAGS.mode == 'decode':\n",
    "      FLAGS.batch_size = FLAGS.beam_size\n",
    "\n",
    "    # If single_pass=True, check we're in decode mode\n",
    "    if FLAGS.single_pass and FLAGS.mode!='decode':\n",
    "      raise Exception(\"The single_pass flag should only be True in decode mode\")\n",
    "\n",
    "    \n",
    "    hparam_list = ['mode', 'lr', 'gpu_num',\n",
    "    #'sampled_greedy_flag', \n",
    "    'gamma', 'eta', \n",
    "    'fixed_eta', 'reward_function', 'intradecoder', \n",
    "    'use_temporal_attention', 'ac_training','rl_training', 'matrix_attention', 'calculate_true_q',\n",
    "    'enc_hidden_dim', 'dec_hidden_dim', 'k', \n",
    "    'scheduled_sampling', 'sampling_probability','fixed_sampling_probability',\n",
    "    'alpha', 'hard_argmax', 'greedy_scheduled_sampling',\n",
    "    'adagrad_init_acc', 'rand_unif_init_mag', \n",
    "    'trunc_norm_init_std', 'max_grad_norm', \n",
    "    'emb_dim', 'batch_size', 'max_dec_steps', 'max_enc_steps',\n",
    "    'dqn_scheduled_sampling', 'dqn_sleep_time', 'E2EBackProp',\n",
    "    'coverage', 'cov_loss_wt', 'pointer_gen']\n",
    "    hps_dict = {}\n",
    "    \n",
    "    flag_members = [attr for attr in dir(FLAGS) if not callable(getattr(FLAGS, attr)) and not attr.startswith(\"__\")]\n",
    "    for m in flag_members:\n",
    "        hps_dict[m] = getattr(FLAGS, m)\n",
    "        \n",
    "    if FLAGS.ac_training:\n",
    "      hps_dict.update({'dqn_input_feature_len':(FLAGS.dec_hidden_dim)})\n",
    "    self.hps = namedtuple(\"HParams\", hps_dict.keys())(**hps_dict)\n",
    "    # creating all the required parameters for DDQN model.\n",
    "    if FLAGS.ac_training:\n",
    "      hparam_list = ['lr', 'dqn_gpu_num', \n",
    "      'dqn_layers', \n",
    "      'dqn_replay_buffer_size', \n",
    "      'dqn_batch_size', \n",
    "      'dqn_target_update',\n",
    "      'dueling_net',\n",
    "      'dqn_polyak_averaging',\n",
    "      'dqn_sleep_time',\n",
    "      'dqn_scheduled_sampling',\n",
    "      'max_grad_norm']\n",
    "      hps_dict = {}\n",
    "      \n",
    "      flag_members = [attr for attr in dir(FLAGS) if not callable(getattr(FLAGS, attr)) and not attr.startswith(\"__\")]\n",
    "      for m in flag_members:\n",
    "        hps_dict[m] = getattr(FLAGS, m)\n",
    "    \n",
    "      hps_dict.update({'dqn_input_feature_len':(FLAGS.dec_hidden_dim)})\n",
    "      hps_dict.update({'vocab_size':self.vocab.size()})\n",
    "      self.dqn_hps = namedtuple(\"HParams\", hps_dict.keys())(**hps_dict)\n",
    "\n",
    "    self.batcher = Batcher(FLAGS.data_path, self.vocab, self.hps, single_pass=FLAGS.single_pass, decode_after=FLAGS.decode_after)\n",
    "\n",
    "    tf.set_random_seed(111) # a seed value for randomness\n",
    "\n",
    "    if self.hps.mode == 'train':\n",
    "      print(\"creating model...\")\n",
    "      self.model = SummarizationModel(self.hps, self.vocab)\n",
    "      if FLAGS.ac_training:\n",
    "        # current DQN with paramters \\Psi\n",
    "        self.dqn = DQN(self.dqn_hps,'current')\n",
    "        # target DQN with paramters \\Psi^{\\prime}\n",
    "        self.dqn_target = DQN(self.dqn_hps,'target')\n",
    "      self.setup_training()\n",
    "    elif self.hps.mode == 'eval':\n",
    "      self.model = SummarizationModel(self.hps, self.vocab)\n",
    "      if FLAGS.ac_training:\n",
    "        self.dqn = DQN(self.dqn_hps,'current')\n",
    "        self.dqn_target = DQN(self.dqn_hps,'target')\n",
    "      self.run_eval()\n",
    "    elif self.hps.mode == 'decode':\n",
    "      decode_model_hps = self.hps  # This will be the hyperparameters for the decoder model\n",
    "      decode_model_hps = self.hps._replace(max_dec_steps=1) # The model is configured with max_dec_steps=1 because we only ever run one step of the decoder at a time (to do beam search). Note that the batcher is initialized with max_dec_steps equal to e.g. 100 because the batches need to contain the full summaries\n",
    "      model = SummarizationModel(decode_model_hps, self.vocab)\n",
    "      if FLAGS.ac_training:\n",
    "        # We need our target DDQN network for collecting Q-estimation at each decoder step.\n",
    "        dqn_target = DQN(self.dqn_hps,'target')\n",
    "      else:\n",
    "        dqn_target = None\n",
    "      decoder = BeamSearchDecoder(model, self.batcher, self.vocab, dqn = dqn_target)\n",
    "      decoder.decode() # decode indefinitely (unless single_pass=True, in which case deocde the dataset exactly once)\n",
    "    else:\n",
    "      raise ValueError(\"The 'mode' flag must be one of train/eval/decode\")\n",
    "\n",
    "  def scheduled_sampling(self, batch_size, sampling_probability, true, estimate):\n",
    "    with variable_scope.variable_scope(\"ScheduledEmbedding\"):\n",
    "      # Return -1s where we do not sample, and sample_ids elsewhere\n",
    "      select_sampler = bernoulli.Bernoulli(probs=sampling_probability, dtype=tf.bool)\n",
    "      select_sample = select_sampler.sample(sample_shape=batch_size)\n",
    "      sample_ids = array_ops.where(\n",
    "                  select_sample,\n",
    "                  tf.range(batch_size),\n",
    "                  gen_array_ops.fill([batch_size], -1))\n",
    "      where_sampling = math_ops.cast(\n",
    "          array_ops.where(sample_ids > -1), tf.int32)\n",
    "      where_not_sampling = math_ops.cast(\n",
    "          array_ops.where(sample_ids <= -1), tf.int32)\n",
    "      _estimate = array_ops.gather_nd(estimate, where_sampling)\n",
    "      _true = array_ops.gather_nd(true, where_not_sampling)\n",
    "\n",
    "      base_shape = array_ops.shape(true)\n",
    "      result1 = array_ops.scatter_nd(indices=where_sampling, updates=_estimate, shape=base_shape)\n",
    "      result2 = array_ops.scatter_nd(indices=where_not_sampling, updates=_true, shape=base_shape)\n",
    "      result = result1 + result2\n",
    "      return result1 + result2\n",
    "\n",
    "def main():\n",
    "  seq2seq = Seq2Seq()\n",
    "  seq2seq.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1219
    },
    "colab_type": "code",
    "id": "uz_50XkKdQWw",
    "outputId": "37a58a06-da70-43cb-e5d7-67d10527b069"
   },
   "outputs": [],
   "source": [
    "class flags_:\n",
    "  pass\n",
    "FLAGS = flags_()\n",
    "\n",
    "default_path = \"\"\n",
    "data_path = \"\"\n",
    "\n",
    "# Where to find data\n",
    "FLAGS.data_path = data_path + 'news_finished_files/chunked/train_*'\n",
    "FLAGS.vocab_path = data_path + 'news_finished_files/vocab'\n",
    "\n",
    "FLAGS.mode = 'train'\n",
    "FLAGS.single_pass = False\n",
    "FLAGS.decode_after = 0\n",
    "FLAGS.decode_from = 'train'\n",
    "\n",
    "FLAGS.log_root = default_path +'logs_26_5'\n",
    "FLAGS.exp_name = 'intradecoder-temporalattention-withpretraining4'\n",
    "\n",
    "FLAGS.example_queue_threads = 4\n",
    "FLAGS.batch_queue_threads   = 2\n",
    "FLAGS.bucketing_cache_size  = 100\n",
    "\n",
    "FLAGS.enc_hidden_dim= 256\n",
    "FLAGS.dec_hidden_dim= 256\n",
    "FLAGS.emb_dim= 128\n",
    "FLAGS.batch_size=25\n",
    "FLAGS.max_enc_steps= 400\n",
    "FLAGS.max_dec_steps= 100 \n",
    "FLAGS.beam_size= 4 \n",
    "FLAGS.min_dec_steps= 35 \n",
    "FLAGS.max_iter= 20000 \n",
    "FLAGS.vocab_size= 50000 \n",
    "FLAGS.lr= 0.15\n",
    "FLAGS.adagrad_init_acc= 0.1 \n",
    "FLAGS.rand_unif_init_mag= 0.02\n",
    "FLAGS.trunc_norm_init_std= 1e-4\n",
    "FLAGS.max_grad_norm= 2.0\n",
    "FLAGS.embedding= False\n",
    "FLAGS.gpu_num= 0\n",
    "\n",
    "FLAGS.pointer_gen= True\n",
    "FLAGS.avoid_trigrams= True\n",
    "FLAGS.share_decoder_weights= False\n",
    "FLAGS.rl_training= True\n",
    "FLAGS.self_critic= True\n",
    "FLAGS.use_discounted_rewards= False\n",
    "FLAGS.use_intermediate_rewards= False\n",
    "FLAGS.convert_to_reinforce_model= True\n",
    "FLAGS.intradecoder= True\n",
    "FLAGS.use_temporal_attention=  True\n",
    "FLAGS.matrix_attention= False#, \n",
    "FLAGS.eta= 0\n",
    "FLAGS.fixed_eta= False\n",
    "FLAGS.gamma= 0.99#,\n",
    "FLAGS.reward_function= 'rouge_l/f_score'\n",
    "\n",
    "FLAGS.ac_training= False\n",
    "FLAGS.dqn_scheduled_sampling= True\n",
    "FLAGS.dqn_layers= '512,256,128'\n",
    "FLAGS.dqn_replay_buffer_size= 100000\n",
    "FLAGS.dqn_batch_size= 100 \n",
    "FLAGS.dqn_target_update= 10000\n",
    "FLAGS.dqn_sleep_time= 2\n",
    "FLAGS.dqn_gpu_num= 0\n",
    "FLAGS.dueling_net= True\n",
    "FLAGS.dqn_polyak_averaging= True\n",
    "FLAGS.calculate_true_q= False\n",
    "FLAGS.dqn_pretrain= False\n",
    "FLAGS.dqn_pretrain_steps= 10000\n",
    "FLAGS.scheduled_sampling= False\n",
    "FLAGS.decay_function= 'linear'#,'linear#, exponential#, inv_sigmoid') \n",
    "FLAGS.sampling_probability= 0\n",
    "FLAGS.fixed_sampling_probability= False\n",
    "FLAGS.hard_argmax= True\n",
    "FLAGS.greedy_scheduled_sampling= False\n",
    "FLAGS.E2EBackProp= False\n",
    "FLAGS.alpha= 1\n",
    "FLAGS.k= 1\n",
    "FLAGS.scheduled_sampling_final_dist= True\n",
    "\n",
    "FLAGS.coverage= True\n",
    "FLAGS.cov_loss_wt= 1.0\n",
    "\n",
    "FLAGS.convert_to_coverage_model= True\n",
    "FLAGS.restore_best_model= False\n",
    "\n",
    "FLAGS.debug= False\n",
    "\n",
    "\n",
    "seq2seq = Seq2Seq()\n",
    "seq2seq.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0lmntKo1p1W"
   },
   "outputs": [],
   "source": [
    "zaksum(article,reference ,summary ,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1219
    },
    "colab_type": "code",
    "id": "RBZrQx4kFLXT",
    "outputId": "245360df-370a-4e44-97a3-1ff0ca0a4acd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting seq2seq_attention in decode mode...\n",
      "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: 958\n",
      "INFO:tensorflow:Building graph...\n",
      "WARNING:tensorflow:From C:\\Users\\Videet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Videet\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-7-a48152114455>:129: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-a48152114455>:131: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Videet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Videet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:batch_size 4, attn_size: 512, emb_size: 128\n",
      "INFO:tensorflow:Adding attention_decoder timestep 0 of 1\n",
      "WARNING:tensorflow:From <ipython-input-6-460cd97f2721>:405: Multinomial.__init__ (from tensorflow.python.ops.distributions.multinomial) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\Videet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\multinomial.py:217: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\Videet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\distributions\\multinomial.py:264: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-d68db4259712>:128: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "INFO:tensorflow:Time to build graph: 5 seconds\n",
      "INFO:tensorflow:Loading checkpoint logs_26_5\\train\\model.ckpt-4055\n",
      "WARNING:tensorflow:From C:\\Users\\Videet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from logs_26_5\\train\\model.ckpt-4055\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  pakistan . coach . mickey . arthur . tuesday . said . felt . gutted . pakistan . super . league . spot . fixing . scandal . adding . players . found . guilty . blame . players . become . greedy . hurting . international . cricket . general . recent . case . hurt . pakistan . arthur . added . notably . many . five . pakistani . players . accused . spot . fixing . psl .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: greed . has . hurt . pakistani . cricket . says . coach . mickey . arthur .\n",
      "INFO:tensorflow:GENERATED SUMMARY: pakistan . coach . felt . gutted . pakistan . super . spot . fixing . fixing sc . s . players . mickey . arthur . arthur said arthur . s players . s pakistan . felt said felt . felt added . gutted added . pakistan many . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  rights . groups . victims . families . held . rally . paris . last . week . police . brutality . recent . baton . rape . young . man . heightened . tensions . alleged . __beatings__ . deaths . police . custody . rally . express . anger . face . repeated . police . violence . warn . perverted . notion . public . security . rights . group . said .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rights . groups . in . paris . hold . rally . against . police . brutality .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rights . rights . families . rally . paris . paris in . paris over . paris pak . s . brutality . man . rape . police . victims . s recent . s express . victims repeated . s face . rally repeated . violence . warn . paris [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  uk . announced . ban . electronic . devices . larger . mobiles . including . laptops . tablets . cabin . baggage . flights . six . middle . eastern . countries . ban . affects . travellers . turkey . lebanon . jordan . egypt . tunisia . saudi . arabia . follows . reports . us . imposed . similar . restrictions . flights . originating . several . middle . eastern . north . african . nations .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: uk . bans . laptops . tablets . on . flights . from . 6 . nations .\n",
      "INFO:tensorflow:GENERATED SUMMARY: uk . announced . electronic . ban . mobiles . laptops . laptops over . laptops in . baggage . flights . uk . flights several . s six . s cabin . s devices . s . devices . ban imposed . mobiles imposed . flights restrictions . originating . middle . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  amid . controversy . punjab . minister . navjot . singh . sidhu . tv . commitments . cricketer . turned . politician . said . nobody . business . 6 . pm . sidhu . indicated . appearing . tv . shows . amount . breach . office . profit . law . however . punjab . cm . amarinder . singh . sought . legal . opinion . sidhu . tv . commitments . despite . minister .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: nobody . s . business . what . i . do . after . 6 . pm . sidhu . on . tv . shows .\n",
      "INFO:tensorflow:GENERATED SUMMARY: nobody . controversy . punjab . sidhu . sidhu in . tv . pm . sidhu over . commitments . politician . sidhu said tv . s cricketer . s politician . s singh . punjab said punjab . nobody . 6 . sidhu amarinder . s tv . sidhu sought . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  salman . khan . paid . 44 . 5 . crore . become . bollywood . highest . advance . tax . payer . 2016 . 17 . kapil . sharma . submitted . advance . tax . 23 . 9 . crore . compared . 7 . crore . 2015 . 16 . however . income . tax . department . declined . give . advance . tax . figures . shah . rukh . khan . amitabh . bachchan . aishwarya . rai . investigation . panama . papers . case .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: salman . pays . 44 . cr . as . bollywood . s . highest . advance . tax . payer .\n",
      "INFO:tensorflow:GENERATED SUMMARY: salman . khan . 44 . crore . crore over . highest . highest s . highest in . crore s . s advance . s . salman . paid . s tax . s khan . paid figures . s rukh . crore rukh . amitabh . aishwarya . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:ARTICLE:  madras . high . court . examined . medical . report . __birthmark__ . scar . found . actor . dhanush . body . mentioned . elderly . couple . claiming . parents . however . report . stated . small . superficial . mole . another . large . mole . removed . hearing . case . deferred . march . 27 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: hc . gets . medical . report . of . no . __birthmark__ . on . dhanush . s . body .\n",
      "INFO:tensorflow:GENERATED SUMMARY: madras . high . court . report . report over . scar . dhanush . report in . found . dhanush s . s actor . madras . s birthmark . medical . examined . examined stated . s small . medical mole . mole . large . mole large . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  karnataka . government . urged . supreme . court . declare . late . tamil . nadu . cm . j . jayalalithaa . convict . disproportionate . assets . case . state . argued . apex . court . february . ruling . lessened . charges . jayalalithaa . due . demise . error . __merited__ . __relook__ . court . judgement . convicted . aiadmk . sasikala . natarajan .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: declare . jayalalithaa . as . convict . in . assets . case . karnataka .\n",
      "INFO:tensorflow:GENERATED SUMMARY: karnataka . government . supreme . declare . late . tamil . j . jayalalithaa . jayalalithaa in . j s . jayalalithaa merited . s due . s convict . s court . supreme ruling . s . court . declare due . demise . error . j merited . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  supreme . court . tuesday . pulled . centre . rbi . demonetisation . measures . taken . asking . give . window . citizens . deposit . old . currency . notes . december . 31 . apex . court . gave . central . government . reserve . bank . india . two . weeks . time . reply . notice .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: why . no . deposit . window . for . old . notes . after . dec . 31 . sc . to . govt .\n",
      "INFO:tensorflow:GENERATED SUMMARY: supreme . court . pulled . centre . rbi . demonetisation . rbi over . demonetisation in . window . rbi s . rbi government . s taken . s measures . s tuesday . court tuesday . pulled gave . centre gave . s window . bank . india [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  attempts . settle . ayodhya . dispute . court . failed . nine . times . 150 . years . first . attempt . made . british . 1859 . fence . erected . separate . places . worship . communities . three . attempts . initiated . former . pms . atal . bihari . vajpayee . chandra . shekhar . pv . narasimha . rao .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: what . have . been . the . past . attempts . to . resolve . ayodhya . dispute .\n",
      "INFO:tensorflow:GENERATED SUMMARY: attempts . settle . ayodhya . dispute . times . failed . 150 . first . 150 in . years . first bihari . s . dispute three . s attempt . s court . settle three . settle initiated . s initiated . pms . atal . 150 bihari . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actress . taapsee . pannu . revealed . wanted . slap . people . eve . teased . college . added . people . pinched . wrong . side . wanted . slap . probably . scared . taapsee . said . use . skills . tackle . harassment .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . wanted . to . slap . people . when . i . was . eve . teased . taapsee .\n",
      "INFO:tensorflow:GENERATED SUMMARY: taapsee . actress . wanted . slap . slap i . eve . eve said . college . taapsee . teased . wrong . pannu . s added . taapsee revealed teased . s pannu . wanted revealed actress . s people . slap side . slap probably . scared . taapsee said taapsee . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  brothers . kasam . __juma__ . two . artisans . bhuj . gujarat . created . __bodice__ . one . outfits . worn . actress . emma . watson . film . beauty . beast . pattern . __bodice__ . hand . embroidered . using . technique . called . __aari__ . work . revealed . __sinad__ . sullivan . assistant . costume . designer . film .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: gujarat . artists . created . emma . s . beauty . and . the . beast . costume .\n",
      "INFO:tensorflow:GENERATED SUMMARY: brothers . kasam . juma . artisans . gujarat . gujarat in . bhuj . one . outfits . brothers . outfits revealed . gujarat revealed . s created . s bodice . two . kasam embroidered . s using . bhuj called . aari . work . outfits [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  man . named . altaf . __chaki__ . reportedly . received . six . __misprinted__ . 500 . notes . bank . baroda . atm . jamnagar . gujarat . according . one . notes . completely . blank . one . side . serial . numbers . two . others . printed . clearly . currency . value . written . hindi . printed . clearly . three . notes .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . gets . six . __misprinted__ . 500 . notes . from . atm .\n",
      "INFO:tensorflow:GENERATED SUMMARY: man . man . altaf . six . 500 . notes . notes in . bank . atm . baroda . atm s . s jamnagar . s serial . s misprinted . altaf side . s chaki . named . six others . clearly . currency . value . notes written . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  newly . restored . site . jesus . christ . tomb . church . holy . sepulchre . jerusalem . old . city . unveiled . public . wednesday . comes . underwent . restoration . nine . months . cost . 4 . million . 26 . crore . around . 50 . experts . national . technical . university . athens . involved . restoration .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jesus . tomb . to . be . unveiled . to . public . after . 4mn . restoration .\n",
      "INFO:tensorflow:GENERATED SUMMARY: newly . restored . site . jesus . tomb . church . jerusalem . holy . jerusalem 2 . s . church around . s old . s cost . s city . s newly . site 4 . s million . crore . crore around . experts . church [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  pakistan . coach . mickey . arthur . tuesday . said . felt . gutted . pakistan . super . league . spot . fixing . scandal . adding . players . found . guilty . blame . players . become . greedy . hurting . international . cricket . general . recent . case . hurt . pakistan . arthur . added . notably . many . five . pakistani . players . accused . spot . fixing . psl .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: greed . has . hurt . pakistani . cricket . says . coach . mickey . arthur .\n",
      "INFO:tensorflow:GENERATED SUMMARY: pakistan . coach . felt . gutted . pakistan . super . spot . fixing . fixing sc . s . players . mickey . arthur . arthur said arthur . s players . s pakistan . felt said felt . felt added . gutted added . pakistan many . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  rights . groups . victims . families . held . rally . paris . last . week . police . brutality . recent . baton . rape . young . man . heightened . tensions . alleged . __beatings__ . deaths . police . custody . rally . express . anger . face . repeated . police . violence . warn . perverted . notion . public . security . rights . group . said .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rights . groups . in . paris . hold . rally . against . police . brutality .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rights . rights . families . rally . paris . paris in . paris over . paris pak . s . brutality . man . rape . police . victims . s recent . s express . victims repeated . s face . rally repeated . violence . warn . paris [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  uk . announced . ban . electronic . devices . larger . mobiles . including . laptops . tablets . cabin . baggage . flights . six . middle . eastern . countries . ban . affects . travellers . turkey . lebanon . jordan . egypt . tunisia . saudi . arabia . follows . reports . us . imposed . similar . restrictions . flights . originating . several . middle . eastern . north . african . nations .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: uk . bans . laptops . tablets . on . flights . from . 6 . nations .\n",
      "INFO:tensorflow:GENERATED SUMMARY: uk . announced . electronic . ban . mobiles . laptops . laptops over . laptops in . baggage . flights . uk . flights several . s six . s cabin . s devices . s . devices . ban imposed . mobiles imposed . flights restrictions . originating . middle . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  amid . controversy . punjab . minister . navjot . singh . sidhu . tv . commitments . cricketer . turned . politician . said . nobody . business . 6 . pm . sidhu . indicated . appearing . tv . shows . amount . breach . office . profit . law . however . punjab . cm . amarinder . singh . sought . legal . opinion . sidhu . tv . commitments . despite . minister .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: nobody . s . business . what . i . do . after . 6 . pm . sidhu . on . tv . shows .\n",
      "INFO:tensorflow:GENERATED SUMMARY: nobody . controversy . punjab . sidhu . sidhu in . tv . pm . sidhu over . commitments . politician . sidhu said tv . s cricketer . s politician . s singh . punjab said punjab . nobody . 6 . sidhu amarinder . s tv . sidhu sought . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  salman . khan . paid . 44 . 5 . crore . become . bollywood . highest . advance . tax . payer . 2016 . 17 . kapil . sharma . submitted . advance . tax . 23 . 9 . crore . compared . 7 . crore . 2015 . 16 . however . income . tax . department . declined . give . advance . tax . figures . shah . rukh . khan . amitabh . bachchan . aishwarya . rai . investigation . panama . papers . case .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: salman . pays . 44 . cr . as . bollywood . s . highest . advance . tax . payer .\n",
      "INFO:tensorflow:GENERATED SUMMARY: salman . khan . 44 . crore . crore over . highest . highest s . highest in . crore s . s advance . s . salman . paid . s tax . s khan . paid figures . s rukh . crore rukh . amitabh . aishwarya . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  madras . high . court . examined . medical . report . __birthmark__ . scar . found . actor . dhanush . body . mentioned . elderly . couple . claiming . parents . however . report . stated . small . superficial . mole . another . large . mole . removed . hearing . case . deferred . march . 27 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: hc . gets . medical . report . of . no . __birthmark__ . on . dhanush . s . body .\n",
      "INFO:tensorflow:GENERATED SUMMARY: madras . high . court . report . report over . scar . dhanush . report in . found . dhanush s . s actor . madras . s birthmark . medical . examined . examined stated . s small . medical mole . mole . large . mole large . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  karnataka . government . urged . supreme . court . declare . late . tamil . nadu . cm . j . jayalalithaa . convict . disproportionate . assets . case . state . argued . apex . court . february . ruling . lessened . charges . jayalalithaa . due . demise . error . __merited__ . __relook__ . court . judgement . convicted . aiadmk . sasikala . natarajan .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: declare . jayalalithaa . as . convict . in . assets . case . karnataka .\n",
      "INFO:tensorflow:GENERATED SUMMARY: karnataka . government . supreme . declare . late . tamil . j . jayalalithaa . jayalalithaa in . j s . jayalalithaa merited . s due . s convict . s court . supreme ruling . s . court . declare due . demise . error . j merited . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  united . states . banned . travellers . coming . 10 . airports . mainly . middle . east . north . africa . carrying . devices . larger . mobile . cabin . response . terror . threats . according . reports . restrictions . reportedly . imposed . passengers . travelling . airports . jordan . egypt . turkey . saudi . arabia . united . arab . emirates . kuwait . morocco . qatar .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: us . bans . devices . larger . than . mobiles . on . some . flights . reports .\n",
      "INFO:tensorflow:GENERATED SUMMARY: united . united . travellers . 10 . 10 over . airports . saudi . north . middle . north s . north i . qatar . s east . s carrying . s devices . banned . banned travelling . to . egypt . turkey . saudi arabia . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  supreme . court . tuesday . pulled . centre . rbi . demonetisation . measures . taken . asking . give . window . citizens . deposit . old . currency . notes . december . 31 . apex . court . gave . central . government . reserve . bank . india . two . weeks . time . reply . notice .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: why . no . deposit . window . for . old . notes . after . dec . 31 . sc . to . govt .\n",
      "INFO:tensorflow:GENERATED SUMMARY: supreme . court . pulled . centre . rbi . demonetisation . rbi over . demonetisation in . window . rbi s . rbi government . s taken . s measures . s tuesday . court tuesday . pulled gave . centre gave . s window . bank . india [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  attempts . settle . ayodhya . dispute . court . failed . nine . times . 150 . years . first . attempt . made . british . 1859 . fence . erected . separate . places . worship . communities . three . attempts . initiated . former . pms . atal . bihari . vajpayee . chandra . shekhar . pv . narasimha . rao .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: what . have . been . the . past . attempts . to . resolve . ayodhya . dispute .\n",
      "INFO:tensorflow:GENERATED SUMMARY: attempts . settle . ayodhya . dispute . times . failed . 150 . first . 150 in . years . first bihari . s . dispute three . s attempt . s court . settle three . settle initiated . s initiated . pms . atal . 150 bihari . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actress . taapsee . pannu . revealed . wanted . slap . people . eve . teased . college . added . people . pinched . wrong . side . wanted . slap . probably . scared . taapsee . said . use . skills . tackle . harassment .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . wanted . to . slap . people . when . i . was . eve . teased . taapsee .\n",
      "INFO:tensorflow:GENERATED SUMMARY: taapsee . actress . wanted . slap . slap i . eve . eve said . college . taapsee . teased . wrong . pannu . s added . taapsee revealed teased . s pannu . wanted revealed actress . s people . slap side . slap probably . scared . taapsee said taapsee . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  brothers . kasam . __juma__ . two . artisans . bhuj . gujarat . created . __bodice__ . one . outfits . worn . actress . emma . watson . film . beauty . beast . pattern . __bodice__ . hand . embroidered . using . technique . called . __aari__ . work . revealed . __sinad__ . sullivan . assistant . costume . designer . film .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: gujarat . artists . created . emma . s . beauty . and . the . beast . costume .\n",
      "INFO:tensorflow:GENERATED SUMMARY: brothers . kasam . juma . artisans . gujarat . gujarat in . bhuj . one . outfits . brothers . outfits revealed . gujarat revealed . s created . s bodice . two . kasam embroidered . s using . bhuj called . aari . work . outfits [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  man . named . altaf . __chaki__ . reportedly . received . six . __misprinted__ . 500 . notes . bank . baroda . atm . jamnagar . gujarat . according . one . notes . completely . blank . one . side . serial . numbers . two . others . printed . clearly . currency . value . written . hindi . printed . clearly . three . notes .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . gets . six . __misprinted__ . 500 . notes . from . atm .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:GENERATED SUMMARY: man . man . altaf . six . 500 . notes . notes in . bank . atm . baroda . atm s . s jamnagar . s serial . s misprinted . altaf side . s chaki . named . six others . clearly . currency . value . notes written . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  newly . restored . site . jesus . christ . tomb . church . holy . sepulchre . jerusalem . old . city . unveiled . public . wednesday . comes . underwent . restoration . nine . months . cost . 4 . million . 26 . crore . around . 50 . experts . national . technical . university . athens . involved . restoration .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jesus . tomb . to . be . unveiled . to . public . after . 4mn . restoration .\n",
      "INFO:tensorflow:GENERATED SUMMARY: newly . restored . site . jesus . tomb . church . jerusalem . holy . jerusalem 2 . s . church around . s old . s cost . s city . s newly . site 4 . s million . crore . crore around . experts . church [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  asserting . ram . mandir . issue . used . medium . gain . power . 25 . years . congress . tuesday . said . mutual . court . settlement . communities . best . way . maintain . social . harmony . bjp . also . hailed . supreme . court . suggestion . amicable . settlement . best . solution . babri . masjid . ram . mandir . issue .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: settlement . on . ram . temple . best . way . to . maintain . harmony . cong .\n",
      "INFO:tensorflow:GENERATED SUMMARY: ram . mutual . mandir . settlement . settlement over . gain . power . best . 25 . years . congress . ram . mandir said congress . s also . mandir social . s used . ram said mandir . court . court suggestion . settlement settlement . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  cbi . registered . fir . unknown . persons . allegedly . using . national . emblem . name . prime . minister . office . dupe . people . pretext . giving . houses . poor . families . case . filed . basis . reference . prime . minister . office . attached . complaint . village . head . jharkhand .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: fir . filed . over . fraud . using . pm . s . office . name . to . dupe . people .\n",
      "INFO:tensorflow:GENERATED SUMMARY: cbi . registered . fir . using . emblem . cbi . name . name in . name 2 . complaint . name filed . s case . s prime . fir pretext . s persons . fir families . s families . using filed . using prime . office . office minister . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  __juri__ . village . jharkhand . east . singhbhum . district . initiated . practice . naming . lanes . educated . girls . sumita . bhattacharya . pursuing . master . degree . history . picked . first . girl . lane . named . despite . high . school . village . literacy . rate . higher . state . average .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jharkhand . village . names . streets . after . most . educated . girls .\n",
      "INFO:tensorflow:GENERATED SUMMARY: jharkhand . juri . jharkhand . singhbhum . practice . practice in . lanes . educated . girls . girls s . educated high . s lane . s history . s district . east . village . jharkhand lane . to . singhbhum named . despite . school . girls [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  united . states . banned . travellers . coming . 10 . airports . mainly . middle . east . north . africa . carrying . devices . larger . mobile . cabin . response . terror . threats . according . reports . restrictions . reportedly . imposed . passengers . travelling . airports . jordan . egypt . turkey . saudi . arabia . united . arab . emirates . kuwait . morocco . qatar .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: us . bans . devices . larger . than . mobiles . on . some . flights . reports .\n",
      "INFO:tensorflow:GENERATED SUMMARY: united . united . travellers . 10 . 10 over . airports . saudi . north . middle . north s . north i . qatar . s east . s carrying . s devices . banned . banned travelling . to . egypt . turkey . saudi arabia . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  number . maoists . killed . security . forces . 2016 . witnessed . jump . 150 . compared . 2015 . centre . informed . lok . sabha . tuesday . number . left . wing . extremists . killed . 2016 . compared . 2015 . increased . 89 . 2015 . 222 . 2016 . number . highest . last . six . years . government . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: killing . of . maoists . witnessed . 150 . increase . in . 2016 . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: maoists . number . killed . forces . 150 . jump . centre . 150 s . 150 in . s . maoists . security . security compared . s compared . maoists compared . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "INFO:tensorflow:We've been decoding with same checkpoint for 60 seconds. Time to load new checkpoint\n",
      "INFO:tensorflow:Loading checkpoint logs_26_5\\train\\model.ckpt-4055\n",
      "INFO:tensorflow:Restoring parameters from logs_26_5\\train\\model.ckpt-4055\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  madhya . pradesh . man . arrested . lodging . false . complaint . via . twitter . indian . railways . personnel . board . bhind . indore . __intercity__ . express . monday . tweet . railway . minister . suresh . prabhu . railway . protection . force . sub . inspector . boarded . train . look . issue . found . intoxicated . tweeted . fake . complaint .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . arrested . for . fake . tweet . complaint . to . railway . minister .\n",
      "INFO:tensorflow:GENERATED SUMMARY: madhya . pradesh . arrested . man . false . complaint . twitter . railways . railways s . twitter tweeted . railways tweeted . s via . s indian . s railway . man protection . s protection . lodging . arrested inspector . train . look . twitter [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  describing . steps . taken . assam . government . curb . rhino . poaching . kaziranga . national . park . environment . minister . anil . madhav . dave . said . forest . staff . empowered . use . firearms . without . prior . sanction . forest . staff . also . provided . immunity . prosecution . minister . added . number . anti . poaching . camps . also . increased . ensure . effective . surveillance .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: kaziranga . guards . can . use . firearms . without . sanction . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: forest . s . steps . assam . rhino . curb . kaziranga . rhino over . park . park sc . s national . park said environment . s minister . s government . assam said assam . to . forest . staff . rhino added . anti . poaching . kaziranga also . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  india . added . pacer . mohammed . shami . squad . final . test . scheduled . begin . march . 25 . dharamshala . shami . last . played . india . november . 2016 . since . due . knee . injury . earlier . indian . captain . kohli . hinted . including . shami . saying . sent . play . vijay . hazare . trophy . match . practice .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: india . include . mohammed . shami . in . the . squad . for . final . test .\n",
      "INFO:tensorflow:GENERATED SUMMARY: india . added . mohammed . shami . test . shami over . test in . test 2 . test s . s 25 . india . s scheduled . shami scheduled . s shami . shami captain . s kohli . squad . final . shami saying . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . diljit . dosanjh . said . never . faced . camp . system . bollywood . added . never . faced . anybody . tried . bad . whoever . met . greeted . respect . love . diljit . known . acting . singing . punjabi . films . made . bollywood . debut . 2016 . film . udta . punjab .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: never . faced . any . camp . system . in . bollywood . diljit . dosanjh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: never . actor . faced . never . camp . system . system over . never s . bollywood . diljit . dosanjh . diljit said diljit . s respect . s love . never said never . s actor . never known . faced known . s never . never made . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  former . manchester . city . liverpool . striker . mario . balotelli . missed . opening . two . minutes . team . nice . ligue . 1 . match . nantes . struggled . loosen . shoelaces . balotelli . walking . start . match . went . towards . bench . assistance . helped . coach . loosened . laces .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: player . misses . two . mins . of . match . as . he . fails . to . undo . laces .\n",
      "INFO:tensorflow:GENERATED SUMMARY: manchester . former . city . liverpool . mario . balotelli . mario in . mario s . missed . balotelli s . s opening . s balotelli . manchester . s striker . liverpool loosen . s city . manchester start . s match . mario went . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  51 . year . old . alleged . member . khalistan . commando . force . __kcf__ . __gursewak__ . singh . __babla__ . arrested . country . made . pistol . delhi . police . tuesday . senior . crime . branch . officials . said . alleged . militant . involved . 75 . cases . include . terror . activities . murder . robbery . loot . majority . cases . registered . punjab . delhi . officials . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: khalistan . commando . force . member . wanted . in . 75 . cases . arrested .\n",
      "INFO:tensorflow:GENERATED SUMMARY: 51 . year . member . khalistan . commando . force . commando over . s . delhi . arrested . delhi s . s gursewak . s include . s country . old . old said year . terror . terror activities . robbery . loot . majority . cases . punjab . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  police . station . bhandara . district . maharashtra . sealed . local . municipal . council . non . payment . taxes . civic . authorities . sealed . police . station . failed . pay . municipal . tax . 1 . 19 . lakh . police . official . said . official . added . seal . broken . two . hours . later . sought . district . collector . intervention .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: maharashtra . police . station . sealed . for . non . payment . of . tax .\n",
      "INFO:tensorflow:GENERATED SUMMARY: police . station . bhandara . maharashtra . sealed . non . sealed over . non in . payment . non said . sealed said police . s authorities . s municipal . bhandara police . to . bhandara said official . seal . broken . broken added . hours . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  government . tuesday . admitted . instances . custodial . deaths . maharashtra . little . compared . states . 35 . instances . state . 2013 . 21 . 2014 . 19 . 2015 . union . minister . kiren . rijiju . said . bjp . satyapal . singh . former . mumbai . police . commissioner . however . called . state . police . best . india .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: more . custodial . deaths . in . maha . than . other . states . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: instances . tuesday . instances . custodial . maharashtra . deaths . maharashtra i . little . states . 35 . 35 however . s compared . s states . s instances . instances said instances . s . bjp . custodial said singh . mumbai . police . maharashtra however . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  two . sufi . clerics . hazrat . nizamuddin . dargah . delhi . gone . missing . pakistan . last . week . returned . india . external . affairs . minister . sushma . swaraj . intervened . matter . duo . went . pay . obeisance . shrine . returning . blamed . pakistani . newspaper . publishing . misleading . article . led . entire . fiasco .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: nizamuddin . clerics . who . went . missing . in . pak . return . to . india .\n",
      "INFO:tensorflow:GENERATED SUMMARY: two . sufi . clerics . nizamuddin . delhi . delhi in . missing . missing 2 . s . pakistan . india . pakistan blamed . s gone . s last . s went . hazrat . clerics went . s shrine . dargah . pakistani . newspaper . missing [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  tibetan . religious . leader . dalai . lama . tuesday . said . global . terrorist . crisis . fast . reaching . nuclear . threshold . crosses . threshold . could . mean . entire . mankind . could . perish . added . dalai . lama . also . said . terrorism . crisis . can . not . resolved . unless . nations . come . together . single . platform .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: global . terrorism . fast . reaching . nuclear . threshold . dalai . lama .\n",
      "INFO:tensorflow:GENERATED SUMMARY: terrorist . s . global . terrorist . crisis . crisis i . i . fast . nuclear . threshold . lama . dalai . lama said lama . s threshold . s could . s leader . global said terrorist . terrorist said crisis . can . crisis said threshold [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  government . tuesday . proposed . make . aadhaar . card . mandatory . filing . income . tax . returns . itr . submit . returns . july . 1 . 2017 . aadhaar . number . would . also . needed . applying . permanent . account . number . pan . according . proposed . bill . pan . cards . linked . aadhaar . deemed . invalid .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: govt . proposes . to . make . aadhaar . mandatory . for . i . t . returns .\n",
      "INFO:tensorflow:GENERATED SUMMARY: aadhaar . government . make . aadhaar . aadhaar over . card . card in . filing . aadhaar s . returns . aadhaar according . s tax . s income . s returns . s . proposed . aadhaar number . card according . card proposed . bill . pan . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  bengaluru . based . graphic . designer . accused . taxi . hailing . startup . ola . using . design . made . hoardings . without . permission . design . shows . line . art . vidhana . soudha . building . bengaluru . 2015 . ola . also . accused . using . picture . app . launch . screen . without . permission . photographer .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . accuses . ola . of . copying . his . design . without . permission .\n",
      "INFO:tensorflow:GENERATED SUMMARY: bengaluru . bengaluru . designer . taxi . ola . ola in . design . startup . using . hoardings . without . ola also . s bengaluru . s using . s hailing . graphic . graphic 2015 . s 2015 . startup also . accused . ola picture . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  commentator . dean . jones . trolled . user . called . jones . cry . baby . one . tweets . jones . tweeted . many . cricketers . refuse . bat . poor . pitches . training . watching . test . day . 5 . reason . user . reply . jones . reminded . australia . currently . holds . border . gavaskar . cup . cricket . world . cup .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: dean . jones . trolls . user . after . he . calls . him . cry . baby .\n",
      "INFO:tensorflow:GENERATED SUMMARY: commentator . commentator . jones . trolled . cry . baby . baby in . tweets . baby s . jones s . tweets tweeted . s jones . jones tweeted jones . s called . dean . user . refuse . bat . australia . jones currently . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  supreme . court . tuesday . directed . karnataka . government . supply . 2 . 000 . cusecs . cauvery . water . tamil . nadu . every . day . matter . next . heard . july . 11 . court . earlier . dismissed . petition . seeking . compensation . states . loss . property . cauvery . water . related . dispute . karnataka . tamil . nadu .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: sc . directs . k . taka . to . release . 2 . 000 . cusecs . water . to . tn .\n",
      "INFO:tensorflow:GENERATED SUMMARY: supreme . supreme . karnataka . supply . 2 . 000 . cauvery . cusecs . water . tamil . dispute . tamil related . s compensation . s tamil . s nadu . s tuesday . court . karnataka seeking . s states . loss . property . cauvery [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  japanese . conglomerate . softbank . backed . deal . invest . 100 . million . android . co . founder . andy . rubin . smartphone . startup . named . essential . startup . reportedly . working . smartphone . compete . apple . iphone . 7 . fall . deal . comes . apple . committed . invest . 1 . billion . softbank . 100 . billion . fund . january . year .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: softbank . cancels . 100mn . funding . in . android . founder . s . startup .\n",
      "INFO:tensorflow:GENERATED SUMMARY: softbank . japanese . deal . softbank . invest . softbank in . 100 . android . android in . android s . s million . s founder . s startup . s deal . s . backed . softbank comes . apple . invest committed . billion [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  bharti . airtel . tuesday . hit . back . reliance . jio . allegations . around . former . fastest . network . ad . campaign . saying . complaint . deliberate . attempt . __malign__ . __brand__ . __misguide__ . __customers__ . mobile . internet . speed . testing . firm . ookla . stood . finding . airtel . fastest . broadband . network . claim . challenged . jio .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jio . s . complaint . over . ad . meant . to . misguide . customers . airtel .\n",
      "INFO:tensorflow:GENERATED SUMMARY: airtel . bharti . hit . back . reliance . fastest . jio . jio s . fastest in . ad . airtel . ad firm . s allegations . airtel saying campaign . s airtel . hit speed . s testing . back firm . to . reliance finding . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  sanjay . dutt . suffered . hairline . rib . fracture . shooting . fight . sequence . group . 19 . 20 . men . film . bhoomi . dutt . got . injured . jump . went . wrong . scene . film . climax . shot . chambal . dutt . advised . rest . continued . shooting . taking . rest . breaks .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: dutt . suffers . hairline . rib . fracture . at . shoot . of . fight . scene .\n",
      "INFO:tensorflow:GENERATED SUMMARY: sanjay . dutt . hairline . rib . fracture . shooting . sequence . fight . 20 . 19 . men . 20 advised . s film . s group . s dutt . s suffered . suffered . sanjay . hairline film . shot . dutt advised . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  google . ordered . us . judge . provide . information . anyone . searched . particular . fraud . victim . name . fraud . case . perpetrator . tried . initiating . transfer . 18 . lakh . victim . account . using . fake . id . google . image . search . victims . name . brings . image . used . id .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: google . ordered . to . name . people . who . searched . for . fraud . victim .\n",
      "INFO:tensorflow:GENERATED SUMMARY: google . ordered . us . judge . anyone . fraud . google . searched . victim . name . fraud in . s name . s account . s particular . s provide . us account . to . judge using . id . google image . search . victims . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  emma . watson . dan . stevens . starrer . beauty . beast . earned . 2 . 300 . crore . 357 . million . opening . weekend . film . opening . weekend . collections . nearly . three . times . higher . __cinderella__ . __moanna__ . __zootopia__ . revealed . bollywood . trade . analyst . taran . adarsh . india . fantasy . musical . earned . 6 . crore . opening . weekend .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: beauty . and . the . beast . earns . 2 . 300 . crore . in . opening . weekend .\n",
      "INFO:tensorflow:GENERATED SUMMARY: emma . watson . stevens . beauty . 2 . 2 over . india . 300 . crore . 357 . 6 . 357 revealed . weekend . s 357 . s opening . dan . watson revealed . trade . trade revealed . taran . adarsh . india adarsh . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  indian . captain . virat . kohli . accused . australian . players . trolling . indian . team . physio . patrick . farhart . australian . daily . called . kohli . donald . trump . world . sport . like . president . trump . kohli . decided . blame . media . means . trying . hide . egg . smeared . right . across . face . article . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: kohli . has . become . the . donald . trump . of . world . sport . aus . media .\n",
      "INFO:tensorflow:GENERATED SUMMARY: indian . indian . virat . kohli . players . trolling . players over . physio . team . trump . daily . kohli pak . s decided . s australian . s kohli . s accused . virat decided . kohli media . s trying . hide . egg [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . ranbir . kapoor . seen . playing . sanjay . dutt . biopic . said . never . put . much . weight . added . plan . reduce . weight . next . phase . shooting . film . directed . rajkumar . hirani . ranbir . reportedly . portray . six . different . phases . dutt . life . starting . 22 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . never . put . on . so . much . weight . before . ranbir . kapoor .\n",
      "INFO:tensorflow:GENERATED SUMMARY: ranbir . actor . ranbir . never . weight . much . dutt . biopic . weight in . biopic s . biopic said biopic . ranbir said ranbir . s shooting . ranbir film . s playing . ranbir directed . s ranbir . much reportedly . weight reportedly . phases . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  __herring__ . fish . communicate . underwater . sounds . created . farting . according . study . high . pitch . buzzing . sound . helps . fish . communicate . night . without . alerting . predators . helping . fish . form . protective . __shoals__ . according . study . noise . always . accompanied . stream . bubbles .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: __herring__ . fish . communicate . through . farts .\n",
      "INFO:tensorflow:GENERATED SUMMARY: fish . herring . communicate . underwater . study . farting . pitch . pitch in . pitch s . pitch according . fish . s helping . s according . s created . fish alerting . s fish . fish helping . fish form . shoals . study according . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  rajya . sabha . tuesday . passed . hiv . aids . prevention . control . bill . 2014 . voice . vote . bill . seeks . ensure . equal . rights . hiv . aids . affected . people . getting . treatment . admission . educational . institutions . getting . jobs . anyone . india . hiv . aids . government . take . care . treatment . health . minister . jp . nadda . said .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rs . passes . bill . to . ensure . equal . rights . of . aids . patients .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rajya . sabha . passed . hiv . aids . bill . bill over . bill s . bill said bill . s 2014 . s anyone . s ensure . hiv getting . s aids . s . rajya . hiv government . aids government . care . bill government . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  taxi . hailing . startup . uber . co . founder . travis . kalanick . might . step . ceo . according . reports . kalanick . reportedly . quit . position . uber . world . valuable . startup . hires . first . coo . comes . resignation . number . top . uber . executives . including . president . jeff . jones . uber . dealing . allegations . promoting . culture . sexism .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: uber . co . founder . travis . might . step . down . as . ceo . report .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:GENERATED SUMMARY: uber . hailing . startup . uber . kalanick . travis . uber in . kalanick over . kalanick s . kalanick including . uber including . s ceo . uber according . s uber . uber number . s co . uber president . s kalanick . uber [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  managing . director . idea . himanshu . kapania . said . integrating . operations . vodafone . idea . cost . combined . entity . around . 13 . 400 . crore . however . said . synergies . leading . cost . saving . result . net . benefit . 65 . 000 . crore . merged . entity . idea . vodafone . confirmed . merger . monday . form . india . largest . telecom . firm .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: vodafone . idea . merger . to . cost . 13 . 400 . crore . says . idea . md .\n",
      "INFO:tensorflow:GENERATED SUMMARY: integrating . leading . integrating . operations . vodafone . idea . cost . combined . idea i . 2 . crore . 400 . crore said entity . s around . idea said idea . integrating said integrating . my . integrating entity . operations confirmed . vodafone monday . idea monday . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  punjab . cm . captain . amarinder . singh . seek . legal . opinion . whether . cabinet . minister . navjot . singh . sidhu . continue . appear . popular . tv . show . hosted . comedian . kapil . sharma . singh . said . know . constitution . law . says . matter . sidhu . given . tourism . local . bodies . portfolios . congress . government .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: punjab . cm . to . seek . legal . opinion . over . sidhu . s . tv . commitments .\n",
      "INFO:tensorflow:GENERATED SUMMARY: punjab . cm . amarinder . seek . legal . cabinet . sidhu . sidhu in . cabinet 2 . bodies . portfolios . s . punjab . opinion . singh . singh singh . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  according . reports . upcoming . superhero . film . power . rangers . first . feature . gay . superhero . character . __trini__ . yellow . ranger . played . actress . becky . g . revealed . one . scene . girlfriend . problems . revealed . early . reviews . film . directed . dean . __israelite__ . film . release . march . 24 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: power . rangers . to . feature . first . gay . superhero . reports .\n",
      "INFO:tensorflow:GENERATED SUMMARY: reports . according . superhero . film . power . rangers . first . gay . feature . gay sc . s . first revealed . s superhero . s trini . s according . reports . reports revealed . superhero revealed . early . reviews . film revealed . gay directed . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  uttar . pradesh . chief . minister . yogi . adityanath . said . lok . sabha . tuesday . state . witness . corruption . communal . riots . government . uttar . pradesh . prime . minister . narendra . modi . dream . state . cm . added . yogi . yet . give . lok . sabha . membership . promised . free . state . __goondagardi__ . hooliganism .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: up . will . witness . no . communal . riots . on . my . watch . yogi .\n",
      "INFO:tensorflow:GENERATED SUMMARY: yogi . uttar . lok . sabha . witness . corruption . witness in . communal . riots . yogi . yogi said yogi . s adityanath . adityanath . minister . yogi government . s minister . chief . state . sabha added . witness yet . corruption lok . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . nawazuddin . siddiqui . said . contract . killers . also . romantic . romance . help . guns . romance . done . many . ways . every . person . way . express . __romanticism__ . added . nawazuddin . actor . already . played . gangster . gangs . wasseypur . seen . playing . contract . killer . babumoshai . __bandukbaaz__ .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: contract . killers . can . romance . with . help . of . guns . nawazuddin .\n",
      "INFO:tensorflow:GENERATED SUMMARY: contract . actor . contract . killers . romantic . romance . help . guns . romance i . romance report . killer . nawazuddin . s actor . s many . s also . contract said contract . s . contract actor . killers played . romantic gangs . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  kitchen . gadget . company . __kitchenaid__ . accused . sexism . posting . ad . pink . appliances . aimed . women . ad . featured . words . __kitchenaid__ . women . alongside . pink . products . part . limited . edition . range . raise . money . breast . cancer . charity . __kitchenaid__ . however . stated . colour . used . symbol . hope .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: firm . accused . of . sexism . for . its . pink . kitchen . gadgets . ad .\n",
      "INFO:tensorflow:GENERATED SUMMARY: kitchen . gadget . kitchenaid . sexism . ad . ad in . pink . appliances . ad 2 . s . women . ad however . s aimed . s pink . s kitchenaid . gadget edition . s raise . money . breast . cancer . charity . ad stated . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  singer . sukhwinder . singh . said . get . married . soon . jump . well . die . 45 . year . old . singer . revealed . music . composer . ar . rahman . pestering . get . married . want . __fantabulous__ . relationship . want . feel . special . added . sukhwinder .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: if . i . dont . get . married . in . 2017 . ill . kill . myself . sukhwinder .\n",
      "INFO:tensorflow:GENERATED SUMMARY: married . singer . get . soon . married . jump . die . well . 45 . die sc . s . sukhwinder . singh . sukhwinder said year . s singer . married revealed get . s rahman . get pestering . married get . married fantabulous . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  american . express . biggest . credit . card . issuer . customer . spending . paid . ceo . ken . chenault . 22 . million . nearly . 144 . crore . work . 2016 . 19 . increase . compensation . puts . par . goldman . sachs . ceo . lloyd . blankfein . notably . __amex__ . counts . warren . buffetts . berkshire . hathaway . biggest . shareholder . climbed . 6 . 5 . 2016 . tumbling . 25 . previous . year .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: american . express . raises . ceo . s . pay . to . 144 . crore . in . a . year .\n",
      "INFO:tensorflow:GENERATED SUMMARY: express . american . biggest . credit . card . issuer . customer . ceo . paid . paid s . paid climbed . s . crore . s spending . s 22 . s american . express . biggest berkshire . s biggest . shareholder . 6 . customer [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "INFO:tensorflow:We've been decoding with same checkpoint for 60 seconds. Time to load new checkpoint\n",
      "INFO:tensorflow:Loading checkpoint logs_26_5\\train\\model.ckpt-4055\n",
      "INFO:tensorflow:Restoring parameters from logs_26_5\\train\\model.ckpt-4055\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  steve . smith . reached . career . high . 941 . rating . points . latest . icc . test . rankings . batsmen . fifth . highest . rating . points . recorded . test . cricket . 130 . year . history . smith . rating . points . fifth . best . ever . bradman . 961 . __len__ . hutton . 945 . jack . hobbs . ricky . ponting . __942__ . notably . highest . rating . player . receive . 1000 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: smith . attains . fifth . highest . batting . rating . in . test . cricket .\n",
      "INFO:tensorflow:GENERATED SUMMARY: smith . steve . smith . career . 941 . icc . test . points . icc s . rankings . highest . highest notably . s latest . s fifth . s points . s reached . high . smith 945 . jack . ricky . ponting . test notably . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  australian . pacer . mitchell . starc . said . looking . forward . bowl . ravichandran . ashwin . australia . might . take . ashwin . advice . hit . badge . starc . given . send . ashwin . second . test . ashwin . tapping . forehead . reply . starc . taunting . abhinav . mukund . similar . manner . earlier .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: i . might . take . ashwin . s . advice . and . hit . him . on . head . says . starc .\n",
      "INFO:tensorflow:GENERATED SUMMARY: australian . pacer . looking . bowl . bowl i . ashwin . ashwin in . australia . advice . starc . ashwin said ashwin . s second . s take . s pacer . mitchell . looking said bowl . looking tapping . bowl forehead . ashwin reply . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  two . member . probe . committee . pakistan . cricket . board . pcb . travel . united . kingdom . look . pakistani . opener . nasir . jamshed . alleged . involvement . pakistan . super . league . psl . spot . fixing . scandal . according . reports . jamshed . earlier . investigated . national . crime . agency . uk . passport . withheld . authorities . granted . bail .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: pcb . panel . to . probe . nasir . jamshed . in . uk . over . psl . spot . fixing .\n",
      "INFO:tensorflow:GENERATED SUMMARY: two . member . probe . pakistan . pcb . travel . board . united . look . pcb s . united earlier . s kingdom . s according . s opener . s committee . probe according . pakistan earlier . to . cricket . crime . pcb agency . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  us . drone . airstrike . afghanistan . killed . pakistani . militant . accused . involvement . deadly . attack . bus . carrying . sri . lanka . cricket . team . 2009 . pakistani . security . sources . claimed . us . unmanned . aircraft . struck . car . carrying . qari . mohammad . yasin . also . known . ustad . aslam . sunday . southwestern . afghan . province . __paktika__ .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: militant . behind . attack . on . lankan . cricket . team . in . pak . killed .\n",
      "INFO:tensorflow:GENERATED SUMMARY: us . us . airstrike . afghanistan . drone . bus . attack . attack in . attack s . deadly . drone sunday . s carrying . s militant . s pakistani . drone carrying . to . us mohammad . s yasin . us known . ustad . bus sunday . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  women . mlas . ruling . tdp . opposition . ysr . congress . involved . scuffle . tuesday . andhra . pradesh . assembly . premises . incident . started . tdp . mla . tried . stop . ysr . congress . member . latter . addressing . media . increasing . atrocities . women . andhra . security . intervene . control . situation .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: women . mlas . get . into . scuffle . in . andhra . pradesh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: women . women . mlas . tdp . opposition . ysr . andhra . scuffle . ysr in . andhra s . scuffle addressing . s congress . s involved . s ysr . tdp tuesday . s tdp . tdp addressing . to . ysr addressing . women [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  greek . police . reportedly . found . neutralised . eight . suspected . parcel . bombs . postal . sorting . centre . athens . monday . parcels . addressed . officials . economic . institutions . companies . various . european . countries . last . week . parcel . bomb . explosion . imf . office . paris . injured . one . person . another . targeting . german . finance . ministry . intercepted .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: greek . police . intercept . parcel . bombs . headed . to . eu . institutes .\n",
      "INFO:tensorflow:GENERATED SUMMARY: greek . police . found . neutralised . suspected . bombs . parcel . postal . sorting . centre . sorting s . s . athens . centre monday . s eight . s reportedly . found reportedly . s imf . paris . injured . one . bombs person . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  centre . tuesday . proposed . lower . limit . cash . transactions . 2 . lakh . earlier . proposal . 3 . lakh . union . budget . penalty . violating . fine . equivalent . amount . transaction . revenue . secretary . hasmukh . adhia . tweeted . move . aims . discourage . cash . payments . well . curb . black . money .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: govt . proposes . ban . on . cash . transactions . above . 2 . lakh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: centre . centre . lower . limit . cash . 2 . transactions . lakh . 3 . lakh sc . s . lakh tweeted . money . centre earlier . s union . s tuesday . proposed . lower tweeted . limit tweeted tweeted . cash tweeted . lakh [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . shah . rukh . khan . revealed . hired . lady . bodyguards . protect . female . fans . added . getting . rude . men . pushing . women . trying . protect . shah . rukh . said . jokingly . love . female . fans . hurts . long . nails . difficult . explaining . nail . marks . wife . kids .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . lady . bodyguards . to . protect . me . from . female . fans . srk .\n",
      "INFO:tensorflow:GENERATED SUMMARY: actor . actor . hired . lady . bodyguards . bodyguards i . protect . female . fans . fans pak . s . rude . shah . khan . shah added . s rukh . hired said hired . hired revealed . female said fans . hurts . bodyguards long . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  railway . track . designed . run . 19 . storey . residential . building . city . chongqing . china . __liziba__ . station . located . building . sixth . eighth . floor . built . combat . lack . space . city . station . noise . insulation . system . trains . reportedly . run . rubber . tyres . air . suspension . units .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: china . has . a . train . track . passing . through . an . apartment .\n",
      "INFO:tensorflow:GENERATED SUMMARY: railway . railway . run . 19 . storey . building . china . china in . building s . chongqing . station . building reportedly . s city . s residential . s designed . designed . track . track station . insulation . system . trains . china [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  asserting . ram . mandir . issue . used . medium . gain . power . 25 . years . congress . tuesday . said . mutual . court . settlement . communities . best . way . maintain . social . harmony . bjp . also . hailed . supreme . court . suggestion . amicable . settlement . best . solution . babri . masjid . ram . mandir . issue .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: settlement . on . ram . temple . best . way . to . maintain . harmony . cong .\n",
      "INFO:tensorflow:GENERATED SUMMARY: ram . mutual . mandir . settlement . settlement over . gain . power . best . 25 . years . congress . ram . mandir said congress . s also . mandir social . s used . ram said mandir . court . court suggestion . settlement settlement . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  __juri__ . village . jharkhand . east . singhbhum . district . initiated . practice . naming . lanes . educated . girls . sumita . bhattacharya . pursuing . master . degree . history . picked . first . girl . lane . named . despite . high . school . village . literacy . rate . higher . state . average .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jharkhand . village . names . streets . after . most . educated . girls .\n",
      "INFO:tensorflow:GENERATED SUMMARY: jharkhand . juri . jharkhand . singhbhum . practice . practice in . lanes . educated . girls . girls s . educated high . s lane . s history . s district . east . village . jharkhand lane . to . singhbhum named . despite . school . girls [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  number . maoists . killed . security . forces . 2016 . witnessed . jump . 150 . compared . 2015 . centre . informed . lok . sabha . tuesday . number . left . wing . extremists . killed . 2016 . compared . 2015 . increased . 89 . 2015 . 222 . 2016 . number . highest . last . six . years . government . added .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: killing . of . maoists . witnessed . 150 . increase . in . 2016 . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: maoists . number . killed . forces . 150 . jump . centre . 150 s . 150 in . s . maoists . security . security compared . s compared . maoists compared . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  cbi . registered . fir . unknown . persons . allegedly . using . national . emblem . name . prime . minister . office . dupe . people . pretext . giving . houses . poor . families . case . filed . basis . reference . prime . minister . office . attached . complaint . village . head . jharkhand .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: fir . filed . over . fraud . using . pm . s . office . name . to . dupe . people .\n",
      "INFO:tensorflow:GENERATED SUMMARY: cbi . registered . fir . using . emblem . cbi . name . name in . name 2 . complaint . name filed . s case . s prime . fir pretext . s persons . fir families . s families . using filed . using prime . office . office minister . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  madhya . pradesh . man . arrested . lodging . false . complaint . via . twitter . indian . railways . personnel . board . bhind . indore . __intercity__ . express . monday . tweet . railway . minister . suresh . prabhu . railway . protection . force . sub . inspector . boarded . train . look . issue . found . intoxicated . tweeted . fake . complaint .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . arrested . for . fake . tweet . complaint . to . railway . minister .\n",
      "INFO:tensorflow:GENERATED SUMMARY: madhya . pradesh . arrested . man . false . complaint . twitter . railways . railways s . twitter tweeted . railways tweeted . s via . s indian . s railway . man protection . s protection . lodging . arrested inspector . train . look . twitter [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  two . sufi . clerics . hazrat . nizamuddin . dargah . delhi . gone . missing . pakistan . last . week . returned . india . external . affairs . minister . sushma . swaraj . intervened . matter . duo . went . pay . obeisance . shrine . returning . blamed . pakistani . newspaper . publishing . misleading . article . led . entire . fiasco .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: nizamuddin . clerics . who . went . missing . in . pak . return . to . india .\n",
      "INFO:tensorflow:GENERATED SUMMARY: two . sufi . clerics . nizamuddin . delhi . delhi in . missing . missing 2 . s . pakistan . india . pakistan blamed . s gone . s last . s went . hazrat . clerics went . s shrine . dargah . pakistani . newspaper . missing [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  india . added . pacer . mohammed . shami . squad . final . test . scheduled . begin . march . 25 . dharamshala . shami . last . played . india . november . 2016 . since . due . knee . injury . earlier . indian . captain . kohli . hinted . including . shami . saying . sent . play . vijay . hazare . trophy . match . practice .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: india . include . mohammed . shami . in . the . squad . for . final . test .\n",
      "INFO:tensorflow:GENERATED SUMMARY: india . added . mohammed . shami . test . shami over . test in . test 2 . test s . s 25 . india . s scheduled . shami scheduled . s shami . shami captain . s kohli . squad . final . shami saying . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  describing . steps . taken . assam . government . curb . rhino . poaching . kaziranga . national . park . environment . minister . anil . madhav . dave . said . forest . staff . empowered . use . firearms . without . prior . sanction . forest . staff . also . provided . immunity . prosecution . minister . added . number . anti . poaching . camps . also . increased . ensure . effective . surveillance .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: kaziranga . guards . can . use . firearms . without . sanction . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: forest . s . steps . assam . rhino . curb . kaziranga . rhino over . park . park sc . s national . park said environment . s minister . s government . assam said assam . to . forest . staff . rhino added . anti . poaching . kaziranga also . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . diljit . dosanjh . said . never . faced . camp . system . bollywood . added . never . faced . anybody . tried . bad . whoever . met . greeted . respect . love . diljit . known . acting . singing . punjabi . films . made . bollywood . debut . 2016 . film . udta . punjab .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: never . faced . any . camp . system . in . bollywood . diljit . dosanjh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: never . actor . faced . never . camp . system . system over . never s . bollywood . diljit . dosanjh . diljit said diljit . s respect . s love . never said never . s actor . never known . faced known . s never . never made . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  51 . year . old . alleged . member . khalistan . commando . force . __kcf__ . __gursewak__ . singh . __babla__ . arrested . country . made . pistol . delhi . police . tuesday . senior . crime . branch . officials . said . alleged . militant . involved . 75 . cases . include . terror . activities . murder . robbery . loot . majority . cases . registered . punjab . delhi . officials . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: khalistan . commando . force . member . wanted . in . 75 . cases . arrested .\n",
      "INFO:tensorflow:GENERATED SUMMARY: 51 . year . member . khalistan . commando . force . commando over . s . delhi . arrested . delhi s . s gursewak . s include . s country . old . old said year . terror . terror activities . robbery . loot . majority . cases . punjab . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  police . station . bhandara . district . maharashtra . sealed . local . municipal . council . non . payment . taxes . civic . authorities . sealed . police . station . failed . pay . municipal . tax . 1 . 19 . lakh . police . official . said . official . added . seal . broken . two . hours . later . sought . district . collector . intervention .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: maharashtra . police . station . sealed . for . non . payment . of . tax .\n",
      "INFO:tensorflow:GENERATED SUMMARY: police . station . bhandara . maharashtra . sealed . non . sealed over . non in . payment . non said . sealed said police . s authorities . s municipal . bhandara police . to . bhandara said official . seal . broken . broken added . hours . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  former . manchester . city . liverpool . striker . mario . balotelli . missed . opening . two . minutes . team . nice . ligue . 1 . match . nantes . struggled . loosen . shoelaces . balotelli . walking . start . match . went . towards . bench . assistance . helped . coach . loosened . laces .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: player . misses . two . mins . of . match . as . he . fails . to . undo . laces .\n",
      "INFO:tensorflow:GENERATED SUMMARY: manchester . former . city . liverpool . mario . balotelli . mario in . mario s . missed . balotelli s . s opening . s balotelli . manchester . s striker . liverpool loosen . s city . manchester start . s match . mario went . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  government . tuesday . admitted . instances . custodial . deaths . maharashtra . little . compared . states . 35 . instances . state . 2013 . 21 . 2014 . 19 . 2015 . union . minister . kiren . rijiju . said . bjp . satyapal . singh . former . mumbai . police . commissioner . however . called . state . police . best . india .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: more . custodial . deaths . in . maha . than . other . states . centre .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:GENERATED SUMMARY: instances . tuesday . instances . custodial . maharashtra . deaths . maharashtra i . little . states . 35 . 35 however . s compared . s states . s instances . instances said instances . s . bjp . custodial said singh . mumbai . police . maharashtra however . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  tibetan . religious . leader . dalai . lama . tuesday . said . global . terrorist . crisis . fast . reaching . nuclear . threshold . crosses . threshold . could . mean . entire . mankind . could . perish . added . dalai . lama . also . said . terrorism . crisis . can . not . resolved . unless . nations . come . together . single . platform .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: global . terrorism . fast . reaching . nuclear . threshold . dalai . lama .\n",
      "INFO:tensorflow:GENERATED SUMMARY: terrorist . s . global . terrorist . crisis . crisis i . i . fast . nuclear . threshold . lama . dalai . lama said lama . s threshold . s could . s leader . global said terrorist . terrorist said crisis . can . crisis said threshold [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  government . tuesday . proposed . make . aadhaar . card . mandatory . filing . income . tax . returns . itr . submit . returns . july . 1 . 2017 . aadhaar . number . would . also . needed . applying . permanent . account . number . pan . according . proposed . bill . pan . cards . linked . aadhaar . deemed . invalid .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: govt . proposes . to . make . aadhaar . mandatory . for . i . t . returns .\n",
      "INFO:tensorflow:GENERATED SUMMARY: aadhaar . government . make . aadhaar . aadhaar over . card . card in . filing . aadhaar s . returns . aadhaar according . s tax . s income . s returns . s . proposed . aadhaar number . card according . card proposed . bill . pan . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  bengaluru . based . graphic . designer . accused . taxi . hailing . startup . ola . using . design . made . hoardings . without . permission . design . shows . line . art . vidhana . soudha . building . bengaluru . 2015 . ola . also . accused . using . picture . app . launch . screen . without . permission . photographer .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . accuses . ola . of . copying . his . design . without . permission .\n",
      "INFO:tensorflow:GENERATED SUMMARY: bengaluru . bengaluru . designer . taxi . ola . ola in . design . startup . using . hoardings . without . ola also . s bengaluru . s using . s hailing . graphic . graphic 2015 . s 2015 . startup also . accused . ola picture . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  commentator . dean . jones . trolled . user . called . jones . cry . baby . one . tweets . jones . tweeted . many . cricketers . refuse . bat . poor . pitches . training . watching . test . day . 5 . reason . user . reply . jones . reminded . australia . currently . holds . border . gavaskar . cup . cricket . world . cup .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: dean . jones . trolls . user . after . he . calls . him . cry . baby .\n",
      "INFO:tensorflow:GENERATED SUMMARY: commentator . commentator . jones . trolled . cry . baby . baby in . tweets . baby s . jones s . tweets tweeted . s jones . jones tweeted jones . s called . dean . user . refuse . bat . australia . jones currently . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  supreme . court . tuesday . directed . karnataka . government . supply . 2 . 000 . cusecs . cauvery . water . tamil . nadu . every . day . matter . next . heard . july . 11 . court . earlier . dismissed . petition . seeking . compensation . states . loss . property . cauvery . water . related . dispute . karnataka . tamil . nadu .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: sc . directs . k . taka . to . release . 2 . 000 . cusecs . water . to . tn .\n",
      "INFO:tensorflow:GENERATED SUMMARY: supreme . supreme . karnataka . supply . 2 . 000 . cauvery . cusecs . water . tamil . dispute . tamil related . s compensation . s tamil . s nadu . s tuesday . court . karnataka seeking . s states . loss . property . cauvery [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  japanese . conglomerate . softbank . backed . deal . invest . 100 . million . android . co . founder . andy . rubin . smartphone . startup . named . essential . startup . reportedly . working . smartphone . compete . apple . iphone . 7 . fall . deal . comes . apple . committed . invest . 1 . billion . softbank . 100 . billion . fund . january . year .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: softbank . cancels . 100mn . funding . in . android . founder . s . startup .\n",
      "INFO:tensorflow:GENERATED SUMMARY: softbank . japanese . deal . softbank . invest . softbank in . 100 . android . android in . android s . s million . s founder . s startup . s deal . s . backed . softbank comes . apple . invest committed . billion [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  bharti . airtel . tuesday . hit . back . reliance . jio . allegations . around . former . fastest . network . ad . campaign . saying . complaint . deliberate . attempt . __malign__ . __brand__ . __misguide__ . __customers__ . mobile . internet . speed . testing . firm . ookla . stood . finding . airtel . fastest . broadband . network . claim . challenged . jio .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jio . s . complaint . over . ad . meant . to . misguide . customers . airtel .\n",
      "INFO:tensorflow:GENERATED SUMMARY: airtel . bharti . hit . back . reliance . fastest . jio . jio s . fastest in . ad . airtel . ad firm . s allegations . airtel saying campaign . s airtel . hit speed . s testing . back firm . to . reliance finding . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  __herring__ . fish . communicate . underwater . sounds . created . farting . according . study . high . pitch . buzzing . sound . helps . fish . communicate . night . without . alerting . predators . helping . fish . form . protective . __shoals__ . according . study . noise . always . accompanied . stream . bubbles .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: __herring__ . fish . communicate . through . farts .\n",
      "INFO:tensorflow:GENERATED SUMMARY: fish . herring . communicate . underwater . study . farting . pitch . pitch in . pitch s . pitch according . fish . s helping . s according . s created . fish alerting . s fish . fish helping . fish form . shoals . study according . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  sanjay . dutt . suffered . hairline . rib . fracture . shooting . fight . sequence . group . 19 . 20 . men . film . bhoomi . dutt . got . injured . jump . went . wrong . scene . film . climax . shot . chambal . dutt . advised . rest . continued . shooting . taking . rest . breaks .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: dutt . suffers . hairline . rib . fracture . at . shoot . of . fight . scene .\n",
      "INFO:tensorflow:GENERATED SUMMARY: sanjay . dutt . hairline . rib . fracture . shooting . sequence . fight . 20 . 19 . men . 20 advised . s film . s group . s dutt . s suffered . suffered . sanjay . hairline film . shot . dutt advised . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  google . ordered . us . judge . provide . information . anyone . searched . particular . fraud . victim . name . fraud . case . perpetrator . tried . initiating . transfer . 18 . lakh . victim . account . using . fake . id . google . image . search . victims . name . brings . image . used . id .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: google . ordered . to . name . people . who . searched . for . fraud . victim .\n",
      "INFO:tensorflow:GENERATED SUMMARY: google . ordered . us . judge . anyone . fraud . google . searched . victim . name . fraud in . s name . s account . s particular . s provide . us account . to . judge using . id . google image . search . victims . [unk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  indian . captain . virat . kohli . accused . australian . players . trolling . indian . team . physio . patrick . farhart . australian . daily . called . kohli . donald . trump . world . sport . like . president . trump . kohli . decided . blame . media . means . trying . hide . egg . smeared . right . across . face . article . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: kohli . has . become . the . donald . trump . of . world . sport . aus . media .\n",
      "INFO:tensorflow:GENERATED SUMMARY: indian . indian . virat . kohli . players . trolling . players over . physio . team . trump . daily . kohli pak . s decided . s australian . s kohli . s accused . virat decided . kohli media . s trying . hide . egg [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . ranbir . kapoor . seen . playing . sanjay . dutt . biopic . said . never . put . much . weight . added . plan . reduce . weight . next . phase . shooting . film . directed . rajkumar . hirani . ranbir . reportedly . portray . six . different . phases . dutt . life . starting . 22 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . never . put . on . so . much . weight . before . ranbir . kapoor .\n",
      "INFO:tensorflow:GENERATED SUMMARY: ranbir . actor . ranbir . never . weight . much . dutt . biopic . weight in . biopic s . biopic said biopic . ranbir said ranbir . s shooting . ranbir film . s playing . ranbir directed . s ranbir . much reportedly . weight reportedly . phases . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "INFO:tensorflow:We've been decoding with same checkpoint for 60 seconds. Time to load new checkpoint\n",
      "INFO:tensorflow:Loading checkpoint logs_26_5\\train\\model.ckpt-4055\n",
      "INFO:tensorflow:Restoring parameters from logs_26_5\\train\\model.ckpt-4055\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  taxi . hailing . startup . uber . co . founder . travis . kalanick . might . step . ceo . according . reports . kalanick . reportedly . quit . position . uber . world . valuable . startup . hires . first . coo . comes . resignation . number . top . uber . executives . including . president . jeff . jones . uber . dealing . allegations . promoting . culture . sexism .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: uber . co . founder . travis . might . step . down . as . ceo . report .\n",
      "INFO:tensorflow:GENERATED SUMMARY: uber . hailing . startup . uber . kalanick . travis . uber in . kalanick over . kalanick s . kalanick including . uber including . s ceo . uber according . s uber . uber number . s co . uber president . s kalanick . uber [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  emma . watson . dan . stevens . starrer . beauty . beast . earned . 2 . 300 . crore . 357 . million . opening . weekend . film . opening . weekend . collections . nearly . three . times . higher . __cinderella__ . __moanna__ . __zootopia__ . revealed . bollywood . trade . analyst . taran . adarsh . india . fantasy . musical . earned . 6 . crore . opening . weekend .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: beauty . and . the . beast . earns . 2 . 300 . crore . in . opening . weekend .\n",
      "INFO:tensorflow:GENERATED SUMMARY: emma . watson . stevens . beauty . 2 . 2 over . india . 300 . crore . 357 . 6 . 357 revealed . weekend . s 357 . s opening . dan . watson revealed . trade . trade revealed . taran . adarsh . india adarsh . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  punjab . cm . captain . amarinder . singh . seek . legal . opinion . whether . cabinet . minister . navjot . singh . sidhu . continue . appear . popular . tv . show . hosted . comedian . kapil . sharma . singh . said . know . constitution . law . says . matter . sidhu . given . tourism . local . bodies . portfolios . congress . government .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: punjab . cm . to . seek . legal . opinion . over . sidhu . s . tv . commitments .\n",
      "INFO:tensorflow:GENERATED SUMMARY: punjab . cm . amarinder . seek . legal . cabinet . sidhu . sidhu in . cabinet 2 . bodies . portfolios . s . punjab . opinion . singh . singh singh . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  rajya . sabha . tuesday . passed . hiv . aids . prevention . control . bill . 2014 . voice . vote . bill . seeks . ensure . equal . rights . hiv . aids . affected . people . getting . treatment . admission . educational . institutions . getting . jobs . anyone . india . hiv . aids . government . take . care . treatment . health . minister . jp . nadda . said .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rs . passes . bill . to . ensure . equal . rights . of . aids . patients .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rajya . sabha . passed . hiv . aids . bill . bill over . bill s . bill said bill . s 2014 . s anyone . s ensure . hiv getting . s aids . s . rajya . hiv government . aids government . care . bill government . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  managing . director . idea . himanshu . kapania . said . integrating . operations . vodafone . idea . cost . combined . entity . around . 13 . 400 . crore . however . said . synergies . leading . cost . saving . result . net . benefit . 65 . 000 . crore . merged . entity . idea . vodafone . confirmed . merger . monday . form . india . largest . telecom . firm .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: vodafone . idea . merger . to . cost . 13 . 400 . crore . says . idea . md .\n",
      "INFO:tensorflow:GENERATED SUMMARY: integrating . leading . integrating . operations . vodafone . idea . cost . combined . idea i . 2 . crore . 400 . crore said entity . s around . idea said idea . integrating said integrating . my . integrating entity . operations confirmed . vodafone monday . idea monday . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  uttar . pradesh . chief . minister . yogi . adityanath . said . lok . sabha . tuesday . state . witness . corruption . communal . riots . government . uttar . pradesh . prime . minister . narendra . modi . dream . state . cm . added . yogi . yet . give . lok . sabha . membership . promised . free . state . __goondagardi__ . hooliganism .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: up . will . witness . no . communal . riots . on . my . watch . yogi .\n",
      "INFO:tensorflow:GENERATED SUMMARY: yogi . uttar . lok . sabha . witness . corruption . witness in . communal . riots . yogi . yogi said yogi . s adityanath . adityanath . minister . yogi government . s minister . chief . state . sabha added . witness yet . corruption lok . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  according . reports . upcoming . superhero . film . power . rangers . first . feature . gay . superhero . character . __trini__ . yellow . ranger . played . actress . becky . g . revealed . one . scene . girlfriend . problems . revealed . early . reviews . film . directed . dean . __israelite__ . film . release . march . 24 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: power . rangers . to . feature . first . gay . superhero . reports .\n",
      "INFO:tensorflow:GENERATED SUMMARY: reports . according . superhero . film . power . rangers . first . gay . feature . gay sc . s . first revealed . s superhero . s trini . s according . reports . reports revealed . superhero revealed . early . reviews . film revealed . gay directed . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . nawazuddin . siddiqui . said . contract . killers . also . romantic . romance . help . guns . romance . done . many . ways . every . person . way . express . __romanticism__ . added . nawazuddin . actor . already . played . gangster . gangs . wasseypur . seen . playing . contract . killer . babumoshai . __bandukbaaz__ .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: contract . killers . can . romance . with . help . of . guns . nawazuddin .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:GENERATED SUMMARY: contract . actor . contract . killers . romantic . romance . help . guns . romance i . romance report . killer . nawazuddin . s actor . s many . s also . contract said contract . s . contract actor . killers played . romantic gangs . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  american . express . biggest . credit . card . issuer . customer . spending . paid . ceo . ken . chenault . 22 . million . nearly . 144 . crore . work . 2016 . 19 . increase . compensation . puts . par . goldman . sachs . ceo . lloyd . blankfein . notably . __amex__ . counts . warren . buffetts . berkshire . hathaway . biggest . shareholder . climbed . 6 . 5 . 2016 . tumbling . 25 . previous . year .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: american . express . raises . ceo . s . pay . to . 144 . crore . in . a . year .\n",
      "INFO:tensorflow:GENERATED SUMMARY: express . american . biggest . credit . card . issuer . customer . ceo . paid . paid s . paid climbed . s . crore . s spending . s 22 . s american . express . biggest berkshire . s biggest . shareholder . 6 . customer [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  kitchen . gadget . company . __kitchenaid__ . accused . sexism . posting . ad . pink . appliances . aimed . women . ad . featured . words . __kitchenaid__ . women . alongside . pink . products . part . limited . edition . range . raise . money . breast . cancer . charity . __kitchenaid__ . however . stated . colour . used . symbol . hope .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: firm . accused . of . sexism . for . its . pink . kitchen . gadgets . ad .\n",
      "INFO:tensorflow:GENERATED SUMMARY: kitchen . gadget . kitchenaid . sexism . ad . ad in . pink . appliances . ad 2 . s . women . ad however . s aimed . s pink . s kitchenaid . gadget edition . s raise . money . breast . cancer . charity . ad stated . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  singer . sukhwinder . singh . said . get . married . soon . jump . well . die . 45 . year . old . singer . revealed . music . composer . ar . rahman . pestering . get . married . want . __fantabulous__ . relationship . want . feel . special . added . sukhwinder .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: if . i . dont . get . married . in . 2017 . ill . kill . myself . sukhwinder .\n",
      "INFO:tensorflow:GENERATED SUMMARY: married . singer . get . soon . married . jump . die . well . 45 . die sc . s . sukhwinder . singh . sukhwinder said year . s singer . married revealed get . s rahman . get pestering . married get . married fantabulous . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  two . member . probe . committee . pakistan . cricket . board . pcb . travel . united . kingdom . look . pakistani . opener . nasir . jamshed . alleged . involvement . pakistan . super . league . psl . spot . fixing . scandal . according . reports . jamshed . earlier . investigated . national . crime . agency . uk . passport . withheld . authorities . granted . bail .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: pcb . panel . to . probe . nasir . jamshed . in . uk . over . psl . spot . fixing .\n",
      "INFO:tensorflow:GENERATED SUMMARY: two . member . probe . pakistan . pcb . travel . board . united . look . pcb s . united earlier . s kingdom . s according . s opener . s committee . probe according . pakistan earlier . to . cricket . crime . pcb agency . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  steve . smith . reached . career . high . 941 . rating . points . latest . icc . test . rankings . batsmen . fifth . highest . rating . points . recorded . test . cricket . 130 . year . history . smith . rating . points . fifth . best . ever . bradman . 961 . __len__ . hutton . 945 . jack . hobbs . ricky . ponting . __942__ . notably . highest . rating . player . receive . 1000 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: smith . attains . fifth . highest . batting . rating . in . test . cricket .\n",
      "INFO:tensorflow:GENERATED SUMMARY: smith . steve . smith . career . 941 . icc . test . points . icc s . rankings . highest . highest notably . s latest . s fifth . s points . s reached . high . smith 945 . jack . ricky . ponting . test notably . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  australian . pacer . mitchell . starc . said . looking . forward . bowl . ravichandran . ashwin . australia . might . take . ashwin . advice . hit . badge . starc . given . send . ashwin . second . test . ashwin . tapping . forehead . reply . starc . taunting . abhinav . mukund . similar . manner . earlier .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . might . take . ashwin . s . advice . and . hit . him . on . head . says . starc .\n",
      "INFO:tensorflow:GENERATED SUMMARY: australian . pacer . looking . bowl . bowl i . ashwin . ashwin in . australia . advice . starc . ashwin said ashwin . s second . s take . s pacer . mitchell . looking said bowl . looking tapping . bowl forehead . ashwin reply . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  us . drone . airstrike . afghanistan . killed . pakistani . militant . accused . involvement . deadly . attack . bus . carrying . sri . lanka . cricket . team . 2009 . pakistani . security . sources . claimed . us . unmanned . aircraft . struck . car . carrying . qari . mohammad . yasin . also . known . ustad . aslam . sunday . southwestern . afghan . province . __paktika__ .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: militant . behind . attack . on . lankan . cricket . team . in . pak . killed .\n",
      "INFO:tensorflow:GENERATED SUMMARY: us . us . airstrike . afghanistan . drone . bus . attack . attack in . attack s . deadly . drone sunday . s carrying . s militant . s pakistani . drone carrying . to . us mohammad . s yasin . us known . ustad . bus sunday . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  women . mlas . ruling . tdp . opposition . ysr . congress . involved . scuffle . tuesday . andhra . pradesh . assembly . premises . incident . started . tdp . mla . tried . stop . ysr . congress . member . latter . addressing . media . increasing . atrocities . women . andhra . security . intervene . control . situation .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: women . mlas . get . into . scuffle . in . andhra . pradesh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: women . women . mlas . tdp . opposition . ysr . andhra . scuffle . ysr in . andhra s . scuffle addressing . s congress . s involved . s ysr . tdp tuesday . s tdp . tdp addressing . to . ysr addressing . women [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  greek . police . reportedly . found . neutralised . eight . suspected . parcel . bombs . postal . sorting . centre . athens . monday . parcels . addressed . officials . economic . institutions . companies . various . european . countries . last . week . parcel . bomb . explosion . imf . office . paris . injured . one . person . another . targeting . german . finance . ministry . intercepted .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: greek . police . intercept . parcel . bombs . headed . to . eu . institutes .\n",
      "INFO:tensorflow:GENERATED SUMMARY: greek . police . found . neutralised . suspected . bombs . parcel . postal . sorting . centre . sorting s . s . athens . centre monday . s eight . s reportedly . found reportedly . s imf . paris . injured . one . bombs person . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  centre . tuesday . proposed . lower . limit . cash . transactions . 2 . lakh . earlier . proposal . 3 . lakh . union . budget . penalty . violating . fine . equivalent . amount . transaction . revenue . secretary . hasmukh . adhia . tweeted . move . aims . discourage . cash . payments . well . curb . black . money .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: govt . proposes . ban . on . cash . transactions . above . 2 . lakh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: centre . centre . lower . limit . cash . 2 . transactions . lakh . 3 . lakh sc . s . lakh tweeted . money . centre earlier . s union . s tuesday . proposed . lower tweeted . limit tweeted tweeted . cash tweeted . lakh [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  railway . track . designed . run . 19 . storey . residential . building . city . chongqing . china . __liziba__ . station . located . building . sixth . eighth . floor . built . combat . lack . space . city . station . noise . insulation . system . trains . reportedly . run . rubber . tyres . air . suspension . units .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: china . has . a . train . track . passing . through . an . apartment .\n",
      "INFO:tensorflow:GENERATED SUMMARY: railway . railway . run . 19 . storey . building . china . china in . building s . chongqing . station . building reportedly . s city . s residential . s designed . designed . track . track station . insulation . system . trains . china [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . shah . rukh . khan . revealed . hired . lady . bodyguards . protect . female . fans . added . getting . rude . men . pushing . women . trying . protect . shah . rukh . said . jokingly . love . female . fans . hurts . long . nails . difficult . explaining . nail . marks . wife . kids .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . lady . bodyguards . to . protect . me . from . female . fans . srk .\n",
      "INFO:tensorflow:GENERATED SUMMARY: actor . actor . hired . lady . bodyguards . bodyguards i . protect . female . fans . fans pak . s . rude . shah . khan . shah added . s rukh . hired said hired . hired revealed . female said fans . hurts . bodyguards long . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  pakistan . coach . mickey . arthur . tuesday . said . felt . gutted . pakistan . super . league . spot . fixing . scandal . adding . players . found . guilty . blame . players . become . greedy . hurting . international . cricket . general . recent . case . hurt . pakistan . arthur . added . notably . many . five . pakistani . players . accused . spot . fixing . psl .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: greed . has . hurt . pakistani . cricket . says . coach . mickey . arthur .\n",
      "INFO:tensorflow:GENERATED SUMMARY: pakistan . coach . felt . gutted . pakistan . super . spot . fixing . fixing sc . s . players . mickey . arthur . arthur said arthur . s players . s pakistan . felt said felt . felt added . gutted added . pakistan many . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  rights . groups . victims . families . held . rally . paris . last . week . police . brutality . recent . baton . rape . young . man . heightened . tensions . alleged . __beatings__ . deaths . police . custody . rally . express . anger . face . repeated . police . violence . warn . perverted . notion . public . security . rights . group . said .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rights . groups . in . paris . hold . rally . against . police . brutality .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rights . rights . families . rally . paris . paris in . paris over . paris pak . s . brutality . man . rape . police . victims . s recent . s express . victims repeated . s face . rally repeated . violence . warn . paris [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  uk . announced . ban . electronic . devices . larger . mobiles . including . laptops . tablets . cabin . baggage . flights . six . middle . eastern . countries . ban . affects . travellers . turkey . lebanon . jordan . egypt . tunisia . saudi . arabia . follows . reports . us . imposed . similar . restrictions . flights . originating . several . middle . eastern . north . african . nations .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: uk . bans . laptops . tablets . on . flights . from . 6 . nations .\n",
      "INFO:tensorflow:GENERATED SUMMARY: uk . announced . electronic . ban . mobiles . laptops . laptops over . laptops in . baggage . flights . uk . flights several . s six . s cabin . s devices . s . devices . ban imposed . mobiles imposed . flights restrictions . originating . middle . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  amid . controversy . punjab . minister . navjot . singh . sidhu . tv . commitments . cricketer . turned . politician . said . nobody . business . 6 . pm . sidhu . indicated . appearing . tv . shows . amount . breach . office . profit . law . however . punjab . cm . amarinder . singh . sought . legal . opinion . sidhu . tv . commitments . despite . minister .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: nobody . s . business . what . i . do . after . 6 . pm . sidhu . on . tv . shows .\n",
      "INFO:tensorflow:GENERATED SUMMARY: nobody . controversy . punjab . sidhu . sidhu in . tv . pm . sidhu over . commitments . politician . sidhu said tv . s cricketer . s politician . s singh . punjab said punjab . nobody . 6 . sidhu amarinder . s tv . sidhu sought . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  salman . khan . paid . 44 . 5 . crore . become . bollywood . highest . advance . tax . payer . 2016 . 17 . kapil . sharma . submitted . advance . tax . 23 . 9 . crore . compared . 7 . crore . 2015 . 16 . however . income . tax . department . declined . give . advance . tax . figures . shah . rukh . khan . amitabh . bachchan . aishwarya . rai . investigation . panama . papers . case .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: salman . pays . 44 . cr . as . bollywood . s . highest . advance . tax . payer .\n",
      "INFO:tensorflow:GENERATED SUMMARY: salman . khan . 44 . crore . crore over . highest . highest s . highest in . crore s . s advance . s . salman . paid . s tax . s khan . paid figures . s rukh . crore rukh . amitabh . aishwarya . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  madras . high . court . examined . medical . report . __birthmark__ . scar . found . actor . dhanush . body . mentioned . elderly . couple . claiming . parents . however . report . stated . small . superficial . mole . another . large . mole . removed . hearing . case . deferred . march . 27 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: hc . gets . medical . report . of . no . __birthmark__ . on . dhanush . s . body .\n",
      "INFO:tensorflow:GENERATED SUMMARY: madras . high . court . report . report over . scar . dhanush . report in . found . dhanush s . s actor . madras . s birthmark . medical . examined . examined stated . s small . medical mole . mole . large . mole large . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  karnataka . government . urged . supreme . court . declare . late . tamil . nadu . cm . j . jayalalithaa . convict . disproportionate . assets . case . state . argued . apex . court . february . ruling . lessened . charges . jayalalithaa . due . demise . error . __merited__ . __relook__ . court . judgement . convicted . aiadmk . sasikala . natarajan .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: declare . jayalalithaa . as . convict . in . assets . case . karnataka .\n",
      "INFO:tensorflow:GENERATED SUMMARY: karnataka . government . supreme . declare . late . tamil . j . jayalalithaa . jayalalithaa in . j s . jayalalithaa merited . s due . s convict . s court . supreme ruling . s . court . declare due . demise . error . j merited . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  united . states . banned . travellers . coming . 10 . airports . mainly . middle . east . north . africa . carrying . devices . larger . mobile . cabin . response . terror . threats . according . reports . restrictions . reportedly . imposed . passengers . travelling . airports . jordan . egypt . turkey . saudi . arabia . united . arab . emirates . kuwait . morocco . qatar .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: us . bans . devices . larger . than . mobiles . on . some . flights . reports .\n",
      "INFO:tensorflow:GENERATED SUMMARY: united . united . travellers . 10 . 10 over . airports . saudi . north . middle . north s . north i . qatar . s east . s carrying . s devices . banned . banned travelling . to . egypt . turkey . saudi arabia . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  supreme . court . tuesday . pulled . centre . rbi . demonetisation . measures . taken . asking . give . window . citizens . deposit . old . currency . notes . december . 31 . apex . court . gave . central . government . reserve . bank . india . two . weeks . time . reply . notice .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: why . no . deposit . window . for . old . notes . after . dec . 31 . sc . to . govt .\n",
      "INFO:tensorflow:GENERATED SUMMARY: supreme . court . pulled . centre . rbi . demonetisation . rbi over . demonetisation in . window . rbi s . rbi government . s taken . s measures . s tuesday . court tuesday . pulled gave . centre gave . s window . bank . india [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  attempts . settle . ayodhya . dispute . court . failed . nine . times . 150 . years . first . attempt . made . british . 1859 . fence . erected . separate . places . worship . communities . three . attempts . initiated . former . pms . atal . bihari . vajpayee . chandra . shekhar . pv . narasimha . rao .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: what . have . been . the . past . attempts . to . resolve . ayodhya . dispute .\n",
      "INFO:tensorflow:GENERATED SUMMARY: attempts . settle . ayodhya . dispute . times . failed . 150 . first . 150 in . years . first bihari . s . dispute three . s attempt . s court . settle three . settle initiated . s initiated . pms . atal . 150 bihari . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actress . taapsee . pannu . revealed . wanted . slap . people . eve . teased . college . added . people . pinched . wrong . side . wanted . slap . probably . scared . taapsee . said . use . skills . tackle . harassment .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . wanted . to . slap . people . when . i . was . eve . teased . taapsee .\n",
      "INFO:tensorflow:GENERATED SUMMARY: taapsee . actress . wanted . slap . slap i . eve . eve said . college . taapsee . teased . wrong . pannu . s added . taapsee revealed teased . s pannu . wanted revealed actress . s people . slap side . slap probably . scared . taapsee said taapsee . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  brothers . kasam . __juma__ . two . artisans . bhuj . gujarat . created . __bodice__ . one . outfits . worn . actress . emma . watson . film . beauty . beast . pattern . __bodice__ . hand . embroidered . using . technique . called . __aari__ . work . revealed . __sinad__ . sullivan . assistant . costume . designer . film .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: gujarat . artists . created . emma . s . beauty . and . the . beast . costume .\n",
      "INFO:tensorflow:GENERATED SUMMARY: brothers . kasam . juma . artisans . gujarat . gujarat in . bhuj . one . outfits . brothers . outfits revealed . gujarat revealed . s created . s bodice . two . kasam embroidered . s using . bhuj called . aari . work . outfits [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  man . named . altaf . __chaki__ . reportedly . received . six . __misprinted__ . 500 . notes . bank . baroda . atm . jamnagar . gujarat . according . one . notes . completely . blank . one . side . serial . numbers . two . others . printed . clearly . currency . value . written . hindi . printed . clearly . three . notes .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . gets . six . __misprinted__ . 500 . notes . from . atm .\n",
      "INFO:tensorflow:GENERATED SUMMARY: man . man . altaf . six . 500 . notes . notes in . bank . atm . baroda . atm s . s jamnagar . s serial . s misprinted . altaf side . s chaki . named . six others . clearly . currency . value . notes written . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  newly . restored . site . jesus . christ . tomb . church . holy . sepulchre . jerusalem . old . city . unveiled . public . wednesday . comes . underwent . restoration . nine . months . cost . 4 . million . 26 . crore . around . 50 . experts . national . technical . university . athens . involved . restoration .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jesus . tomb . to . be . unveiled . to . public . after . 4mn . restoration .\n",
      "INFO:tensorflow:GENERATED SUMMARY: newly . restored . site . jesus . tomb . church . jerusalem . holy . jerusalem 2 . s . church around . s old . s cost . s city . s newly . site 4 . s million . crore . crore around . experts . church [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actress . alia . bhatt . sister . shaheen . bhatt . slammed . media . taking . pictures . aishwarya . rai . bachchan . father . __krishnaraj__ . rai . funeral . never . understood . media . showing . high . profile . funerals . take . pictures . grieving . family . members . wrote . post . shaheen . wrote . insensitivity . mind . boggling .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: alia . s . sister . slams . media . for . taking . pics . of . aish . at . funeral .\n",
      "INFO:tensorflow:GENERATED SUMMARY: alia . actress . bhatt . sister . alia . shaheen . aishwarya . aishwarya over . aishwarya s . aishwarya wrote . s pictures . s taking . alia rai . s bhatt . alia funerals . s sister . sister pictures . family . aishwarya members . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "INFO:tensorflow:We've been decoding with same checkpoint for 61 seconds. Time to load new checkpoint\n",
      "INFO:tensorflow:Loading checkpoint logs_26_5\\train\\model.ckpt-4055\n",
      "INFO:tensorflow:Restoring parameters from logs_26_5\\train\\model.ckpt-4055\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  housing . co . founders . employees . dissatisfied . board . allocation . ousted . co . founder . rahul . yadav . shares . worth . 200 . crore . yadav . pledged . shares . distributed . among . employees . housing . ceo . jason . kothari . freecharge . ceo . allotted . 6 . lakh . shares . employees . claim . receive . thousands . shares . received . hundreds .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rahul . yadav . s . esops . improperly . issued . claims . housing . staff .\n",
      "INFO:tensorflow:GENERATED SUMMARY: housing . co . founders . employees . dissatisfied . allocation . rahul . rahul in . founder . crore . 200 . shares . crore allotted . s shares . s yadav . s co . employees kothari . to . founders allotted . lakh . shares shares . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  technology . major . apple . today . unveiled . limited . edition . version . iphone . 7 . iphone . 7 . plus . smartphones . red . colour . first . time . ever . iphones . aluminium . finish . available . 128 . gb . 256 . gb . options . march . 24 . apple . previously . released . special . edition . phone . cases . __watchbands__ . support . aids . charity .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: apple . unveils . iphone . 7 . and . 7 . plus . in . red . colour .\n",
      "INFO:tensorflow:GENERATED SUMMARY: apple . technology . today . apple . iphone . iphone to . iphone in . 7 . version . plus . iphone edition . s 7 . s plus . apple gb . s apple . apple options . s 24 . apple previously . released . iphone previously . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  isro . commissioned . one . metre . hypersonic . wind . tunnel . shock . tunnel . world . third . largest . facility . vikram . sarabhai . space . centre . thiruvananthapuram . aerospace . vehicles . enter . earth . atmosphere . high . speeds . creating . shock . waves . simulated . tunnel . optimising . aircraft . design .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: isro . commissions . world . s . 3rd . largest . hypersonic . wind . tunnel .\n",
      "INFO:tensorflow:GENERATED SUMMARY: isro . isro . commissioned . tunnel . tunnel over . tunnel in . tunnel 2 . tunnel s . tunnel creating . s third . isro aerospace . s tunnel . s hypersonic . s commissioned . isro atmosphere . s earth . tunnel high . shock . tunnel [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  sri . lanka . cricket . monday . declared . dead . local . newspaper . published . obituary . reading . affectionate . remembrance . sri . lankan . cricket . died . oval . 19 . march . 2017 . body . cremated . ashes . taken . bangladesh . similar . satirical . obituary . english . cricket . 1882 . gave . birth . ashes . series .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: newspaper . writes . ashes . like . obituary . for . sri . lanka . cricket .\n",
      "INFO:tensorflow:GENERATED SUMMARY: sri . sri . declared . dead . newspaper . newspaper in . obituary . reading . affectionate . obituary s . affectionate similar . s . cricket . cricket published . s local . cricket monday . s taken . dead taken . s satirical . obituary obituary . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actress . rani . mukerji . made . acting . debut . 1996 . bengali . film . __biyer__ . phool . directed . father . ram . mukherjee . made . bollywood . debut . film . raja . ki . __aayegi__ . baraat . later . year . rani . turned . 39 . tuesday . best . known . movies . kuch . kuch . hota . hai . mardaani . black . one . killed . jessica .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rani . mukerji . made . acting . debut . in . her . father . s . bengali . film .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rani . actress . made . acting . debut . bengali . bengali in . 1996 . film . phool . father . ram . s . rani . debut directed . s film . s made . mukerji . mukerji tuesday . s movies . kuch . hota . bengali [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  us . startup . __zapier__ . offering . employees . 6 . 5 . lakh . leave . bay . area . since . rent . region . almost . doubled . last . five . years . offer . availed . employees . commit . work . least . one . year . work . great . company . live . wherever . want . said . __zapier__ . ceo .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: startup . offers . employees . 6 . 5 . lakh . each . to . leave . bay . area .\n",
      "INFO:tensorflow:GENERATED SUMMARY: us . startup . offering . employees . 6 . lakh . 6 in . leave . bay . leave said . lakh said bay . s work . s since . s offering . offering least . s least . us . 5 . work . great . 6 company . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  vodafone . ceo . vittorio . colao . responding . question . whether . merger . idea . due . reliance . jio . said . jio . talk . jio . market . leader . someone . else . stated . kumar . mangalam . birla . merged . entity . chairman . said . business . logic . driven . combination .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: merger . with . idea . not . because . of . reliance . jio . vodafone . ceo .\n",
      "INFO:tensorflow:GENERATED SUMMARY: vodafone . ceo . vittorio . jio . idea . merger . reliance . reliance said . jio said jio . reliance stated . s else . vodafone . s leader . s responding . ceo said ceo . vodafone stated . kumar . mangalam . birla . merger said reliance . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  limited . time . nap . cafe . opened . tokyo . japan . featuring . ten . beds . worth . 5 . 8 . lakh . customers . allowed . nap . maximum . two . hours . ordering . least . one . food . item . cafe . menu . visitors . served . free . cups . coffee . sleeping . waking .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: cafe . in . japan . lets . guests . take . a . nap . on . beds . worth . 5 . 8 . lakh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: limited . limited . cafe . tokyo . japan . japan in . beds . japan s . 5 . 8 . beds least . s ten . nap . s featuring . cafe ordering . s opened . time . nap least . item . menu . visitors . beds [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  share . buyback . means . repurchase . shares . company . usually . proposed . company . shares . undervalued . buybacks . reduce . supply . shares . thereby . facilitating . higher . price . reduction . number . shares . earnings . per . share . increase . company . spends . cash . buy . stock . cash . assets . balance . sheet . reduce .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: what . is . the . meaning . of . a . share . buyback .\n",
      "INFO:tensorflow:GENERATED SUMMARY: buyback . share . means . repurchase . shares . usually . undervalued . buybacks . shares in . shares s . shares company . s company . buyback . s shares . repurchase earnings . s share . company . company company . spends . cash . buy [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  poster . spotted . outside . congress . office . lucknow . announcing . award . 5 . lakh . person . finds . poll . strategist . prashant . kishor . presents . party . workers . poster . removed . immediately . spotted . state . congress . chief . raj . babbar . notably . prashant . kishor . congress . chief . poll . strategist . recent . uttar . pradesh . polls .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: cong . poster . offers . 5l . reward . for . finding . prashant . kishor .\n",
      "INFO:tensorflow:GENERATED SUMMARY: poster . poster . outside . lucknow . lucknow over . award . 5 . 5 s . lakh . 5 report . s recent . polls . s person . s announcing . s spotted . congress . office . poster notably . prashant . kishor . award chief . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  television . personality . kim . kardashian . opening . robbery . incident . paris . said . mentally . prepared . raped . killed . robbers . grabbed . legs . clothes . robe . said . describing . incident . took . tragic . horrific . experience . let . diminish . added . kim .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . mentally . prepared . myself . to . be . raped . kim . on . paris . robbery .\n",
      "INFO:tensorflow:GENERATED SUMMARY: kim . television . prepared . mentally . kim . robbery . paris . paris my . s . paris i . paris said paris . s robe . kim said kim . s kardashian . kardashian . personality . kardashian said describing . describing . raped . tragic . paris took . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  rani . mukerji . said . post . pictures . daughter . adira . social . media . husband . aditya . chopra . private . person . respects . added . also . hate . saying . fans . ask . pictures . stay . away . social . media . adira . born . 2015 . couple . first . child .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . do . not . post . adira . s . pic . as . aditya . is . a . private . person . rani .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rani . post . pictures . daughter . adira . husband . adira i . husband s . aditya . rani . private . mukerji . rani said mukerji . s person . s social . post said post . post saying . pictures pictures . away . media . adira social . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . anupam . kher . said . rules . censor . board . need . revisited . since . written . 1952 . added . present . government . appointed . committee . headed . filmmaker . shyam . benegal . know . happened . recommendations . anupam . said . even . though . new . rules . made . implemented . n .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: the . rules . of . the . censor . board . need . to . be . revisited . anupam .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rules . actor . rules . censor . board . need . revisited . revisited i . since . 1952 . anupam . kher . anupam said know . s added . anupam added . [unk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . tom . cruise . training . stunt . upcoming . film . mission . impossible . 6 . year . revealed . producer . david . ellison . going . unbelievable . thing . tom . cruise . done . movie . added . david . earlier . tom . filmed . stunt . top . 2 . 717 . feet . tall . burj . khalifa . world . tallest . skyscraper .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: tom . cruise . training . for . mission . impossible . 6 . stunt . for . a . yr .\n",
      "INFO:tensorflow:GENERATED SUMMARY: tom . actor . cruise . tom . stunt . mission . impossible . impossible in . 6 . 6 s . impossible revealed . s upcoming . s year . training . s . cruise revealed . tom revealed . producer . david . ellison . unbelievable . tom [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  iit . kanpur . annual . technological . entrepreneurial . festival . __techkriti__ . themed . __factualising__ . __fictions__ . held . march . 23 . 26 . __techkriti__ . host . talks . author . balaji . viswanathan . padma . shri . awardee . vijay . prasad . __dimri__ . among . others . hyperloop . exhibition . competitions . like . __robowars__ . also . held . playback . singer . sonu . nigam . perform . __techkriti__ . 2017 . final . night .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: iit . kanpur . to . host . __techkriti__ . 2017 . from . march . 23 . to . 26 .\n",
      "INFO:tensorflow:GENERATED SUMMARY: iit . kanpur . technological . iit . festival . themed . themed over . sonu . march . 23 . 26 . iit report . s factualising . s fictions . s techkriti . entrepreneurial . technological robowars . s robowars . iit also . playback . sonu singer . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  taking . dig . bjp . appointing . two . deputy . chief . ministers . shiv . sena . said . aimed . keeping . adityanath . free . perform . religious . duties . rather . performing . religious . duties . adityanath . focus . delivering . good . governance . sena . wrote . mouthpiece . saamana . notably . shiv . sena . given . deputy . cm . post . bjp . maharashtra .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: yogi . left . free . to . do . religious . duties . sena . on . 2 . deputy . cms .\n",
      "INFO:tensorflow:GENERATED SUMMARY: taking . dig . bjp . free . keeping . perform . ministers . sena . shiv . sena s . shiv said sena . s wrote . appointing . s deputy . s . appointing said . adityanath . keeping wrote . adityanath notably . shiv notably . sena notably . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  india . fourth . biggest . software . services . exporter . hcl . technologies . buy . back . shares . worth . 3 . 500 . crore . following . similar . move . tcs . hcl . tech . approved . repurchase . 3 . 5 . crore . shares . 1 . 000 . notably . firms . large . amounts . unused . cash . reserves . opting . share . buybacks . return . surplus . cash . shareholders .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: hcl . technologies . approves . 3 . 500 . crore . share . buyback . plan .\n",
      "INFO:tensorflow:GENERATED SUMMARY: india . india . biggest . exporter . hcl . buy . back . buy s . back s . 3 . crore . 500 . s worth . s technologies . s hcl . biggest notably . s . services . exporter amounts . cash . reserves . opting . back share . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  image . snowy . russian . landscape . covered . dark . angular . lines . stretching . miles . taken . iss . astronaut . thomas . pesquet . left . social . media . users . confused . nature . formations . nasa . explained . formation . occurred . region . major . __shelterbelt__ . line . trees . planted . protect . crops . reduce . erosion .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: astronaut . captures . geometric . russian . snowy . landscape . photo .\n",
      "INFO:tensorflow:GENERATED SUMMARY: image . image . russian . covered . dark . iss . astronaut . lines . stretching . iss s . astronaut major . s explained . s miles . s thomas . s taken . snowy . landscape . landscape occurred . to . dark major . line . trees . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  world . football . governing . body . fifa . banned . __ghanian__ . referee . joseph . __odartei__ . __lamptey__ . found . guilty . influencing . result . world . cup . qualifier . south . africa . senegal . november . last . year . south . africa . 2 . 1 . first . goal . coming . penalty . awarded . handball . despite . ball . hitting . defender . leg .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: referee . found . guilty . of . fixing . wc . qualifier . banned . for . life .\n",
      "INFO:tensorflow:GENERATED SUMMARY: world . world . governing . fifa . referee . referee i . s . joseph . referee in . referee s . s odartei . s lamptey . s result . s africa . football . body . body first . goal . penalty . awarded . handball . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  google . maps . introduced . feature . beta . version . android . app . remind . users . parked . cars . users . manually . enter . details . parking . spot . p . icon . placed . map . help . navigate . back . spot . later . users . also . share . saved . location . others .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: google . maps . to . remind . users . of . their . parking . location .\n",
      "INFO:tensorflow:GENERATED SUMMARY: google . google . maps . beta . android . app . users . parked . android in . users s . cars . cars placed . s users . s remind . s introduced . feature . feature placed . to . beta navigate . spot . android later . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  hollywood . actor . george . clooney . surprised . pat . adams . fan . stays . care . home . uk . flowers . 87th . birthday . adams . previously . taken . part . home . wish . upon . star . programme . encourages . residents . make . wish . staff . attempt . fulfil . adams . wished . meeting . george . clooney .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: george . clooney . surprises . 87 . year . old . fan . with . b . day . flowers .\n",
      "INFO:tensorflow:GENERATED SUMMARY: george . hollywood . george . surprised . surprised over . pat . fan . uk . uk s . home . flowers . uk programme . s stays . s 87th . s actor . clooney . george programme . to . surprised residents . wish . staff . fan attempt . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  england . wales . cricket . board . asked . players . play . bold . brave . cricket . attract . new . players . fans . sport . work . every . time . cricket . privileged . __pastime__ . winning . battle . playground . also . winning . battle . car . park . said . ecb . chief . executive . tom . harrison .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: england . told . to . play . brave . cricket . to . entice . new . fans .\n",
      "INFO:tensorflow:GENERATED SUMMARY: england . england . cricket . players . players over . bold . brave . new . fans . attract . players said . cricket said battle . s cricket . s asked . board . wales . asked . battle . play . winning . car . park . park [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  comedian . sunil . grover . reacting . kapil . sharma . apology . shouting . sunil . asked . act . like . god . earlier . kapil . admitted . two . argument . shouted . sunil . first . time . five . years . reports . also . said . kapil . physically . assaulted . sunil . flight .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: do . not . act . like . a . god . sunil . grover . to . kapil . sharma .\n",
      "INFO:tensorflow:GENERATED SUMMARY: comedian . sunil . grover . kapil . apology . shouting . apology said . sunil said . kapil said . shouting said asked . s shouted . s asked . sunil earlier . s sunil . sunil sunil . s time . years . reports . reports said kapil . [unk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  25 . year . old . man . arrested . ghazipur . posting . objectionable . picture . uttar . pradesh . chief . minister . yogi . adityanath . facebook . police . said . monday . badshah . abdul . razak . forged . identity . create . facebook . account . uploaded . image . tension . intensified . hindu . yuva . vahini . members . gathered . protest . act .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . arrested . for . posting . !!__objectionable__!! . !!__pic__!! . of . up . cm . on . fb .\n",
      "INFO:tensorflow:GENERATED SUMMARY: 25 . year . old . man . ghazipur . objectionable . objectionable over . objectionable s . picture . facebook . yogi . facebook said uttar . s pradesh . s minister . s . arrested . man said facebook . s image . tension . intensified . hindu . yogi [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  according . forbes . 2017 . world . billionaires . list . microsoft . co . founder . bill . gates . world . richest . person . fortune . 86 . billion . amazons . jeff . bezos . biggest . gainer . list . fortune . jumped . 27 . 6 . billion . last . year . ranking . among . top . three . billionaires . notably . marked . first . time . forbes . reported . 2 . 000 . billionaires .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: who . are . the . 10 . richest . people . in . the . world .\n",
      "INFO:tensorflow:GENERATED SUMMARY: forbes . according . 2017 . world . billionaires . list . microsoft . bill . gates . gates s . gates notably . s . bezos . s person . s billion . s according . forbes . forbes three . s billionaires . marked . first . gates [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  passport . colours . derived . four . primary . colours . red . green . blue . black . according . __arton__ . group . runs . passport . database . blue . passports . believed . belong . new . world . nations . including . india . australia . red . passports . usually . belong . nations . communist . history . additionally . black . passports . used . new . zealand . among . others . known . rarest .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: why . do . passports . have . four . primary . colours .\n",
      "INFO:tensorflow:GENERATED SUMMARY: passport . passport . colours . primary . red . blue . black . passports . black in . blue s . blue used . s according . s arton . s belong . colours nations . s colours . colours additionally . black additionally . passports used . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  kapil . sharma . apologised . sunil . grover . twitter . admitting . facebook . post . two . argument . shouted . sunil . flight . recently . reports . emerged . stating . kapil . reportedly . drunk . assaulted . sunil . flight . reported . sunil . quit . kapil . sharma . show . following . incident . nnn .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: kapil . sharma . apologises . to . sunil . grover . on . twitter .\n",
      "INFO:tensorflow:GENERATED SUMMARY: kapil . sharma . sunil . twitter . twitter over . facebook . facebook i . kapil . facebook s . facebook reported . s flight . s two . s post . s sharma . apologised . sharma reportedly . sunil sunil . flight . sunil reported . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . __upen__ . patel . series . posts . twitter . accused . ex . girlfriend . karishma . tanna . lying . using . worst . feeling . world . knowing . used . lied . read . one . posts . also . tagged . karishma . post . wrote . thanks . caption . however . tweets . deleted .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: __upen__ . patel . accuses . ex . girlfriend . karishma . tanna . of . using . him .\n",
      "INFO:tensorflow:GENERATED SUMMARY: patel . actor . patel . series . twitter . girlfriend . twitter over . karishma . girlfriend s . tanna . karishma wrote . s read . s knowing . s used . patel read . series read . posts . posts also . tagged . twitter wrote . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  first . time . world . 227 . women . billionaires . led . 94 . year . old . l . oral . heiress . liliane . bettencourt . net . worth . 39 . 5 . billion . according . forbes . alice . walton . daughter . walmart . founder . sam . walton . world . second . richest . woman . world . third . richest . woman . jacqueline . mars . granddaughter . founders . american . candy . company . mars .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: who . are . the . richest . women . in . the . world .\n",
      "INFO:tensorflow:GENERATED SUMMARY: first . time . world . women . billionaires . 94 . l . mars . l in . oral . oral american . s world . s year . s net . s led . 227 . time third . to . richest . woman . jacqueline . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  asserting . ram . mandir . issue . used . medium . gain . power . 25 . years . congress . tuesday . said . mutual . court . settlement . communities . best . way . maintain . social . harmony . bjp . also . hailed . supreme . court . suggestion . amicable . settlement . best . solution . babri . masjid . ram . mandir . issue .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: settlement . on . ram . temple . best . way . to . maintain . harmony . cong .\n",
      "INFO:tensorflow:GENERATED SUMMARY: ram . mutual . mandir . settlement . settlement over . gain . power . best . 25 . years . congress . ram . mandir said congress . s also . mandir social . s used . ram said mandir . court . court suggestion . settlement settlement . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  cbi . registered . fir . unknown . persons . allegedly . using . national . emblem . name . prime . minister . office . dupe . people . pretext . giving . houses . poor . families . case . filed . basis . reference . prime . minister . office . attached . complaint . village . head . jharkhand .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: fir . filed . over . fraud . using . pm . s . office . name . to . dupe . people .\n",
      "INFO:tensorflow:GENERATED SUMMARY: cbi . registered . fir . using . emblem . cbi . name . name in . name 2 . complaint . name filed . s case . s prime . fir pretext . s persons . fir families . s families . using filed . using prime . office . office minister . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  __juri__ . village . jharkhand . east . singhbhum . district . initiated . practice . naming . lanes . educated . girls . sumita . bhattacharya . pursuing . master . degree . history . picked . first . girl . lane . named . despite . high . school . village . literacy . rate . higher . state . average .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jharkhand . village . names . streets . after . most . educated . girls .\n",
      "INFO:tensorflow:GENERATED SUMMARY: jharkhand . juri . jharkhand . singhbhum . practice . practice in . lanes . educated . girls . girls s . educated high . s lane . s history . s district . east . village . jharkhand lane . to . singhbhum named . despite . school . girls [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  number . maoists . killed . security . forces . 2016 . witnessed . jump . 150 . compared . 2015 . centre . informed . lok . sabha . tuesday . number . left . wing . extremists . killed . 2016 . compared . 2015 . increased . 89 . 2015 . 222 . 2016 . number . highest . last . six . years . government . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: killing . of . maoists . witnessed . 150 . increase . in . 2016 . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: maoists . number . killed . forces . 150 . jump . centre . 150 s . 150 in . s . maoists . security . security compared . s compared . maoists compared . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:ARTICLE:  madhya . pradesh . man . arrested . lodging . false . complaint . via . twitter . indian . railways . personnel . board . bhind . indore . __intercity__ . express . monday . tweet . railway . minister . suresh . prabhu . railway . protection . force . sub . inspector . boarded . train . look . issue . found . intoxicated . tweeted . fake . complaint .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . arrested . for . fake . tweet . complaint . to . railway . minister .\n",
      "INFO:tensorflow:GENERATED SUMMARY: madhya . pradesh . arrested . man . false . complaint . twitter . railways . railways s . twitter tweeted . railways tweeted . s via . s indian . s railway . man protection . s protection . lodging . arrested inspector . train . look . twitter [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  two . sufi . clerics . hazrat . nizamuddin . dargah . delhi . gone . missing . pakistan . last . week . returned . india . external . affairs . minister . sushma . swaraj . intervened . matter . duo . went . pay . obeisance . shrine . returning . blamed . pakistani . newspaper . publishing . misleading . article . led . entire . fiasco .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: nizamuddin . clerics . who . went . missing . in . pak . return . to . india .\n",
      "INFO:tensorflow:GENERATED SUMMARY: two . sufi . clerics . nizamuddin . delhi . delhi in . missing . missing 2 . s . pakistan . india . pakistan blamed . s gone . s last . s went . hazrat . clerics went . s shrine . dargah . pakistani . newspaper . missing [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "INFO:tensorflow:We've been decoding with same checkpoint for 60 seconds. Time to load new checkpoint\n",
      "INFO:tensorflow:Loading checkpoint logs_26_5\\train\\model.ckpt-4055\n",
      "INFO:tensorflow:Restoring parameters from logs_26_5\\train\\model.ckpt-4055\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  describing . steps . taken . assam . government . curb . rhino . poaching . kaziranga . national . park . environment . minister . anil . madhav . dave . said . forest . staff . empowered . use . firearms . without . prior . sanction . forest . staff . also . provided . immunity . prosecution . minister . added . number . anti . poaching . camps . also . increased . ensure . effective . surveillance .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: kaziranga . guards . can . use . firearms . without . sanction . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: forest . s . steps . assam . rhino . curb . kaziranga . rhino over . park . park sc . s national . park said environment . s minister . s government . assam said assam . to . forest . staff . rhino added . anti . poaching . kaziranga also . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  india . added . pacer . mohammed . shami . squad . final . test . scheduled . begin . march . 25 . dharamshala . shami . last . played . india . november . 2016 . since . due . knee . injury . earlier . indian . captain . kohli . hinted . including . shami . saying . sent . play . vijay . hazare . trophy . match . practice .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: india . include . mohammed . shami . in . the . squad . for . final . test .\n",
      "INFO:tensorflow:GENERATED SUMMARY: india . added . mohammed . shami . test . shami over . test in . test 2 . test s . s 25 . india . s scheduled . shami scheduled . s shami . shami captain . s kohli . squad . final . shami saying . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . diljit . dosanjh . said . never . faced . camp . system . bollywood . added . never . faced . anybody . tried . bad . whoever . met . greeted . respect . love . diljit . known . acting . singing . punjabi . films . made . bollywood . debut . 2016 . film . udta . punjab .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: never . faced . any . camp . system . in . bollywood . diljit . dosanjh .\n",
      "INFO:tensorflow:GENERATED SUMMARY: never . actor . faced . never . camp . system . system over . never s . bollywood . diljit . dosanjh . diljit said diljit . s respect . s love . never said never . s actor . never known . faced known . s never . never made . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  former . manchester . city . liverpool . striker . mario . balotelli . missed . opening . two . minutes . team . nice . ligue . 1 . match . nantes . struggled . loosen . shoelaces . balotelli . walking . start . match . went . towards . bench . assistance . helped . coach . loosened . laces .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: player . misses . two . mins . of . match . as . he . fails . to . undo . laces .\n",
      "INFO:tensorflow:GENERATED SUMMARY: manchester . former . city . liverpool . mario . balotelli . mario in . mario s . missed . balotelli s . s opening . s balotelli . manchester . s striker . liverpool loosen . s city . manchester start . s match . mario went . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  51 . year . old . alleged . member . khalistan . commando . force . __kcf__ . __gursewak__ . singh . __babla__ . arrested . country . made . pistol . delhi . police . tuesday . senior . crime . branch . officials . said . alleged . militant . involved . 75 . cases . include . terror . activities . murder . robbery . loot . majority . cases . registered . punjab . delhi . officials . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: khalistan . commando . force . member . wanted . in . 75 . cases . arrested .\n",
      "INFO:tensorflow:GENERATED SUMMARY: 51 . year . member . khalistan . commando . force . commando over . s . delhi . arrested . delhi s . s gursewak . s include . s country . old . old said year . terror . terror activities . robbery . loot . majority . cases . punjab . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  police . station . bhandara . district . maharashtra . sealed . local . municipal . council . non . payment . taxes . civic . authorities . sealed . police . station . failed . pay . municipal . tax . 1 . 19 . lakh . police . official . said . official . added . seal . broken . two . hours . later . sought . district . collector . intervention .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: maharashtra . police . station . sealed . for . non . payment . of . tax .\n",
      "INFO:tensorflow:GENERATED SUMMARY: police . station . bhandara . maharashtra . sealed . non . sealed over . non in . payment . non said . sealed said police . s authorities . s municipal . bhandara police . to . bhandara said official . seal . broken . broken added . hours . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  government . tuesday . admitted . instances . custodial . deaths . maharashtra . little . compared . states . 35 . instances . state . 2013 . 21 . 2014 . 19 . 2015 . union . minister . kiren . rijiju . said . bjp . satyapal . singh . former . mumbai . police . commissioner . however . called . state . police . best . india .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: more . custodial . deaths . in . maha . than . other . states . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: instances . tuesday . instances . custodial . maharashtra . deaths . maharashtra i . little . states . 35 . 35 however . s compared . s states . s instances . instances said instances . s . bjp . custodial said singh . mumbai . police . maharashtra however . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  supreme . court . tuesday . directed . karnataka . government . supply . 2 . 000 . cusecs . cauvery . water . tamil . nadu . every . day . matter . next . heard . july . 11 . court . earlier . dismissed . petition . seeking . compensation . states . loss . property . cauvery . water . related . dispute . karnataka . tamil . nadu .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: sc . directs . k . taka . to . release . 2 . 000 . cusecs . water . to . tn .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:GENERATED SUMMARY: supreme . supreme . karnataka . supply . 2 . 000 . cauvery . cusecs . water . tamil . dispute . tamil related . s compensation . s tamil . s nadu . s tuesday . court . karnataka seeking . s states . loss . property . cauvery [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  tibetan . religious . leader . dalai . lama . tuesday . said . global . terrorist . crisis . fast . reaching . nuclear . threshold . crosses . threshold . could . mean . entire . mankind . could . perish . added . dalai . lama . also . said . terrorism . crisis . can . not . resolved . unless . nations . come . together . single . platform .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: global . terrorism . fast . reaching . nuclear . threshold . dalai . lama .\n",
      "INFO:tensorflow:GENERATED SUMMARY: terrorist . s . global . terrorist . crisis . crisis i . i . fast . nuclear . threshold . lama . dalai . lama said lama . s threshold . s could . s leader . global said terrorist . terrorist said crisis . can . crisis said threshold [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  government . tuesday . proposed . make . aadhaar . card . mandatory . filing . income . tax . returns . itr . submit . returns . july . 1 . 2017 . aadhaar . number . would . also . needed . applying . permanent . account . number . pan . according . proposed . bill . pan . cards . linked . aadhaar . deemed . invalid .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: govt . proposes . to . make . aadhaar . mandatory . for . i . t . returns .\n",
      "INFO:tensorflow:GENERATED SUMMARY: aadhaar . government . make . aadhaar . aadhaar over . card . card in . filing . aadhaar s . returns . aadhaar according . s tax . s income . s returns . s . proposed . aadhaar number . card according . card proposed . bill . pan . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  bengaluru . based . graphic . designer . accused . taxi . hailing . startup . ola . using . design . made . hoardings . without . permission . design . shows . line . art . vidhana . soudha . building . bengaluru . 2015 . ola . also . accused . using . picture . app . launch . screen . without . permission . photographer .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: man . accuses . ola . of . copying . his . design . without . permission .\n",
      "INFO:tensorflow:GENERATED SUMMARY: bengaluru . bengaluru . designer . taxi . ola . ola in . design . startup . using . hoardings . without . ola also . s bengaluru . s using . s hailing . graphic . graphic 2015 . s 2015 . startup also . accused . ola picture . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  commentator . dean . jones . trolled . user . called . jones . cry . baby . one . tweets . jones . tweeted . many . cricketers . refuse . bat . poor . pitches . training . watching . test . day . 5 . reason . user . reply . jones . reminded . australia . currently . holds . border . gavaskar . cup . cricket . world . cup .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: dean . jones . trolls . user . after . he . calls . him . cry . baby .\n",
      "INFO:tensorflow:GENERATED SUMMARY: commentator . commentator . jones . trolled . cry . baby . baby in . tweets . baby s . jones s . tweets tweeted . s jones . jones tweeted jones . s called . dean . user . refuse . bat . australia . jones currently . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  japanese . conglomerate . softbank . backed . deal . invest . 100 . million . android . co . founder . andy . rubin . smartphone . startup . named . essential . startup . reportedly . working . smartphone . compete . apple . iphone . 7 . fall . deal . comes . apple . committed . invest . 1 . billion . softbank . 100 . billion . fund . january . year .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: softbank . cancels . 100mn . funding . in . android . founder . s . startup .\n",
      "INFO:tensorflow:GENERATED SUMMARY: softbank . japanese . deal . softbank . invest . softbank in . 100 . android . android in . android s . s million . s founder . s startup . s deal . s . backed . softbank comes . apple . invest committed . billion [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  sanjay . dutt . suffered . hairline . rib . fracture . shooting . fight . sequence . group . 19 . 20 . men . film . bhoomi . dutt . got . injured . jump . went . wrong . scene . film . climax . shot . chambal . dutt . advised . rest . continued . shooting . taking . rest . breaks .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: dutt . suffers . hairline . rib . fracture . at . shoot . of . fight . scene .\n",
      "INFO:tensorflow:GENERATED SUMMARY: sanjay . dutt . hairline . rib . fracture . shooting . sequence . fight . 20 . 19 . men . 20 advised . s film . s group . s dutt . s suffered . suffered . sanjay . hairline film . shot . dutt advised . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  bharti . airtel . tuesday . hit . back . reliance . jio . allegations . around . former . fastest . network . ad . campaign . saying . complaint . deliberate . attempt . __malign__ . __brand__ . __misguide__ . __customers__ . mobile . internet . speed . testing . firm . ookla . stood . finding . airtel . fastest . broadband . network . claim . challenged . jio .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jio . s . complaint . over . ad . meant . to . misguide . customers . airtel .\n",
      "INFO:tensorflow:GENERATED SUMMARY: airtel . bharti . hit . back . reliance . fastest . jio . jio s . fastest in . ad . airtel . ad firm . s allegations . airtel saying campaign . s airtel . hit speed . s testing . back firm . to . reliance finding . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  __herring__ . fish . communicate . underwater . sounds . created . farting . according . study . high . pitch . buzzing . sound . helps . fish . communicate . night . without . alerting . predators . helping . fish . form . protective . __shoals__ . according . study . noise . always . accompanied . stream . bubbles .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: __herring__ . fish . communicate . through . farts .\n",
      "INFO:tensorflow:GENERATED SUMMARY: fish . herring . communicate . underwater . study . farting . pitch . pitch in . pitch s . pitch according . fish . s helping . s according . s created . fish alerting . s fish . fish helping . fish form . shoals . study according . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  google . ordered . us . judge . provide . information . anyone . searched . particular . fraud . victim . name . fraud . case . perpetrator . tried . initiating . transfer . 18 . lakh . victim . account . using . fake . id . google . image . search . victims . name . brings . image . used . id .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: google . ordered . to . name . people . who . searched . for . fraud . victim .\n",
      "INFO:tensorflow:GENERATED SUMMARY: google . ordered . us . judge . anyone . fraud . google . searched . victim . name . fraud in . s name . s account . s particular . s provide . us account . to . judge using . id . google image . search . victims . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  emma . watson . dan . stevens . starrer . beauty . beast . earned . 2 . 300 . crore . 357 . million . opening . weekend . film . opening . weekend . collections . nearly . three . times . higher . __cinderella__ . __moanna__ . __zootopia__ . revealed . bollywood . trade . analyst . taran . adarsh . india . fantasy . musical . earned . 6 . crore . opening . weekend .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: beauty . and . the . beast . earns . 2 . 300 . crore . in . opening . weekend .\n",
      "INFO:tensorflow:GENERATED SUMMARY: emma . watson . stevens . beauty . 2 . 2 over . india . 300 . crore . 357 . 6 . 357 revealed . weekend . s 357 . s opening . dan . watson revealed . trade . trade revealed . taran . adarsh . india adarsh . [unk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  indian . captain . virat . kohli . accused . australian . players . trolling . indian . team . physio . patrick . farhart . australian . daily . called . kohli . donald . trump . world . sport . like . president . trump . kohli . decided . blame . media . means . trying . hide . egg . smeared . right . across . face . article . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: kohli . has . become . the . donald . trump . of . world . sport . aus . media .\n",
      "INFO:tensorflow:GENERATED SUMMARY: indian . indian . virat . kohli . players . trolling . players over . physio . team . trump . daily . kohli pak . s decided . s australian . s kohli . s accused . virat decided . kohli media . s trying . hide . egg [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . ranbir . kapoor . seen . playing . sanjay . dutt . biopic . said . never . put . much . weight . added . plan . reduce . weight . next . phase . shooting . film . directed . rajkumar . hirani . ranbir . reportedly . portray . six . different . phases . dutt . life . starting . 22 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . have . never . put . on . so . much . weight . before . ranbir . kapoor .\n",
      "INFO:tensorflow:GENERATED SUMMARY: ranbir . actor . ranbir . never . weight . much . dutt . biopic . weight in . biopic s . biopic said biopic . ranbir said ranbir . s shooting . ranbir film . s playing . ranbir directed . s ranbir . much reportedly . weight reportedly . phases . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  taxi . hailing . startup . uber . co . founder . travis . kalanick . might . step . ceo . according . reports . kalanick . reportedly . quit . position . uber . world . valuable . startup . hires . first . coo . comes . resignation . number . top . uber . executives . including . president . jeff . jones . uber . dealing . allegations . promoting . culture . sexism .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: uber . co . founder . travis . might . step . down . as . ceo . report .\n",
      "INFO:tensorflow:GENERATED SUMMARY: uber . hailing . startup . uber . kalanick . travis . uber in . kalanick over . kalanick s . kalanick including . uber including . s ceo . uber according . s uber . uber number . s co . uber president . s kalanick . uber [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  rajya . sabha . tuesday . passed . hiv . aids . prevention . control . bill . 2014 . voice . vote . bill . seeks . ensure . equal . rights . hiv . aids . affected . people . getting . treatment . admission . educational . institutions . getting . jobs . anyone . india . hiv . aids . government . take . care . treatment . health . minister . jp . nadda . said .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rs . passes . bill . to . ensure . equal . rights . of . aids . patients .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rajya . sabha . passed . hiv . aids . bill . bill over . bill s . bill said bill . s 2014 . s anyone . s ensure . hiv getting . s aids . s . rajya . hiv government . aids government . care . bill government . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  punjab . cm . captain . amarinder . singh . seek . legal . opinion . whether . cabinet . minister . navjot . singh . sidhu . continue . appear . popular . tv . show . hosted . comedian . kapil . sharma . singh . said . know . constitution . law . says . matter . sidhu . given . tourism . local . bodies . portfolios . congress . government .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: punjab . cm . to . seek . legal . opinion . over . sidhu . s . tv . commitments .\n",
      "INFO:tensorflow:GENERATED SUMMARY: punjab . cm . amarinder . seek . legal . cabinet . sidhu . sidhu in . cabinet 2 . bodies . portfolios . s . punjab . opinion . singh . singh singh . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  managing . director . idea . himanshu . kapania . said . integrating . operations . vodafone . idea . cost . combined . entity . around . 13 . 400 . crore . however . said . synergies . leading . cost . saving . result . net . benefit . 65 . 000 . crore . merged . entity . idea . vodafone . confirmed . merger . monday . form . india . largest . telecom . firm .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: vodafone . idea . merger . to . cost . 13 . 400 . crore . says . idea . md .\n",
      "INFO:tensorflow:GENERATED SUMMARY: integrating . leading . integrating . operations . vodafone . idea . cost . combined . idea i . 2 . crore . 400 . crore said entity . s around . idea said idea . integrating said integrating . my . integrating entity . operations confirmed . vodafone monday . idea monday . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  uttar . pradesh . chief . minister . yogi . adityanath . said . lok . sabha . tuesday . state . witness . corruption . communal . riots . government . uttar . pradesh . prime . minister . narendra . modi . dream . state . cm . added . yogi . yet . give . lok . sabha . membership . promised . free . state . __goondagardi__ . hooliganism .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: up . will . witness . no . communal . riots . on . my . watch . yogi .\n",
      "INFO:tensorflow:GENERATED SUMMARY: yogi . uttar . lok . sabha . witness . corruption . witness in . communal . riots . yogi . yogi said yogi . s adityanath . adityanath . minister . yogi government . s minister . chief . state . sabha added . witness yet . corruption lok . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  according . reports . upcoming . superhero . film . power . rangers . first . feature . gay . superhero . character . __trini__ . yellow . ranger . played . actress . becky . g . revealed . one . scene . girlfriend . problems . revealed . early . reviews . film . directed . dean . __israelite__ . film . release . march . 24 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: power . rangers . to . feature . first . gay . superhero . reports .\n",
      "INFO:tensorflow:GENERATED SUMMARY: reports . according . superhero . film . power . rangers . first . gay . feature . gay sc . s . first revealed . s superhero . s trini . s according . reports . reports revealed . superhero revealed . early . reviews . film revealed . gay directed . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  actor . nawazuddin . siddiqui . said . contract . killers . also . romantic . romance . help . guns . romance . done . many . ways . every . person . way . express . __romanticism__ . added . nawazuddin . actor . already . played . gangster . gangs . wasseypur . seen . playing . contract . killer . babumoshai . __bandukbaaz__ .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: contract . killers . can . romance . with . help . of . guns . nawazuddin .\n",
      "INFO:tensorflow:GENERATED SUMMARY: contract . actor . contract . killers . romantic . romance . help . guns . romance i . romance report . killer . nawazuddin . s actor . s many . s also . contract said contract . s . contract actor . killers played . romantic gangs . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  american . express . biggest . credit . card . issuer . customer . spending . paid . ceo . ken . chenault . 22 . million . nearly . 144 . crore . work . 2016 . 19 . increase . compensation . puts . par . goldman . sachs . ceo . lloyd . blankfein . notably . __amex__ . counts . warren . buffetts . berkshire . hathaway . biggest . shareholder . climbed . 6 . 5 . 2016 . tumbling . 25 . previous . year .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: american . express . raises . ceo . s . pay . to . 144 . crore . in . a . year .\n",
      "INFO:tensorflow:GENERATED SUMMARY: express . american . biggest . credit . card . issuer . customer . ceo . paid . paid s . paid climbed . s . crore . s spending . s 22 . s american . express . biggest berkshire . s biggest . shareholder . 6 . customer [unk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  kitchen . gadget . company . __kitchenaid__ . accused . sexism . posting . ad . pink . appliances . aimed . women . ad . featured . words . __kitchenaid__ . women . alongside . pink . products . part . limited . edition . range . raise . money . breast . cancer . charity . __kitchenaid__ . however . stated . colour . used . symbol . hope .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: firm . accused . of . sexism . for . its . pink . kitchen . gadgets . ad .\n",
      "INFO:tensorflow:GENERATED SUMMARY: kitchen . gadget . kitchenaid . sexism . ad . ad in . pink . appliances . ad 2 . s . women . ad however . s aimed . s pink . s kitchenaid . gadget edition . s raise . money . breast . cancer . charity . ad stated . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  singer . sukhwinder . singh . said . get . married . soon . jump . well . die . 45 . year . old . singer . revealed . music . composer . ar . rahman . pestering . get . married . want . __fantabulous__ . relationship . want . feel . special . added . sukhwinder .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: if . i . dont . get . married . in . 2017 . ill . kill . myself . sukhwinder .\n",
      "INFO:tensorflow:GENERATED SUMMARY: married . singer . get . soon . married . jump . die . well . 45 . die sc . s . sukhwinder . singh . sukhwinder said year . s singer . married revealed get . s rahman . get pestering . married get . married fantabulous . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  two . member . probe . committee . pakistan . cricket . board . pcb . travel . united . kingdom . look . pakistani . opener . nasir . jamshed . alleged . involvement . pakistan . super . league . psl . spot . fixing . scandal . according . reports . jamshed . earlier . investigated . national . crime . agency . uk . passport . withheld . authorities . granted . bail .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: pcb . panel . to . probe . nasir . jamshed . in . uk . over . psl . spot . fixing .\n",
      "INFO:tensorflow:GENERATED SUMMARY: two . member . probe . pakistan . pcb . travel . board . united . look . pcb s . united earlier . s kingdom . s according . s opener . s committee . probe according . pakistan earlier . to . cricket . crime . pcb agency . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  steve . smith . reached . career . high . 941 . rating . points . latest . icc . test . rankings . batsmen . fifth . highest . rating . points . recorded . test . cricket . 130 . year . history . smith . rating . points . fifth . best . ever . bradman . 961 . __len__ . hutton . 945 . jack . hobbs . ricky . ponting . __942__ . notably . highest . rating . player . receive . 1000 .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: smith . attains . fifth . highest . batting . rating . in . test . cricket .\n",
      "INFO:tensorflow:GENERATED SUMMARY: smith . steve . smith . career . 941 . icc . test . points . icc s . rankings . highest . highest notably . s latest . s fifth . s points . s reached . high . smith 945 . jack . ricky . ponting . test notably . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  australian . pacer . mitchell . starc . said . looking . forward . bowl . ravichandran . ashwin . australia . might . take . ashwin . advice . hit . badge . starc . given . send . ashwin . second . test . ashwin . tapping . forehead . reply . starc . taunting . abhinav . mukund . similar . manner . earlier .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: i . might . take . ashwin . s . advice . and . hit . him . on . head . says . starc .\n",
      "INFO:tensorflow:GENERATED SUMMARY: australian . pacer . looking . bowl . bowl i . ashwin . ashwin in . australia . advice . starc . ashwin said ashwin . s second . s take . s pacer . mitchell . looking said bowl . looking tapping . bowl forehead . ashwin reply . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  us . drone . airstrike . afghanistan . killed . pakistani . militant . accused . involvement . deadly . attack . bus . carrying . sri . lanka . cricket . team . 2009 . pakistani . security . sources . claimed . us . unmanned . aircraft . struck . car . carrying . qari . mohammad . yasin . also . known . ustad . aslam . sunday . southwestern . afghan . province . __paktika__ .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: militant . behind . attack . on . lankan . cricket . team . in . pak . killed .\n",
      "INFO:tensorflow:GENERATED SUMMARY: us . us . airstrike . afghanistan . drone . bus . attack . attack in . attack s . deadly . drone sunday . s carrying . s militant . s pakistani . drone carrying . to . us mohammad . s yasin . us known . ustad . bus sunday . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  first . look . poster . farhan . akhtar . annu . kapoor . starrer . fakir . venice . unveiled . film . reportedly . revolves . around . two . __conmen__ . hired . find . person . art . installation . project . venice . directed . anand . __surapur__ . film . whose . release . delayed . supposed . akhtar . debut . film . bollywood .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: 1st . look . poster . of . farhan . akhtar . s . the . fakir . of . venice . out .\n",
      "INFO:tensorflow:GENERATED SUMMARY: first . look . poster . farhan . farhan in . annu . venice . fakir . venice in . akhtar . venice directed . s venice . s film . s starrer . s directed . poster directed . look directed . anand . film . release . release whose . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "INFO:tensorflow:We've been decoding with same checkpoint for 61 seconds. Time to load new checkpoint\n",
      "INFO:tensorflow:Loading checkpoint logs_26_5\\train\\model.ckpt-4055\n",
      "INFO:tensorflow:Restoring parameters from logs_26_5\\train\\model.ckpt-4055\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  spacesuit . worn . astronaut . neil . armstrong . became . first . man . moon . 1969 . manufactured . __seamstresses__ . usually . worked . bras . __girdles__ . spacesuit . comprised . 21 . layers . different . materials . hand . sewn . women . consumer . brand . __playtex__ . suit . developed . able . withstand . temperatures . __500c__ .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: neil . armstrongs . spacesuit . was . made . by . a . bra . manufacturer .\n",
      "INFO:tensorflow:GENERATED SUMMARY: spacesuit . worn . astronaut . astronaut what . s . man . moon . moon s . moon in . moon developed . s manufactured . astronaut became . s 1969 . astronaut different . s neil . neil . first . astronaut consumer . brand . man developed . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  congress . vice . president . rahul . gandhi . must . make . way . others . interested . leading . party . kerala . youth . congress . leader . cr . mahesh . said . please . make . way . great . entity . roots . country . getting . snapped . said . workers . prepared . die . see . congress . live . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: rahul . must . make . way . if . he . can . not . lead . kerala . cong . leader .\n",
      "INFO:tensorflow:GENERATED SUMMARY: rahul . congress . president . rahul . must . rahul said . make . kerala . kerala said . interested . kerala country . s others . s way . s congress . rahul way . to . vice . gandhi . rahul country . snapped . workers . kerala [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  technology . major . apple . ceo . tim . cook . tuesday . visited . billion . dollar . chinese . startup . ofo . provides . bicycles . rent . ofo . backed . china . top . taxi . hailing . service . didi . chuxing . turn . received . 1 . billion . funding . apple . 2016 . cook . said . investment . didi . chuxing . help . apple . better . understand . chinese . market .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: apple . ceo . visits . 1 . billion . chinese . bike . sharing . startup .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:GENERATED SUMMARY: apple . major . ceo . apple . cook . cook in . billion . dollar . cook 2 . s . china . ofo . s tuesday . apple provides . s tim . s technology . apple 2016 . to . apple said apple . didi . chuxing . apple [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  founders . 170 . startups . including . paytm . ola . written . letter . home . minister . rajnath . singh . stayzilla . row . co . founder . yogendra . vasupal . arrested . unpaid . dues . founders . opposed . arrest . called . free . fair . investigation . matter . notably . another . stayzilla . co . founder . received . threats . son . life .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: 170 . founders . write . open . letter . to . home . minister . on . stayzilla .\n",
      "INFO:tensorflow:GENERATED SUMMARY: founders . founders . startups . paytm . ola . ola in . letter . rajnath . home . rajnath in . rajnath notably . s minister . s written . s home . startups called . s startups . 170 . ola investigation . s ola . stayzilla . ola notably . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  union . minister . uma . bharti . tuesday . said . use . red . beacons . stopping . traffic . delaying . flights . acceptable . minister . official . duty . minister . missing . official . meeting . might . delay . projects . cause . losses . worth . crores . exchequer . added . notably . comes . uttar . pradesh . punjab . cms . barred . ministers . using . red . beacons .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: red . beacon . delaying . flights . for . minister . acceptable . bharti .\n",
      "INFO:tensorflow:GENERATED SUMMARY: union . minister . use . red . beacons . stopping . traffic . flights . delaying . acceptable . uma . missing . bharti . duty . s added . bharti said minister . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  brian . __mcclendon__ . vice . president . maps . uber . quit . ride . hailing . startup . almost . two . years . joining . __mcclendon__ . joined . uber . 10 . years . working . google . moving . kansas . explore . politics . exit . follows . number . top . level . exits . uber . including . president . self . driving . technology . unit . head .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: uber . s . vice . president . of . mapping . quits .\n",
      "INFO:tensorflow:GENERATED SUMMARY: brian . mcclendon . maps . uber . uber over . quit . startup . uber in . quit in . uber including . s ride . s hailing . uber mcclendon . s mcclendon . vice . uber number . s top . maps including . uber president . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  police . complaint . filed . __srijato__ . __bandopadhyay__ . 42 . west . bengal . poet . hurting . religious . sentiments . hindus . according . complainant . 2nd . year . student . facebook . poem . makes . derogatory . remarks . hindu . symbol . trishul . hindu . __samhati__ . president . said . poem . written . basis . false . information . uttar . pradesh . cm . yogi . adityanath .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: fir . against . bengal . poet . for . hurting . religious . sentiments .\n",
      "INFO:tensorflow:GENERATED SUMMARY: police . complaint . bandopadhyay . 42 . west . hurting . poet . hindus . sentiments . hindus i . cm . yogi . s president . s according . s hindu . s filed . srijato . srijato said bandopadhyay . poem . poem said basis . false . hindus said hindus . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  billionaire . philanthropist . bill . gates . monday . met . us . president . donald . trump . white . house . discuss . trump . budget . proposal . would . impose . cuts . foreign . aid . comes . trump . claimed . budget . would . prioritise . security . well . americans . gates . responded . article . arguing . providing . overseas . aid . helps . keep . americans . safe .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: bill . gates . meets . donald . trump . to . discuss . cut . in . foreign . aid .\n",
      "INFO:tensorflow:GENERATED SUMMARY: billionaire . billionaire . bill . gates . trump . trump in . us . white . house . trump sc . s trump . s president . s budget . s donald . s monday . philanthropist . met . met responded . to . us providing . trump providing . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  reliance . jio . filed . complaint . advertising . standards . council . india . accusing . airtel . airing . misleading . ads . network . data . speed . jio . asked . action . taken . airtel . new . advertisement . claims . officially . india . fastest . network . jio . questioned . methodology . used . airtel . stating . can . not . use . word . officially . advertisements .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: jio . files . complaint . against . airtel . for . misleading . ads .\n",
      "INFO:tensorflow:GENERATED SUMMARY: reliance . reliance . complaint . jio . airtel . airtel in . airtel i . s . ads . airtel stating . s jio . s accusing . s data . s advertising . jio network . s fastest . jio questioned . s airtel . india . can . misleading . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  reacting . china . displeasure . tibetan . spiritual . leader . dalai . lama . visit . bihar . congress . leader . shashi . tharoor . tuesday . said . recognising . political . leader . spiritual . leader . china . stand . irrelevant . want . invite . major . buddhist . leader . conference . privilege . added .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: we . recognise . dalai . lama . as . spiritual . leader . tharoor .\n",
      "INFO:tensorflow:GENERATED SUMMARY: china . reacting . displeasure . china . dalai . china in . lama . bihar . tharoor . bihar s . lama major . s congress . s visit . s leader . china said china . s spiritual . leader . stand . irrelevant . want . dalai major . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  indian . batsman . cheteshwar . pujara . moved . second . place . icc . test . rankings . following . double . ton . ranchi . test . australia . rating . __861__ . points . pujara . climbed . four . spots . reach . career . best . ranking . virat . kohli . india . second . best . batsman . currently . ranked . fourth . overall . steve . smith . continues . number . one .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: pujara . s . double . ton . helps . him . become . 2nd . best . test . batsman .\n",
      "INFO:tensorflow:GENERATED SUMMARY: pujara . batsman . cheteshwar . pujara . icc . test . icc s . test in . rankings . double . double currently . s following . s second . pujara ranking . s place . pujara second . to . second . icc currently . ranked . test currently . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  hrd . minister . prakash . javadekar . said . nearly . 20 . posts . teachers . higher . educational . institutions . central . universities . vacant . filling . vacancies . central . universities . ongoing . continuous . process . added . javadekar . said . students . keen . take . teaching . jobs . resulting . shortage . faculty . members .\n",
      "INFO:tensorflow:REFERENCE SUMMARY: 20 . teachers . posts . vacant . in . higher . education . institutes .\n",
      "INFO:tensorflow:GENERATED SUMMARY: hrd . minister . nearly . 20 . posts . teachers . teachers i . i . higher . universities . javadekar . universities said javadekar . s added . s central . 20 said 20 . s . nearly added . posts said 20 said teachers . higher said posts . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n",
      "\n",
      "INFO:tensorflow:ARTICLE:  many . 77 . cases . sedition . registered . different . parts . country . 2014 . 2015 . minister . state . home . hansraj . ahir . said . lok . sabha . tuesday . 47 . cases . registered . 2014 . jharkhand . witnessed . highest . 18 . 30 . cases . 2015 . bihar . topped . list . nine . cases .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:REFERENCE SUMMARY: 77 . cases . of . sedition . registered . in . 2014 . 2015 . centre .\n",
      "INFO:tensorflow:GENERATED SUMMARY: 77 . many . cases . sedition . parts . parts i . 30 . 2014 . home . bihar . hansraj . home i . s country . s registered . s cases . cases said 77 . 77 . jharkhand . highest . 18 . 30 witnessed . [unk]\n",
      "\n",
      "INFO:tensorflow:Wrote visualization data to logs_26_5\\decode\\attn_vis_data.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-10e44884f606>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[0mseq2seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m \u001b[0mseq2seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-96bf845654fc>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[0mdqn_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m       \u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeamSearchDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdqn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m       \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# decode indefinitely (unless single_pass=True, in which case deocde the dataset exactly once)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The 'mode' flag must be one of train/eval/decode\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-6f50df1c1d8b>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mbest_hyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_beam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dqn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dqn_sess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dqn_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mbest_hyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_beam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m       \u001b[1;31m# Extract the output ids from the hypothesis and convert back to words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m       \u001b[0moutput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbest_hyp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-2436915bbe59>\u001b[0m in \u001b[0;36mrun_beam_search\u001b[1;34m(sess, model, vocab, batch, dqn, dqn_sess, dqn_graph)\u001b[0m\n\u001b[0;32m    137\u001b[0m                         \u001b[0mprev_coverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprev_coverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                         \u001b[0mprev_decoder_outputs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintradecoder\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                         prev_encoder_es = encoder_es if FLAGS.use_temporal_attention else tf.stack([], axis=0))\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mac_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-a48152114455>\u001b[0m in \u001b[0;36mdecode_onestep\u001b[1;34m(self, sess, batch, latest_tokens, enc_states, dec_init_states, prev_coverage, prev_decoder_outputs, prev_encoder_es)\u001b[0m\n\u001b[0;32m    836\u001b[0m       \u001b[0mfeed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_encoder_es\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_encoder_es\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_return\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# run the decoder step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[1;31m# Convert results['states'] (a single LSTMStateTuple) into a list of LSTMStateTuple -- one for each hypothesis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class flags_:\n",
    "  pass\n",
    "FLAGS = flags_()\n",
    "\n",
    "default_path = \"\"\n",
    "data_path = \"\"\n",
    "\n",
    "# Where to find data\n",
    "FLAGS.data_path = data_path + 'news_finished_files/chunked/train_*'\n",
    "FLAGS.vocab_path = data_path + 'news_finished_files/vocab'\n",
    "\n",
    "FLAGS.mode = 'decode'\n",
    "FLAGS.single_pass = False\n",
    "FLAGS.decode_after = 0\n",
    "FLAGS.decode_from = 'train'\n",
    "\n",
    "FLAGS.log_root = default_path +'logs_26_5'\n",
    "FLAGS.exp_name = 'intradecoder-temporalattention-withpretraining4'\n",
    "\n",
    "FLAGS.example_queue_threads = 4\n",
    "FLAGS.batch_queue_threads   = 2\n",
    "FLAGS.bucketing_cache_size  = 100\n",
    "\n",
    "FLAGS.enc_hidden_dim= 256\n",
    "FLAGS.dec_hidden_dim= 256\n",
    "FLAGS.emb_dim= 128\n",
    "FLAGS.batch_size=25\n",
    "FLAGS.max_enc_steps= 400\n",
    "FLAGS.max_dec_steps= 100 \n",
    "FLAGS.beam_size= 4 \n",
    "FLAGS.min_dec_steps= 35 \n",
    "FLAGS.max_iter= 20000 \n",
    "FLAGS.vocab_size= 50000 \n",
    "FLAGS.lr= 0.15\n",
    "FLAGS.adagrad_init_acc= 0.1 \n",
    "FLAGS.rand_unif_init_mag= 0.02\n",
    "FLAGS.trunc_norm_init_std= 1e-4\n",
    "FLAGS.max_grad_norm= 2.0\n",
    "FLAGS.embedding= False\n",
    "FLAGS.gpu_num= 0\n",
    "\n",
    "FLAGS.pointer_gen= True\n",
    "FLAGS.avoid_trigrams= True\n",
    "FLAGS.share_decoder_weights= False\n",
    "FLAGS.rl_training= True\n",
    "FLAGS.self_critic= True\n",
    "FLAGS.use_discounted_rewards= False\n",
    "FLAGS.use_intermediate_rewards= False\n",
    "FLAGS.convert_to_reinforce_model= True\n",
    "FLAGS.intradecoder= True\n",
    "FLAGS.use_temporal_attention=  True\n",
    "FLAGS.matrix_attention= False#, \n",
    "FLAGS.eta= 0\n",
    "FLAGS.fixed_eta= False\n",
    "FLAGS.gamma= 0.99#,\n",
    "FLAGS.reward_function= 'rouge_l/f_score'\n",
    "\n",
    "FLAGS.ac_training= False\n",
    "FLAGS.dqn_scheduled_sampling= True\n",
    "FLAGS.dqn_layers= '512,256,128'\n",
    "FLAGS.dqn_replay_buffer_size= 100000\n",
    "FLAGS.dqn_batch_size= 100 \n",
    "FLAGS.dqn_target_update= 10000\n",
    "FLAGS.dqn_sleep_time= 2\n",
    "FLAGS.dqn_gpu_num= 0\n",
    "FLAGS.dueling_net= True\n",
    "FLAGS.dqn_polyak_averaging= True\n",
    "FLAGS.calculate_true_q= False\n",
    "FLAGS.dqn_pretrain= False\n",
    "FLAGS.dqn_pretrain_steps= 10000\n",
    "FLAGS.scheduled_sampling= False\n",
    "FLAGS.decay_function= 'linear'#,'linear#, exponential#, inv_sigmoid') \n",
    "FLAGS.sampling_probability= 0\n",
    "FLAGS.fixed_sampling_probability= False\n",
    "FLAGS.hard_argmax= True\n",
    "FLAGS.greedy_scheduled_sampling= False\n",
    "FLAGS.E2EBackProp= False\n",
    "FLAGS.alpha= 1\n",
    "FLAGS.k= 1\n",
    "FLAGS.scheduled_sampling_final_dist= True\n",
    "\n",
    "FLAGS.coverage= True\n",
    "FLAGS.cov_loss_wt= 1.0\n",
    "\n",
    "FLAGS.convert_to_coverage_model= True\n",
    "FLAGS.restore_best_model= False\n",
    "\n",
    "FLAGS.debug= False\n",
    "\n",
    "\n",
    "seq2seq = Seq2Seq()\n",
    "seq2seq.main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VOhw4MViOlgb",
    "_qp2qJEoUH77",
    "CI_dHFzBG33S",
    "Ifr6xPCCH-6r",
    "oeWwMVL0Ic2r",
    "oapGiq-KLiFL",
    "XpqwdCAVK5uE",
    "Fxvwy-T5KvIz",
    "y4EKS0e5IsEB",
    "f5lP9ZHFMLlD",
    "Vl42_ExqL17l",
    "HArp8MJDMePU",
    "HyX9o0teI4fb",
    "0CT_2nWdJNnD",
    "-BUkkzzj20LP",
    "e7ViuuP3IIeL"
   ],
   "name": "Model 5 RL Policy Gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
